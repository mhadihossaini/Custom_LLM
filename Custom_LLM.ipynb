{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8011b129",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9948d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import re\n",
    "import nltk\n",
    "import PyPDF2\n",
    "import string\n",
    "import requests\n",
    "import requests\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5664d92",
   "metadata": {},
   "source": [
    "### Fetching data from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e393be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 1 - Title: Language Models are Few-Shot Learners, Author: Tom B. Brown\n",
      "\n",
      "\n",
      "Book 2 - Title: Explorations in Artificial Intelligence and Machine Learning, Author: Prof. Roberto V. Zicari\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fetch_text_from_pdf(pdf_link):\n",
    "    try:\n",
    "        # Download the PDF file from the provided link\n",
    "        response = requests.get(pdf_link)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check if the response content type is PDF\n",
    "        if response.headers.get('content-type') == 'application/pdf':\n",
    "            # You have successfully fetched the PDF content\n",
    "            pdf_content = response.content\n",
    "\n",
    "            # Create a BytesIO stream from the PDF content\n",
    "            pdf_stream = BytesIO(pdf_content)\n",
    "\n",
    "            # Create a PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfFileReader(pdf_stream)\n",
    "\n",
    "            # Initialize a variable to store the extracted text\n",
    "            extracted_text = \"\"\n",
    "\n",
    "            # Extract text from each page of the PDF\n",
    "            for page_num in range(pdf_reader.numPages):\n",
    "                page = pdf_reader.getPage(page_num)\n",
    "                extracted_text += page.extractText()\n",
    "\n",
    "            return extracted_text  # Return the extracted text\n",
    "\n",
    "        else:\n",
    "            print(\"The fetched content is not a PDF.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text is None:\n",
    "        return \"\"  # Return an empty string if text is None\n",
    "\n",
    "    # Remove non-printable characters and Unicode escape sequences\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # TODO: Add preprocess steps as per data, Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text into individual words\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and punctuation from the tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)  # Access the punctuation characters\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Collect and preprocess data from the PDFs\n",
    "corpus = []  # Use a list to store preprocessed text for each book\n",
    "\n",
    "# List of books on Chanakya Neeti with their PDF links\n",
    "books = [\n",
    "    {\"title\": \"Language Models are Few-Shot Learners\", \n",
    "     \"author\": \"Tom B. Brown\", \"pdf_link\": \"https://arxiv.org/pdf/2005.14165.pdf\"},\n",
    "    {\"title\": \"Explorations in Artificial Intelligence and Machine Learning\", \n",
    "     \"author\": \"Prof. Roberto V. Zicari\", \"pdf_link\": \"https://www.routledge.com/rsc/downloads/AI_FreeBook.pdf\"},\n",
    "    \n",
    "    # Add more books to the list\n",
    "]\n",
    "\n",
    "\n",
    "for book in books:\n",
    "    pdf_link = book[\"pdf_link\"]\n",
    "    text = fetch_text_from_pdf(pdf_link)\n",
    "    \n",
    "    if text is not None:\n",
    "        processed_text = preprocess_text(text)\n",
    "        corpus.append(processed_text)  # Append the preprocessed text for each book to the corpus list\n",
    "\n",
    "# Print the preprocessed data\n",
    "for i, book in enumerate(books):\n",
    "    print(f\"Book {i + 1} - Title: {book['title']}, Author: {book['author']}\")\n",
    "    #print(corpus[i])  # Print the preprocessed text for each book\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9182943",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the text data\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "max_seq_length = 50\n",
    "lstm_units = 256\n",
    "output_units = vocab_size\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Convert text to sequences (tokenization)\n",
    "X_sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "# Create training sequences (X_train) and labels (y_train) for text generation\n",
    "sequences = []\n",
    "for seq in X_sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        sequences.append(seq[:i+1])\n",
    "\n",
    "# Pad sequences\n",
    "X_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='pre', truncating='pre')\n",
    "\n",
    "# -Spliting the data into training and validation sets-\n",
    "# - 80% for training and 20% for validation\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X_padded) * split_ratio)\n",
    "\n",
    "X_train = X_padded[:split_index, :-1]\n",
    "y_train = X_padded[:split_index, -1]\n",
    "\n",
    "X_val = X_padded[split_index:, :-1]\n",
    "y_val = X_padded[split_index:, -1]\n",
    "\n",
    "# -Building the model-\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_seq_length-1),\n",
    "    LSTM(lstm_units),\n",
    "    Dense(output_units, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('custom_llm_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
