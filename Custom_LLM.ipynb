{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc24b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import PyPDF2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import string\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b41842",
   "metadata": {},
   "source": [
    "### Fetching data from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dbeb470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 1 - Title: Language Models are Few-Shot Learners, Author: Tom B. Brown\n",
      "language models few-shot learners tom b. brown\u0003benjamin mann\u0003nick ryder\u0003melanie subbiah\u0003 jared kaplanyprafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell sandhini agarwal ariel herbert-voss gretchen krueger tom henighan rewon child aditya ramesh daniel m. ziegler jeffrey wu clemens winter christopher hesse mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christopher berner sam mccandlish alec radford ilya sutskever dario amodei openai abstract recent work demonstrated substantial gains many nlp tasks benchmarks pre-training large corpus text followed ne-tuning specic task typically task-agnostic architecture method still requires task-specic ne-tuning datasets thousands tens thousands examples contrast humans generally perform new language task examples simple instructions something current nlp systems still largely struggle show scaling language models greatly improves task-agnostic few-shot performance sometimes even reaching competitiveness prior state-of-the-art ne- tuning approaches specically train gpt-3 autoregressive language model 175 billion parameters 10x previous non-sparse language model test performance few-shot setting tasks gpt-3 applied without gradient updates ne-tuning tasks few-shot demonstrations specied purely via text interaction model gpt-3 achieves strong performance many nlp datasets including translation question-answering cloze tasks well several tasks require on-the-y reasoning domain adaptation unscrambling words using novel word sentence performing 3-digit arithmetic time also identify datasets gpt-3s few-shot learning still struggles well datasets gpt-3 faces methodological issues related training large web corpora finally nd gpt-3 generate samples news articles human evaluators difculty distinguishing articles written humans discuss broader societal impacts nding gpt-3 general \u0003equal contribution yjohns hopkins university openai author contributions listed end paper.arxiv:2005.14165v4 cs.cl 22 jul 2020contents 1 introduction 3 2 approach 6 2.1 model architectures 8 2.2 training dataset 8 2.3 training process 9 2.4 evaluation 10 3 results 10 3.1 language modeling cloze completion tasks 11 3.2 closed book question answering 13 3.3 translation 14 3.4 winograd-style tasks 16 3.5 common sense reasoning 17 3.6 reading comprehension 18 3.7 superglue 18 3.8 nli 20 3.9 synthetic qualitative tasks 21 4 measuring preventing memorization benchmarks 29 5 limitations 33 6 broader impacts 34 6.1 misuse language models 35 6.2 fairness bias representation 36 6.3 energy usage 39 7 related work 39 8 conclusion 40 details common crawl filtering 43 b details model training 43 c details test set contamination studies 43 total compute used train language models 46 e human quality assessment synthetic news articles 46 f additional samples gpt-3 48 g details task phrasing specications 50 h results tasks model sizes 63 21 introduction recent years featured trend towards pre-trained language representations nlp systems applied increasingly exible task-agnostic ways downstream transfer first single-layer representations learned using word vectors mccd13 psm14 fed task-specic architectures rnns multiple layers representations contextual state used form stronger representations dl15 mbxs17 pnzty18 though still applied task-specic architectures recently pre-trained recurrent transformer language models vsp+17 directly ne-tuned entirely removing need task-specic architectures rnss18 dclt18 hr18 last paradigm led substantial progress many challenging nlp tasks reading comprehension question answering textual entailment many others continued advance based new architectures algorithms rsr+19 log+19 ydy+19 lcg+19 however major limitation approach architecture task-agnostic still need task-specic datasets task-specic ne-tuning achieve strong performance desired task typically requires ne-tuning dataset thousands hundreds thousands examples specic task removing limitation would desirable several reasons first practical perspective need large dataset labeled examples every new task limits applicability language models exists wide range possible useful language tasks encompassing anything correcting grammar generating examples abstract concept critiquing short story many tasks difcult collect large supervised training dataset especially process must repeated every new task second potential exploit spurious correlations training data fundamentally grows expressiveness model narrowness training distribution create problems pre-training plus ne-tuning paradigm models designed large absorb information pre-training ne-tuned narrow task distributions instance hlw+20 observe larger models necessarily generalize better out-of-distribution evidence suggests generalization achieved paradigm poor model overly specic training distribution generalize well outside ydc+19 mpl19 thus performance ne-tuned models specic benchmarks even nominally human-level may exaggerate actual performance underlying task gsl+18 nk19 third humans require large supervised datasets learn language tasks brief directive natural language e.g please tell sentence describes something happy something sad tiny number demonstrations e.g two examples people acting brave please give third example bravery often figure 1.1 language model meta-learning unsupervised pre-training language model develops broad set skills pattern recognition abilities uses abilities inference time rapidly adapt recognize desired task use term in-context learning describe inner loop process occurs within forward-pass upon sequence sequences diagram intended representative data model would see pre-training intended show sometimes repeated sub-tasks embedded within single sequence 3figure 1.2 larger models make increasingly efcient use in-context information show in-context learning performance simple task requiring model remove random symbols word without natural language task description see sec 3.9.2 steeper in-context learning curves large models demonstrate improved ability learn task contextual information see qualitatively similar behavior across wide range tasks sufcient enable human perform new task least reasonable degree competence aside pointing conceptual limitation current nlp techniques adaptability practical advantages allows humans seamlessly mix together switch many tasks skills example performing addition lengthy dialogue broadly useful would someday like nlp systems uidity generality one potential route towards addressing issues meta-learning1 context language models means model develops broad set skills pattern recognition abilities training time uses abilities inference time rapidly adapt recognize desired task illustrated figure 1.1 recent work rwc+19 attempts via call in-context learning using text input pretrained language model form task specication model conditioned natural language instruction and/or demonstrations task expected complete instances task simply predicting comes next shown initial promise approach still achieves results far inferior ne-tuning example rwc+19 achieves 4 natural questions even 55 f1 coqa result 35 points behind state art meta-learning clearly requires substantial improvement order viable practical method solving language tasks another recent trend language modeling may offer way forward recent years capacity transformer language models increased substantially 100 million parameters rnss18 300 million parameters dclt18 1.5 billion parameters rwc+19 8 billion parameters spp+19 11 billion parameters rsr+19 nally 17 billion parameters tur20 increase brought improvements text synthesis and/or downstream nlp tasks evidence suggesting log loss correlates well many downstream tasks follows smooth trend improvement scale kmh+20 since in-context learning involves absorbing many skills tasks within parameters model plausible in-context learning abilities might show similarly strong gains scale 1in context language models sometimes called zero-shot transfer term potentially ambiguous method zero-shot sense gradient updates performed often involves providing inference-time demonstrations model truly learning zero examples avoid confusion use term meta-learning capture inner-loop outer-loop structure general method term context-learning refer inner loop meta-learning specialize description zero-shot one-shot few-shot depending many demonstrations provided inference time terms intended remain agnostic question whether model learns new tasks scratch inference time simply recognizes patterns seen training important issue discuss later paper meta-learning intended encompass possibilities simply describes inner-outer loop structure 4figure 1.3 aggregate performance 42 accuracy-denominated benchmarks zero-shot performance improves steadily model size few-shot performance increases rapidly demonstrating larger models procient in-context learning see figure 3.8 detailed analysis superglue standard nlp benchmark suite paper test hypothesis training 175 billion parameter autoregressive language model call gpt-3 measuring in-context learning abilities specically evaluate gpt-3 two dozen nlp datasets well several novel tasks designed test rapid adaptation tasks unlikely directly contained training set task evaluate gpt-3 3 conditions few-shot learning in-context learning allow many demonstrations models context window typically 10 100 b one-shot learning allow one demonstration c zero-shot learning demonstrations allowed instruction natural language given model gpt-3 could also principle evaluated traditional ne-tuning setting leave future work figure 1.2 illustrates conditions study shows few-shot learning simple task requiring model remove extraneous symbols word model performance improves addition natural language task description number examples models context k. few-shot learning also improves dramatically model size though results case particularly striking general trends model size number examples in-context hold tasks study emphasize learning curves involve gradient updates ne-tuning increasing numbers demonstrations given conditioning broadly nlp tasks gpt-3 achieves promising results zero-shot one-shot settings few-shot setting sometimes competitive even occasionally surpasses state-of-the-art despite state-of-the-art held ne-tuned models example gpt-3 achieves 81.5 f1 coqa zero-shot setting 84.0 f1 coqa one-shot setting 85.0 f1 few-shot setting similarly gpt-3 achieves 64.3 accuracy triviaqa zero-shot setting 68.0 one-shot setting 71.2 few-shot setting last state-of-the-art relative ne-tuned models operating closed-book setting gpt-3 also displays one-shot few-shot prociency tasks designed test rapid adaption on-the-y reasoning include unscrambling words performing arithmetic using novel words sentence seeing dened also show few-shot setting gpt-3 generate synthetic news articles human evaluators difculty distinguishing human-generated articles time also nd tasks few-shot performance struggles even scale gpt-3 includes natural language inference tasks like anli dataset reading comprehension datasets like race quac presenting broad characterization gpt-3s strengths weaknesses including limitations hope stimulate study few-shot learning language models draw attention progress needed heuristic sense overall results seen figure 1.3 aggregates various tasks though seen rigorous meaningful benchmark 5we also undertake systematic study data contamination growing problem training high capacity models datasets common crawl potentially include content test datasets simply content often exists web paper develop systematic tools measure data contamination quantify distorting effects although nd data contamination minimal effect gpt-3s performance datasets identify datasets could inating results either report results datasets note asterisk depending severity addition also train series smaller models ranging 125 million parameters 13 billion parameters order compare performance gpt-3 zero one few-shot settings broadly tasks nd relatively smooth scaling model capacity three settings one notable pattern gap zero- one- few-shot performance often grows model capacity perhaps suggesting larger models procient meta-learners finally given broad spectrum capabilities displayed gpt-3 discuss concerns bias fairness broader societal impacts attempt preliminary analysis gpt-3s characteristics regard remainder paper organized follows section 2 describe approach methods training gpt-3 evaluating section 3 presents results full range tasks zero- one- few-shot settings section 4 addresses questions data contamination train-test overlap section 5 discusses limitations gpt-3 section 6 discusses broader impacts section 7 reviews related work section 8 concludes 2 approach basic pre-training approach including model data training similar process described rwc+19 relatively straightforward scaling model size dataset size diversity length training use in-context learning also similar rwc+19 work systematically explore different settings learning within context therefore start section explicitly dening contrasting different settings evaluating gpt-3 could principle evaluate gpt-3 settings seen lying spectrum much task-specic data tend rely specically identify least four points spectrum see figure 2.1 illustration fine-tuning ft common approach recent years involves updating weights pre-trained model training supervised dataset specic desired task typically thousands hundreds thousands labeled examples used main advantage ne-tuning strong performance many benchmarks main disadvantages need new large dataset every task potential poor generalization out-of-distribution mpl19 potential exploit spurious features training data gsl+18 nk19 potentially resulting unfair comparison human performance work ne-tune gpt-3 focus task-agnostic performance gpt-3 ne-tuned principle promising direction future work few-shot fs term use work refer setting model given demonstrations task inference time conditioning rwc+19 weight updates allowed shown figure 2.1 typical dataset example context desired completion example english sentence french translation few-shot works giving kexamples context completion one nal example context model expected provide completion typically set kin range 10 100 many examples models context window nctx= 2048 main advantages few-shot major reduction need task-specic data reduced potential learn overly narrow distribution large narrow ne-tuning dataset main disadvantage results method far much worse state-of-the-art ne-tuned models also small amount task specic data still required indicated name few-shot learning described language models related few-shot learning used contexts ml hyc01 vbl+16 involve learning based broad distribution tasks case implicit pre-training data rapidly adapting new task one-shot 1s few-shot except one demonstration allowed addition natural language description task shown figure 1. reason distinguish one-shot few-shot zero-shot closely matches way tasks communicated humans example asking humans generate dataset human worker service example mechanical turk common give one demonstration task contrast sometimes difcult communicate content format task examples given 6figure 2.1 zero-shot one-shot few-shot contrasted traditional ne-tuning panels show four methods performing task language model ne-tuning traditional method whereas zero- one- few-shot study work require model perform task forward passes test time typically present model dozen examples shot setting exact phrasings task descriptions examples prompts found appendix g. zero-shot 0s one-shot except demonstrations allowed model given natural language instruction describing task method provides maximum convenience potential robustness avoidance spurious correlations unless occur broadly across large corpus pre-training data also challenging setting cases may even difcult humans understand format task without prior examples setting cases unfairly hard example someone asked make table world records 200m dash request ambiguous may clear exactly format table included even careful clarication understanding precisely desired difcult nevertheless least settings zero-shot closest humans perform tasks example translation example figure 2.1 human would likely know text instruction figure 2.1 shows four methods using example translating english french paper focus zero-shot one-shot few-shot aim comparing competing alternatives different problem settings offer varying trade-off performance specic benchmarks sample efciency especially highlight few-shot results many slightly behind state-of-the-art ne-tuned models ultimately however one-shot even sometimes zero-shot seem like fairest comparisons human performance important targets future work sections 2.1-2.3 give details models training data training process respectively section 2.4 discusses details few-shot one-shot zero-shot evaluations 7model name nparamsnlayersdmodelnheadsdhead batch size learning rate gpt-3 small 125m 12 768 12 64 0.5m 6:0\u000210\u00004 gpt-3 medium 350m 24 1024 16 64 0.5m 3:0\u000210\u00004 gpt-3 large 760m 24 1536 16 96 0.5m 2:5\u000210\u00004 gpt-3 xl 1.3b 24 2048 24 128 1m 2:0\u000210\u00004 gpt-3 2.7b 2.7b 32 2560 32 80 1m 1:6\u000210\u00004 gpt-3 6.7b 6.7b 32 4096 32 128 2m 1:2\u000210\u00004 gpt-3 13b 13.0b 40 5140 40 128 2m 1:0\u000210\u00004 gpt-3 175b gpt-3 175.0b 96 12288 96 128 3.2m 0:6\u000210\u00004 table 2.1 sizes architectures learning hyper-parameters batch size tokens learning rate models trained models trained total 300 billion tokens 2.1 model architectures use model architecture gpt-2 rwc+19 including modied initialization pre-normalization reversible tokenization described therein exception use alternating dense locally banded sparse attention patterns layers transformer similar sparse transformer cgrs19 study dependence ml performance model size train 8 different sizes model ranging three orders magnitude 125 million parameters 175 billion parameters last model call gpt-3 previous work kmh+20 suggests enough training data scaling validation loss approximately smooth power law function size training models many different sizes allows us test hypothesis validation loss downstream language tasks table 2.1 shows sizes architectures 8 models nparams total number trainable parameters nlayers total number layers dmodel number units bottleneck layer always feedforward layer four times size bottleneck layer 4\u0003dmodel anddhead dimension attention head models use context window nctx= 2048 tokens partition model across gpus along depth width dimension order minimize data-transfer nodes precise architectural parameters model chosen based computational efciency load-balancing layout models across gpus previous work kmh+20 suggests validation loss strongly sensitive parameters within reasonably broad range 2.2 training dataset datasets language models rapidly expanded culminating common crawl dataset2 rsr+19 constituting nearly trillion words size dataset sufcient train largest models without ever updating sequence twice however found unltered lightly ltered versions common crawl tend lower quality curated datasets therefore took 3 steps improve average quality datasets 1 downloaded ltered version commoncrawl based similarity range high-quality reference corpora 2 performed fuzzy deduplication document level within across datasets prevent redundancy preserve integrity held-out validation set accurate measure overtting 3 also added known high-quality reference corpora training mix augment commoncrawl increase diversity details rst two points processing common crawl described appendix a. third added several curated high-quality datasets including expanded version webtext dataset rwc+19 collected scraping links longer period time rst described kmh+20 two internet-based books corpora books1 books2 english-language wikipedia table 2.2 shows nal mixture datasets used training commoncrawl data downloaded 41 shards monthly commoncrawl covering 2016 2019 constituting 45tb compressed plaintext ltering 570gb ltering roughly equivalent 400 billion byte-pair-encoded tokens note training datasets sampled proportion size rather datasets view higher-quality sampled frequently commoncrawl books2 datasets sampled less training datasets sampled 2-3 times essentially accepts small amount overtting exchange higher quality training data 2https //commoncrawl.org/the-data/ 8figure 2.2 total compute used training based analysis scaling laws neural language models kmh+20 train much larger models many fewer tokens typical consequence although gpt-3 3b almost 10x larger roberta-large 355m params models took roughly 50 petaop/s-days compute pre-training methodology calculations found appendix d. datasetquantity tokens weight training mixepochs elapsed training 300b tokens common crawl ltered 410 billion 60 0.44 webtext2 19 billion 22 2.9 books1 12 billion 8 1.9 books2 55 billion 8 0.43 wikipedia 3 billion 3 3.4 table 2.2 datasets used train gpt-3 weight training mix refers fraction examples training drawn given dataset intentionally make proportional size dataset result train 300 billion tokens datasets seen 3.4 times training datasets seen less major methodological concern language models pretrained broad swath internet data particularly large models capacity memorize vast amounts content potential contamination downstream tasks test development sets inadvertently seen pre-training reduce contamination searched attempted remove overlaps development test sets benchmarks studied paper unfortunately bug ltering caused us ignore overlaps due cost training feasible retrain model section 4 characterize impact remaining overlaps future work aggressively remove data contamination 2.3 training process found kmh+20 mkat18 larger models typically use larger batch size require smaller learning rate measure gradient noise scale training use guide choice batch size mkat18 table 2.1 shows parameter settings used train larger models without running memory use mixture model parallelism within matrix multiply model parallelism across layers network models trained v100 gpus part high-bandwidth cluster provided microsoft details training process hyperparameter settings described appendix b 92.4 evaluation few-shot learning evaluate example evaluation set randomly drawing kexamples tasks training set conditioning delimited 1 2 newlines depending task lambada storycloze supervised training set available draw conditioning examples development set evaluate test set winograd original superglue version one dataset draw conditioning examples directly kcan value 0 maximum amount allowed models context window nctx= 2048 models typically ts 10to100examples larger values kare usually always better separate development test set available experiment values kon development set run best value test set tasks see appendix g also use natural language prompt addition fork= 0 instead demonstrations tasks involve choosing one correct completion several options multiple choice provide kexamples context plus correct completion followed one example context compare lm likelihood completion tasks compare per-token likelihood normalize length however small number datasets arc openbookqa race gain additional benet measured development set normalizing unconditional probability completion computingp completionjcontext p completionjanswer context answer context string `` answer `` '' `` used prompt completion answer otherwise generic tasks involve binary classication give options semantically meaningful names e.g true false rather 0 1 treat task like multiple choice also sometimes frame task similar done rsr+19 see appendix g details tasks free-form completion use beam search parameters rsr+19 beam width 4 length penalty 0:6. score model using f1 similarity score bleu exact match depending standard dataset hand final results reported test set publicly available model size learning setting zero- one- few-shot test set private model often large test server report results development set submit test server small number datasets superglue triviaqa piqa able make submission work submit 200b few-shot results report development set results everything else 3 results figure 3.1 display training curves 8 models described section 2. graph also include 6 additional extra-small models 100,000 parameters observed kmh+20 language modeling performance follows power-law making efcient use training compute extending trend two orders magnitude observe slight departure power-law one might worry improvements cross-entropy loss come modeling spurious details training corpus however see following sections improvements cross-entropy loss lead consistent performance gains across broad spectrum natural language tasks evaluate 8 models described section 2 175 billion parameter parameter gpt-3 7 smaller models wide range datasets group datasets 9 categories representing roughly similar tasks section 3.1 evaluate traditional language modeling tasks tasks similar language modeling cloze tasks sentence/paragraph completion tasks section 3.2 evaluate closed book question answering tasks tasks require using information stored models parameters answer general knowledge questions section 3.3 evaluate models ability translate languages especially one-shot few-shot section 3.4 evaluate models performance winograd schema-like tasks section 3.5 evaluate datasets involve commonsense reasoning question answering section 3.6 evaluate reading comprehension tasks section 3.7 evaluate superglue benchmark suite 3.8 briey explore nli finally section 3.9 invent additional tasks designed especially probe in-context learning abilities tasks focus on-the-y reasoning adaptation skills open-ended text synthesis evaluate tasks few-shot one-shot zero-shot settings 10figure 3.1 smooth scaling performance compute performance measured terms cross-entropy validation loss follows power-law trend amount compute used training power-law behavior observed kmh+20 continues additional two orders magnitude small deviations predicted curve gure exclude embedding parameters compute parameter counts setting ptb sota zero-shot 35.8a gpt-3 zero-shot 20.5 table 3.1 zero-shot results ptb language modeling dataset many common language modeling datasets omitted derived wikipedia sources included gpt-3s training data rwc+19 3.1 language modeling cloze completion tasks section test gpt-3s performance traditional task language modeling well related tasks involve predicting single word interest completing sentence paragraph choosing possible completions piece text 3.1.1 language modeling calculate zero-shot perplexity penn tree bank ptb mkm+94 dataset measured rwc+19 omit 4 wikipedia-related tasks work entirely contained training data also omit one-billion word benchmark due high fraction dataset contained training set ptb escapes issues due predating modern internet largest model sets new sota ptb substantial margin 15 points achieving perplexity 20.50. note since ptb traditional language modeling dataset clear separation examples dene one-shot few-shot evaluation around measure zero-shot 3.1.2 lambada lambada dataset pkl+16 tests modeling long-range dependencies text model asked predict last word sentences require reading paragraph context recently suggested continued scaling language models yielding diminishing returns difcult benchmark bht+20 reect small 1.5 improvement achieved doubling model size two recent state art results spp+19 11settinglambada acc lambada ppl storycloze acc hellaswag acc sota 68.0a8.63b91.8c85.6d gpt-3 zero-shot 76.2 3.00 83.2 78.9 gpt-3 one-shot 72.5 3.35 84.7 78.1 gpt-3 few-shot 86.4 1.92 87.7 79.3 table 3.2 performance cloze completion tasks gpt-3 signicantly improves sota lambada achieving respectable performance two difcult completion prediction datasets.a tur20 b rwc+19 c ldl19 lch+20 figure 3.2 lambada few-shot capability language models results strong boost accuracy gpt-3 2.7b outperforms sota 17b parameter turing-nlg tur20 setting gpt-3 175b advances state art 18 note zero-shot uses different format one-shot few-shot described text tur20 argue continuing expand hardware data sizes orders magnitude path forward nd path still promising zero-shot setting gpt-3 achieves 76 lambada gain 8 previous state art lambada also demonstration exibility few-shot learning provides way address problem classically occurs dataset although completion lambada always last word sentence standard language model way knowing detail thus assigns probability correct ending also valid continuations paragraph problem partially addressed past stop-word lters rwc+19 ban continuation words few-shot setting instead allows us frame task cloze-test allows language model infer examples completion exactly one word desired use following ll-in-the-blank format alice friends bob alice went visit friend bob george bought baseball equipment ball glove presented examples formatted way gpt-3 achieves 86.4 accuracy few-shot setting increase 18 previous state-of-the-art observe few-shot performance improves strongly model size setting decreases performance smallest model almost 20 gpt-3 improves accuracy 10 finally ll-in-blank method effective one-shot always performs worse zero-shot setting perhaps models still require several examples recognize pattern 12setting naturalqs webqs triviaqa rag fine-tuned open-domain lpp+20 44.5 45.5 68.0 t5-11b+ssm fine-tuned closed-book rrs20 36.6 44.7 60.5 t5-11b fine-tuned closed-book 34.5 37.4 50.1 gpt-3 zero-shot 14.6 14.4 64.3 gpt-3 one-shot 23.0 25.3 68.0 gpt-3 few-shot 29.9 41.5 71.2 table 3.3 results three open-domain qa tasks gpt-3 shown few- one- zero-shot settings compared prior sota results closed book open domain settings triviaqa few-shot result evaluated wiki split test server one note caution analysis test set contamination identied signicant minority lambada dataset appears present training data however analysis performed section 4 suggests negligible impact performance 3.1.3 hellaswag hellaswag dataset zhb+19 involves picking best ending story set instructions examples adversarially mined difcult language models remaining easy humans achieve 95.6 accuracy gpt-3 achieves 78.1 accuracy one-shot setting 79.3 accuracy few-shot setting outperforming 75.4 accuracy ne-tuned 1.5b parameter language model zhr+19 still fair amount lower overall sota 85.6 achieved ne-tuned multi-task model alum 3.1.4 storycloze next evaluate gpt-3 storycloze 2016 dataset mch+16 involves selecting correct ending sentence ve-sentence long stories gpt-3 achieves 83.2 zero-shot setting 87.7 few-shot setting k= 70 still 4.1 lower ne-tuned sota using bert based model ldl19 improves previous zero-shot results roughly 10 3.2 closed book question answering section measure gpt-3s ability answer questions broad factual knowledge due immense amount possible queries task normally approached using information retrieval system nd relevant text combination model learns generate answer given question retrieved text since setting allows system search condition text potentially contains answer denoted open-book rrs20 recently demonstrated large language model perform surprisingly well directly answering questions without conditioning auxilliary information denote restrictive evaluation setting closed-book work suggests even higher-capacity models could perform even better test hypothesis gpt-3 evaluate gpt-3 3 datasets rrs20 natural questions kpr+19 webquestions bcfl13 triviaqa jcwz17 using splits note addition results closed-book setting use few-shot one-shot zero-shot evaluations represent even stricter setting previous closed-book qa work addition external content allowed ne-tuning q dataset also permitted results gpt-3 shown table 3.3. triviaqa achieve 64.3 zero-shot setting 68.0 one-shot setting 71.2 few-shot setting zero-shot result already outperforms ne-tuned t5-11b 14.2 also outperforms version q tailored span prediction pre-training 3.8 one-shot result improves 3.7 matches sota open-domain qa system ne-tunes also makes use learned retrieval mechanism 15.3b parameter dense vector index 21m documents lpp+20 gpt-3s few-shot result improves performance another 3.2 beyond webquestions webqs gpt-3 achieves 14.4 zero-shot setting 25.3 one-shot setting 41.5 few-shot setting compares 37.4 ne-tuned t5-11b 44.7 ne-tuned t5-11b+ssm uses q a-specic pre-training procedure gpt-3 few-shot setting approaches performance state-of-the-art ne-tuned models notably compared triviaqa webqs shows much larger gain zero-shot few-shot indeed zero-shot one-shot performance poor perhaps suggesting webqs questions 13figure 3.3 triviaqa gpt3s performance grows smoothly model size suggesting language models continue absorb knowledge capacity increases one-shot few-shot performance make signicant gains zero-shot behavior matching exceeding performance sota ne-tuned open-domain model rag lpp+20 and/or style answers out-of-distribution gpt-3 nevertheless gpt-3 appears able adapt distribution recovering strong performance few-shot setting natural questions nqs gpt-3 achieves 14.6 zero-shot setting 23.0 one-shot setting 29.9 few-shot setting compared 36.6 ne-tuned t5 11b+ssm similar webqs large gain zero-shot few-shot may suggest distribution shift may also explain less competitive performance compared triviaqa webqs particular questions nqs tend towards ne-grained knowledge wikipedia specically could testing limits gpt-3s capacity broad pretraining distribution overall one three datasets gpt-3s one-shot matches open-domain ne-tuning sota two datasets approaches performance closed-book sota despite using ne-tuning 3 datasets nd performance scales smoothly model size figure 3.3 appendix h figure h.7 possibly reecting idea model capacity translates directly knowledge absorbed parameters model 3.3 translation gpt-2 lter used multilingual collection documents produce english dataset due capacity concerns even ltering gpt-2 showed evidence multilingual capability performed non-trivially translating french english despite training 10 megabytes remaining french text since increase capacity two orders magnitude gpt-2 gpt-3 also expand scope training dataset include representation languages though remains area improvement discussed 2.2 majority data derived raw common crawl quality-based ltering although gpt-3s training data still primarily english 93 word count also includes 7 text languages languages documented supplemental material order better understand translation capability also expand analysis include two additional commonly studied languages german romanian existing unsupervised machine translation approaches often combine pretraining pair monolingual datasets back-translation shb15 bridge two languages controlled way contrast gpt-3 learns blend training data mixes many languages together natural way combining word sentence document level gpt-3 also uses single training objective customized designed task particular however one few-shot settings arent strictly comparable prior unsupervised work since make use small amount paired examples 1 64 corresponds page two in-context training data results shown table 3.4. zero-shot gpt-3 receives natural language description task still underperforms recent unsupervised nmt results however providing single example demonstration 14setting en fr fr en en de de en en ro ro en sota supervised 45.6a35.0b41.2c40.2d38.5e39.9e xlm lc19 33.4 33.3 26.4 34.3 33.3 31.8 mass stq+19 37.5 34.9 28.3 35.2 35.2 33.1 mbart lgg+20 29.8 34.0 35.0 30.5 gpt-3 zero-shot 25.2 21.2 24.6 27.2 14.1 19.9 gpt-3 one-shot 28.3 33.7 26.2 30.4 20.6 38.6 gpt-3 few-shot 32.6 39.2 29.7 40.6 21.0 39.5 table 3.4 few-shot gpt-3 outperforms previous unsupervised nmt work 5 bleu translating english reecting strength english lm report bleu scores wmt14 fr en wmt16 de en wmt16 ro en datasets measured multi-bleu.perl xlms tokeniza- tion order compare closely prior unsupervised nmt work sacrebleuf pos18 results re- ported appendix h. underline indicates unsupervised few-shot sota bold indicates supervised sota relative condence.a eoag18 b dhkh14 c wxh+18 or16 e lgg+20 f sacrebleu signature bleu+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20 figure 3.4 few-shot translation performance 6 language pairs model capacity increases consistent trend improvement across datasets model scales well tendency translation english stronger translation english 15setting winograd winogrande xl fine-tuned sota 90.1a84.6b gpt-3 zero-shot 88.3 70.2 gpt-3 one-shot 89.7 73.2 gpt-3 few-shot 88.6 77.7 table 3.5 results wsc273 version winograd schemas adversarial winogrande dataset see section 4 details potential contamination winograd test set.a sbbc19 b lyn+20 figure 3.5 zero- one- few-shot performance adversarial winogrande dataset model capacity scales scaling relatively smooth gains few-shot learning increasing model size few-shot gpt-3 175b competitive ne-tuned roberta-large translation task improves performance 7 bleu nears competitive performance prior work gpt-3 full few-shot setting improves another 4 bleu resulting similar average performance prior unsupervised nmt work gpt-3 noticeable skew performance depending language direction three input languages studied gpt-3 signicantly outperforms prior unsupervised nmt work translating english underperforms translating direction performance en-ro noticeable outlier 10 bleu worse prior unsupervised nmt work could weakness due reusing byte-level bpe tokenizer gpt-2 developed almost entirely english training dataset fr-en de-en shot gpt-3 outperforms best supervised result could nd due unfamiliarity literature appearance un-competitive benchmarks suspect results represent true state art ro-en shot gpt-3 performs within 0.5 bleu overall sota achieved combination unsupervised pretraining supervised netuning 608k labeled examples backtranslation lhcg19b finally across language pairs across three settings zero- one- few-shot smooth trend improvement model capacity shown figure 3.4 case few-shot results scaling three settings shown appendix h. 3.4 winograd-style tasks winograd schemas challenge ldm12 classical task nlp involves determining word pronoun refers pronoun grammatically ambiguous semantically unambiguous human recently ne-tuned language models achieved near-human performance original winograd dataset difcult versions 16setting piqa arc easy arc challenge openbookqa fine-tuned sota 79.4 92.0 kks+20 78.5 kks+20 87.2 kks+20 gpt-3 zero-shot 80.5 68.8 51.4 57.6 gpt-3 one-shot 80.5 71.2 53.2 58.8 gpt-3 few-shot 82.8 70.1 51.5 65.4 table 3.6 gpt-3 results three commonsense reasoning tasks piqa arc openbookqa gpt-3 few-shot piqa result evaluated test server see section 4 details potential contamination issues piqa test set figure 3.6 gpt-3 results piqa zero-shot one-shot few-shot settings largest model achieves score development set three conditions exceeds best recorded score task adversarially-mined winogrande dataset sbbc19 still signicantly lag human performance test gpt-3s performance winograd winogrande usual zero- one- few-shot setting winograd test gpt-3 original set 273 winograd schemas using partial evaluation method described rwc+19 note setting differs slightly wsc task superglue benchmark presented binary classication requires entity extraction convert form described section winograd gpt-3 achieves 88.3 89.7 88.6 zero-shot one-shot few-shot settings showing clear in-context learning cases achieving strong results points state-of-the-art estimated human performance note contamination analysis found winograd schemas training data appears small effect results see section 4 difcult winogrande dataset nd gains in-context learning gpt-3 achieves 70.2 zero-shot setting 73.2 one-shot setting 77.7 few-shot setting comparison ne-tuned roberta model achieves 79 state-of-the-art 84.6 achieved ne-tuned high capacity model t5 human performance task reported sbbc19 94.0 3.5 common sense reasoning next consider three datasets attempt capture physical scientic reasoning distinct sentence completion reading comprehension broad knowledge question answering rst physicalqa piqa bzb+19 asks common sense questions physical world works intended probe grounded understanding world gpt-3 achieves 81.0 accuracy zero-shot 80.5 accuracy one-shot 82.8 accuracy few-shot last measured piqas test server compares favorably 79.4 accuracy prior state-of-the-art 17setting coqa drop quac squadv2 race-h race-m fine-tuned sota 90.7a89.1b74.4c93.0d90.0e93.1e gpt-3 zero-shot 81.5 23.6 41.5 59.5 45.5 58.4 gpt-3 one-shot 84.0 34.3 43.3 65.4 45.9 57.4 gpt-3 few-shot 85.0 36.5 44.3 69.8 46.8 58.1 table 3.7 results reading comprehension tasks scores f1 except results race report accuracy jzc+19 b jn20 c ai19 qia20 e spp+19 ne-tuned roberta piqa shows relatively shallow scaling model size still 10 worse human performance gpt-3s few-shot even zero-shot result outperform current state-of-the-art analysis agged piqa potential data contamination issue despite hidden test labels therefore conservatively mark result asterisk see section 4 details arc cce+18 dataset multiple-choice questions collected 3rd 9th grade science exams challenge version dataset ltered questions simple statistical information retrieval methods unable correctly answer gpt-3 achieves 51.4 accuracy zero-shot setting 53.2 one-shot setting 51.5 few-shot setting approaching performance ne-tuned roberta baseline 55.9 uniedqa kks+20 easy version dataset questions either mentioned baseline approaches answered correctly gpt-3 achieves 68.8 71.2 70.1 slightly exceeds ne-tuned roberta baseline kks+20 however results still much worse overall sotas achieved uniedqa exceeds gpt-3s few-shot results 27 challenge set 22 easy set openbookqa mcks18 gpt-3 improves signicantly zero shot settings still 20 points short overall sota gpt-3s few-shot performance similar ne-tuned bert large baseline leaderboard overall in-context learning gpt-3 shows mixed results commonsense reasoning tasks small inconsistent gains observed one few-shot learning settings piqa arc signicant improvement observed openbookqa gpt-3 sets sota new piqa dataset evaluation settings 3.6 reading comprehension next evaluate gpt-3 task reading comprehension use suite 5 datasets including abstractive multiple choice span based answer formats dialog single question settings observe wide spread gpt-3s performance across datasets suggestive varying capability different answer formats general observe gpt-3 par initial baselines early results trained using contextual representations respective dataset gpt-3 performs best within 3 points human baseline coqa rcm19 free-form conversational dataset performs worst 13 f1 elmo baseline quac chi+18 dataset requires modeling structured dialog acts answer span selections teacher-student interactions drop dwd+19 dataset testing discrete reasoning numeracy context reading comprehension gpt-3 few-shot setting outperforms ne-tuned bert baseline original paper still well human performance state-of-the-art approaches augment neural networks symbolic systems rll+19 squad 2.0 rjl18 gpt-3 demonstrates few-shot learning capabilities improving almost 10 f1 69.8 compared zero-shot setting allows slightly outperform best ne-tuned result original paper race lxl+17 multiple choice dataset middle school high school english examinations gpt-3 performs relatively weakly competitive earliest work utilizing contextual representations still 45 behind sota 3.7 superglue order better aggregate results nlp tasks compare popular models bert roberta systematic way also evaluate gpt-3 standardized collection datasets superglue benchmark wpn+19 wpn+19 clc+19 dmst19 rbg11 kcr+18 zll+18 dgm06 bhdd+06 gmdd07 bdd+09 pcc18 phr+18 gpt-3s test-set performance superglue dataset shown table 3.8. few-shot setting used 32 examples tasks sampled randomly training set tasks except wsc 18figure 3.7 gpt-3 results coqa reading comprehension task gpt-3 175b achieves 85 f1 few-shot setting points behind measured human performance state-of-the-art ne-tuned models zero-shot one-shot performance points behind gains few-shot largest bigger models superglue boolq cb cb copa rte average accuracy accuracy f1 accuracy accuracy fine-tuned sota 89.0 91.0 96.9 93.9 94.8 92.5 fine-tuned bert-large 69.0 77.4 83.6 75.7 70.6 71.7 gpt-3 few-shot 71.8 76.4 75.6 52.0 92.0 69.0 wic wsc multirc multirc record record accuracy accuracy accuracy f1a accuracy f1 fine-tuned sota 76.1 93.8 62.3 88.2 92.5 93.3 fine-tuned bert-large 69.6 64.6 24.1 70.0 71.3 72.0 gpt-3 few-shot 49.4 80.1 30.5 75.4 90.2 91.1 table 3.8 performance gpt-3 superglue compared ne-tuned baselines sota results reported test set gpt-3 few-shot given total 32 examples within context task performs gradient updates 19figure 3.8 performance superglue increases model size number examples context value ofk= 32 means model shown 32 examples per task 256 examples total divided across 8 tasks superglue report gpt-3 values dev set numbers directly comparable dotted reference lines test set results table 3.8 bert-large reference model ne-tuned superglue training set 125k examples whereas bert++ rst ne-tuned multinli 392k examples swag 113k examples ne-tuning superglue training set total 630k ne-tuning examples nd difference performance bert-large bert++ roughly equivalent difference gpt-3 one example per context versus eight examples per context multirc sampled new set examples use context problem wsc multirc used set randomly drawn examples training set context problems evaluated observe wide range gpt-3s performance across tasks copa record gpt-3 achieves near-sota performance one-shot few-shot settings copa falling couple points short achieving second place leaderboard rst place held ne-tuned 11 billion parameter model t5 wsc performance still relatively strong achieving 80.1 few-shot setting note gpt-3 achieves 88.6 original winograd dataset described section 3.4 boolq multirc rte performance reasonable roughly matching ne-tuned bert-large cb see signs life 75.6 few-shot setting wic notable weak spot few-shot performance 49.4 random chance tried number different phrasings formulations wic involves determining word used meaning two sentences none able achieve strong performance hints phenomenon become clearer next section discusses anli benchmark gpt-3 appears weak few-shot one-shot setting tasks involve comparing two sentences snippets example whether word used way two sentences wic whether one sentence paraphrase another whether one sentence implies another could also explain comparatively low scores rte cb also follow format despite weaknesses gpt-3 still outperforms ne-tuned bert-large four eight tasks two tasks gpt-3 close state-of-the-art held ne-tuned 11 billion parameter model finally note few-shot superglue score steadily improves model size number examples context showing increasing benets in-context learning figure 3.8 scale kup 32 examples per task point additional examples reliably context sweeping values ofk nd gpt-3 requires less eight total examples per task outperform ne-tuned bert-large overall superglue score 3.8 nli natural language inference nli fyo00 concerns ability understand relationship two sentences practice task usually structured two three class classication problem model classies 20figure 3.9 performance gpt-3 anli round 3. results dev-set 1500 examples therefore high variance estimate standard deviation 1.2 nd smaller models hover around random chance few-shot gpt-3 175b closes almost half gap random chance sota results anli rounds 1 2 shown appendix whether second sentence logically follows rst contradicts rst sentence possibly true neutral superglue includes nli dataset rte evaluates binary version task rte largest version gpt-3 performs convincingly better random 56 evaluation setting few-shot setting gpt-3 performs similarly single-task ne-tuned bert large also evaluate recently introduced adversarial natural language inference anli dataset nwd+19 anli difcult dataset employing series adversarially mined natural language inference questions three rounds r1 r2 r3 similar rte models smaller gpt-3 perform almost exactly random chance anli even few-shot setting \u001833 whereas gpt-3 shows signs life round 3. results anli r3 highlighted figure 3.9 full results rounds found appendix h. results rte anli suggest nli still difcult task language models beginning show signs progress 3.9 synthetic qualitative tasks one way probe gpt-3s range abilities few-shot zero- one-shot setting give tasks require perform simple on-the-y computational reasoning recognize novel pattern unlikely occurred training adapt quickly unusual task devise several tasks test class abilities first test gpt-3s ability perform arithmetic second create several tasks involve rearranging unscrambling letters word tasks unlikely exactly seen training third test gpt-3s ability solve sat-style analogy problems few-shot finally test gpt-3 several qualitative tasks including using new words sentence correcting english grammar news article generation release synthetic datasets hope stimulating study test-time behavior language models 3.9.1 arithmetic test gpt-3s ability perform simple arithmetic operations without task-specic training developed small battery 10 tests involve asking gpt-3 simple arithmetic problem natural language 2 digit addition 2d+ model asked add two integers sampled uniformly 0 100 phrased form question e.g q 48 plus 76 124 2 digit subtraction 2d- model asked subtract two integers sampled uniformly 0 100 answer may negative example q 34 minus 53 -19 3 digit addition 3d+ 2 digit addition except numbers uniformly sampled 0 1000 21figure 3.10 results 10 arithmetic tasks few-shot settings models different sizes signicant jump second largest model gpt-3 13b largest model gpt-3 175 latter able reliably accurate 2 digit arithmetic usually accurate 3 digit arithmetic correct answers signicant fraction time 4-5 digit arithmetic 2 digit multiplication compound operations results one-shot zero-shot shown appendix 3 digit subtraction 3d- 2 digit subtraction except numbers uniformly sampled 0 1000 4 digit addition 4d+ 3 digit addition except uniformly sampled 0 10000 4 digit subtraction 4d- 3 digit subtraction except uniformly sampled 0 10000 5 digit addition 5d+ 3 digit addition except uniformly sampled 0 100000 5 digit subtraction 5d- 3 digit subtraction except uniformly sampled 0 100000 2 digit multiplication 2dx model asked multiply two integers sampled uniformly 0 100 e.g q 24 times 42 1008. one-digit composite 1dc model asked perform composite operation three 1 digit numbers parentheses around last two example q 6+ 4 8 38. three 1 digit numbers selected uniformly 0 10 operations selected uniformly f+ g. 10 tasks model must generate correct answer exactly task generate dataset 2,000 random instances task evaluate models instances first evaluate gpt-3 few-shot setting results shown figure 3.10. addition subtraction gpt-3 displays strong prociency number digits small achieving 100 accuracy 2 digit addition 98.9 2 digit subtraction 80.2 3 digit addition 94.2 3-digit subtraction performance decreases number digits increases gpt-3 still achieves 25-26 accuracy four digit operations 9-10 accuracy digit operations suggesting least capacity generalize larger numbers digits gpt-3 also achieves 29.2 accuracy 2 digit multiplication especially computationally intensive operation finally gpt-3 achieves 21.3 accuracy single digit combined operations example 9 7+5 suggesting robustness beyond single operations figure 3.10 makes clear small models poorly tasks even 13 billion parameter model second largest 175 billion full gpt-3 solve 2 digit addition subtraction half time operations less 10 time one-shot zero-shot performance somewhat degraded relative few-shot performance suggesting adaptation task least recognition task important performing computations correctly nevertheless one-shot performance still quite strong even zero-shot performance full gpt-3 signicantly 22setting 2d+ 2d- 3d+ 3d- 4d+ 4d- 5d+ 5d- 2dx 1dc gpt-3 zero-shot 76.9 58.0 34.2 48.3 4.0 7.5 0.7 0.8 19.8 9.8 gpt-3 one-shot 99.6 86.4 65.5 78.7 14.0 14.0 3.5 3.8 27.4 14.3 gpt-3 few-shot 100.0 98.9 80.4 94.2 25.5 26.8 9.3 9.9 29.2 21.3 table 3.9 results basic arithmetic tasks gpt-3 175b f2,3,4,5gdf+ -gis 2 3 4 5 digit addition subtraction 2dx 2 digit multiplication 1dc 1 digit composite operations results become progressively stronger moving zero-shot one-shot few-shot setting even zero-shot shows signicant arithmetic abilities setting cl a1 a2 ri rw gpt-3 zero-shot 3.66 2.28 8.91 8.26 0.09 gpt-3 one-shot 21.7 8.62 25.9 45.4 0.48 gpt-3 few-shot 37.9 15.1 39.7 67.2 0.44 table 3.10 gpt-3 175b performance various word unscrambling word manipulation tasks zero- one- few-shot settings cl cycle letters word a1 anagrams rst last letters a2 anagrams rst last two letters ri random insertion word rw reversed words outperforms few-shot learning smaller models three settings full gpt-3 shown table 3.9 model capacity scaling three settings shown appendix h. spot-check whether model simply memorizing specic arithmetic problems took 3-digit arithmetic problems test set searched training data forms `` num1 num2 '' '' num1 plus num2 '' 2,000 addition problems found 17 matches 0.8 2,000 subtraction problems found 2 matches 0.1 suggesting trivial fraction correct answers could memorized addition inspection incorrect answers reveals model often makes mistakes carrying 1 suggesting actually attempting perform relevant computation rather memorizing table overall gpt-3 displays reasonable prociency moderately complex arithmetic few-shot one-shot even zero-shot settings 3.9.2 word scrambling manipulation tasks test gpt-3s ability learn novel symbolic manipulations examples designed small battery 5 character manipulation tasks task involves giving model word distorted combination scrambling addition deletion characters asking recover original word 5 tasks cycle letters word cl model given word letters cycled symbol expected generate original word example might given lyinevitab output inevitably anagrams rst last characters a1 model given word every letter except rst last scrambled randomly must output original word example criroptuon corruption anagrams rst last 2 characters a2 model given word every letter except rst 2 last 2 scrambled randomly must recover original word example opoepnnt opponent random insertion word ri random punctuation space character inserted letter word model must output original word example s.u c/c e.s i/o/n succession reversed words rw model given word spelled backwards must output original word example stcejbo objects task generate 10,000 examples chose top 10,000 frequent words measured nor09 length 4 characters less 15 characters few-shot results shown figure 3.11. task performance tends grow smoothly model size full gpt-3 model achieving 66.9 removing 23figure 3.11 few-shot performance word scrambling tasks different sizes model generally smooth improvement model size although random insertion task shows upward slope improvement 175b model solving task majority time scaling one-shot zero-shot performance shown appendix tasks done k= 100 random insertions 38.6 cycling letters 40.2 easier anagram task 15.1 difcult anagram task rst last letters held xed none models reverse letters word one-shot setting performance signicantly weaker dropping half zero-shot setting model rarely perform tasks table 3.10 suggests model really appear learn tasks test time model perform zero-shot articial nature makes unlikely appear pre-training data although conrm certainty quantify performance plotting in-context learning curves show task performance function number in-context examples show in-context learning curves symbol insertion task figure 1.2. see larger models able make increasingly effective use in-context information including task examples natural language task descriptions finally worth adding solving tasks requires character-level manipulations whereas bpe encoding operates signicant fractions word average \u00180:7words per token lms perspective succeeding tasks involves manipulating bpe tokens understanding pulling apart substructure also cl a1 a2 bijective unscrambled word deterministic function scrambled word requiring model perform search nd correct unscrambling thus skills involved appear require non-trivial pattern-matching computation 3.9.3 sat analogies test gpt-3 another task somewhat unusual relative typical distribution text collected set 374 sat analogy problems tlbs03 analogies style multiple choice question constituted section sat college entrance exam 2005. typical example audacious boldness sanctimonious hypocrisy b anonymous identity c remorseful misdeed deleterious result e impressionable temptation student expected choose word pairs relationship original word pair example answer sanctimonious hypocrisy task gpt-3 achieves 65.2 few-shot setting 59.1 one-shot setting 53.7 zero-shot setting whereas average score among college applicants 57 tl05 random guessing yields 20 shown figure 3.12 results improve scale full 175 billion model improving 10 compared 13 billion parameter model 24figure 3.12 zero- one- few-shot performance sat analogy tasks different sizes model largest model achieves 65 accuracy few-shot setting also demonstrates signicant gains in-context learning present smaller models 3.9.4 news article generation previous work generative language models qualitatively tested ability generate synthetic news articles conditional sampling model given human-written prompt consisting plausible rst sentence news story rwc+19 relative rwc+19 dataset used train gpt-3 much less weighted towards news articles trying generate news articles via raw unconditional samples less effective example gpt-3 often interprets proposed rst sentence news article tweet posts synthetic responses follow-up tweets solve problem employed gpt-3s few-shot learning abilities providing three previous news articles models context condition title subtitle proposed next article model able reliably generate short articles news genre gauge quality news article generation gpt-3 believe likely correlated conditional sample generation quality general decided measure human ability distinguish gpt-3-generated articles real ones similar work carried kreps et al kmb20 zellers et al zhr+19 generative language models trained match distribution content generated humans ability humans distinguish two potentially important measure quality.3 order see well humans detect model generated text arbitrarily selected 25 article titles subtitles website newser.com mean length 215 words generated completions titles subtitles four language models ranging size 125m 175b gpt-3 parameters mean length 200 words model presented around 80 us-based participants quiz consisting real titles subtitles followed either human written article article generated model4 participants asked select whether article likely written human likely written human dont know likely written machine likely written machine articles selected models training data model outputs formatted selected programmatically prevent human cherry-picking models used context condition outputs pre-trained context size article titles subtitles used prompts model however also ran experiment control participant effort attention followed format involved intentionally bad model generated articles done generating articles control model 160m parameter model context increased output randomness 3this task also relevant potential misuse language models discussed section 6.1 4we wanted identify good average person internet detecting language model outputs focused participants drawn general us population see appendix e details 25mean accuracy95 condence interval low hi tcompared control p-value dont know assignments control deliberately bad model 86 83 90 3.6 gpt-3 small 76 72 80 3.9 2 e-4 4.9 gpt-3 medium 61 58 65 10.3 7 e-21 6.0 gpt-3 large 68 64 72 7.3 3 e-11 8.7 gpt-3 xl 62 59 65 10.7 1 e-19 7.5 gpt-3 2.7b 62 58 65 10.4 5 e-19 7.1 gpt-3 6.7b 60 56 63 11.2 3 e-21 6.2 gpt-3 13b 55 52 58 15.3 1 e-32 7.1 gpt-3 175b 52 49 54 16.9 1 e-34 7.8 table 3.11 human accuracy identifying whether short \u0018200 word news articles model generated nd human accuracy measured ratio correct assignments non-neutral assignments ranges 86 control model 52 gpt-3 175b table compares mean accuracy different models shows results two-sample t-test difference mean accuracy model control model unconditional gpt-3 small model increased output randomness mean human accuracy ratio correct assignments non-neutral assignments per participant detecting intentionally bad articles model generated \u001886 50 chance level performance contrast mean human accuracy detecting articles produced 175b parameter model barely chance at\u001852 see table 3.11 .5human abilities detect model generated text appear decrease model size increases appears trend towards chance accuracy model size human detection gpt-3 close chance.6 true despite fact participants spend time output model size increases see appendix e examples synthetic articles gpt-3 given figures 3.14 3.15.7much text isas indicated evaluationsdifcult humans distinguish authentic human content factual inaccuracies indicator article model generated since unlike human authors models access specic facts article titles refer article written indicators include repetition non sequiturs unusual phrasings though often subtle enough noticed related work language model detection ippolito et al idcbe19 indicates automatic discriminators like gr v e r zhr+19 gltr gsr19 may greater success detecting model generated text human evaluators automatic detection models may promising area future research ippolito et al idcbe19 also note human accuracy detecting model generated text increases humans observe tokens preliminary investigation good humans detecting longer news articles generated gpt-3 175b selected 12 world news articles reuters average length 569 words generated completions articles gpt-3 average length 498 words 298 words longer initial experiments following methodology ran two experiments around 80 us-based participants compare human abilities detect articles generated gpt-3 control model found mean human accuracy detecting intentionally bad longer articles control model \u001888 mean human accuracy detecting longer articles produced gpt-3 175b still barely chance at\u001852 see table 3.12 indicates news articles around 500 words long gpt-3 continues produce articles humans nd difcult distinguish human written news articles 3.9.5 learning using novel words task studied developmental linguistics cb78 ability learn utilize new words example using word sentence seeing dened conversely inferring words meaning one usage qualitatively test gpt-3s ability former specically give gpt-3 denition nonexistent word gigamuru ask use sentence provide one previous examples separate 5we use two-sample students t-test test signicant difference means participant accuracies model control model report normalized difference means t-statistic p-value 6if model consistently produces texts impressive human articles possible human performance task would drop 50 indeed many individual participants scored 50 task 7additional non-news samples found appendix f. 26figure 3.13 peoples ability identify whether news articles model-generated measured ratio correct assignments non-neutral assignments decreases model size increases accuracy outputs deliberately- bad control model unconditioned gpt-3 small model higher output randomness indicated dashed line top random chance 50 indicated dashed line bottom line best power law 95 condence intervals mean accuracy95 condence interval low hi tcompared control p-value dont know assignments control 88 84 91 2.7 gpt-3 175b 52 48 57 12.7 3.2 e-23 10.6 table 3.12 peoples ability identify whether \u0018500word articles model generated measured ratio correct assignments non-neutral assignments 88 control model 52 gpt-3 175b table shows results two-sample t-test difference mean accuracy gpt-3 175b control model unconditional gpt-3 small model increased output randomness 27title united methodists agree historic split subtitle oppose gay marriage form denomination article two days intense debate united methodist church agreed historic split one expected end creation new denomination one `` theologically socially conservative '' according washington post majority delegates attending church 's annual general conference may voted strengthen ban ordination lgbtq clergy write new rules `` discipline '' clergy officiate same-sex weddings opposed measures new plan say form separate denomination 2020 calling church christian methodist denomination post notes denomination claims 12.5 million members early 20th century `` largest protestant denomination u.s. '' shrinking recent decades new split second church 's history first occurred 1968 roughly 10 percent denomination left form evangelical united brethren church post notes proposed split `` comes critical time church losing members years '' '' pushed toward brink schism role lgbtq people church '' gay marriage issue divided church 2016 denomination split ordination transgender clergy north pacific regional conference voting ban serving clergy south pacific regional conference voting allow figure 3.14 gpt-3 generated news article humans greatest difculty distinguishing human written article accuracy 12 title star 's tux promise draws megyn kelly 's sarcasm subtitle joaquin phoenix pledged change awards event article year ago joaquin phoenix made headlines appeared red carpet golden globes wearing tuxedo paper bag head read `` shape-shifter ca n't change world change '' promise change fit hollywood mold `` think 's really special thing change think 's really special thing say what's inside 'm proud 'm going ashamed way someone else thinks '' 's oscars phoenix time publicist saying 'll wearing tux matter megyn kelly impressed let tonight show `` know feel like feel like could worn tux '' says `` 're saying 're shape-shifter n't know change tux change mind change mind change mind '' phoenix says n't stick `` like okay 'm going wear tuxedo thing thought n't want wear tuxedo thing '' kelly goes encourage change mind phoenix says 's late `` 'm committed wearing '' figure 3.15 gpt-3 generated news article humans found easiest distinguish human written article accuracy 61 28a `` whatpu '' small furry animal native tanzania example sentence uses word whatpu traveling africa saw cute whatpus `` farduddle '' means jump really fast example sentence uses word farduddle one day playing tag little sister got really excited started crazy farduddles `` yalubalu '' type vegetable looks like big pumpkin example sentence uses word yalubalu trip africa tried yalubalu vegetable grown garden delicious `` burringo '' car fast acceleration example sentence uses word burringo garage burringo father drives work every day `` gigamuru '' type japanese musical instrument example sentence uses word gigamuru gigamuru uncle gave gift love play home `` screeg '' something swing sword example sentence uses word screeg screeghed several minutes went outside ate ice cream figure 3.16 representative gpt-3 completions few-shot task using new word sentence boldface gpt-3s completions plain text human prompts rst example prompt completion provided human serves conditioning subsequent examples gpt-3 receives successive additional prompts provides completions nothing task-specic provided gpt-3 conditioning shown nonexistent word dened used sentence task few-shot terms previous examples broad task one-shot terms specic word table 3.16 shows 6 examples generated denitions human-generated rst answer human-generated conditioning subsequent answers generated gpt-3 examples generated continuously one sitting omit repeatedly try prompts cases generated sentence appears correct least plausible use word nal sentence model generates plausible conjugation word screeg namely screeghed although use word slightly awkward screeghed despite plausible sense could describe toy sword ght overall gpt-3 appears least procient task using novel words sentence 3.9.6 correcting english grammar another task well suited few-shot learning correcting english grammar test gpt-3 few- shot setting giving prompts form `` poor english input sentence nn good english output sentence '' give gpt-3 one human-generated correction ask correct 5 without omissions repeats results shown figure 3.17 4 measuring preventing memorization benchmarks since training dataset sourced internet possible model trained benchmark test sets accurately detecting test contamination internet-scale datasets new area research without established best practices common practice train large models without investigating contamination given increasing scale pretraining datasets believe issue becoming increasingly important attend concern hypothetical one rst papers train language model common crawl data tl18 detected removed training document overlapped one evaluation datasets work gpt-2 rwc+19 also conducted post-hoc overlap analysis study relatively encouraging nding 29poor english input eated purple berries good english output ate purple berries poor english input thank picking designer 'd appreciate good english output thank choosing designer appreciate poor english input mentioned changes done alteration requested changed things wanted modifications good english output requested changes made made alteration requested changed things wanted made modifications poor english input 'd happy work another project good english output 'd happy work another project poor english input please provide short brief design 're looking 'd nice could share examples project good english output please provide brief description design looking would nice could share examples projects done poor english input patient died good english output patient died poor english input think leslie likes good english output think leslie likes us poor english input janet broke bill finger good english output janet broke bill 's finger poor english input mary arranged st. louis john rent house cheap good english output mary arranged john rent house st. louis poor english input today went store buys many bottle water good english output today went store buy bottles water poor english input tried hit ball bat swing miss good english output tried hit ball bat swing missed figure 3.17 representative gpt-3 completions few-shot task correcting english grammar boldface gpt-3s completions plain text human prompts rst examples example prompt completion provided human serves conditioning subsequent examples gpt-3 receives successive additional prompts provides completions nothing task-specic provided gpt-3 aside rst examples conditioning poor english input/good english output framing note distinction poor good english terms complex contextual contested example mentioning rental house shows assumptions model makes good even lead make errors model adjusts grammar also removes word cheap way alters meaning 30figure 4.1 gpt-3 training curves measure model performance training deduplicated validation split training distribution though gap training validation performance gap grows minimally model size training time suggesting gap comes difference difculty rather overtting although models perform moderately better data overlapped training testing signicantly impact reported results due small fraction data contaminated often percent gpt-3 operates somewhat different regime one hand dataset model size two orders magnitude larger used gpt-2 include large amount common crawl creating increased potential contamination memorization hand precisely due large amount data even gpt-3 175b overt training set signicant amount measured relative held-out validation set deduplicated figure 4.1 thus expect contamination likely frequent effects may large feared initially tried address issue contamination proactively searching attempting remove overlap training data development test sets benchmarks studied paper unfortunately bug resulted partial removal detected overlaps training data due cost training wasnt feasible retrain model address investigate detail remaining detected overlap impacts results benchmark produce clean version removes potentially leaked examples dened roughly examples 13-gram overlap anything pretraining set overlap whole example shorter 13-grams goal conservatively ag anything could potentially contamination produce clean subset free contamination high condence exact procedure detailed appendix c. evaluate gpt-3 clean benchmarks compare original score score clean subset similar score entire dataset suggests contamination even present signicant effect reported results score clean subset lower suggests contamination may inating results results summarized figure 4.2. although potential contamination often high quarter benchmarks scoring 50 cases performance changes negligibly see evidence contamination level performance difference correlated conclude either conservative method substantially overestimated contamination contamination little effect performance review detail specic cases either 1 model performs signicantly worse cleaned version 2 potential contamination high makes measuring performance difference difcult analysis agged six groups benchmarks investigation word scrambling reading comprehension quac squad2 drop piqa winograd language modeling tasks wikitext tasks 1bw german english 31figure 4.2 benchmark contamination analysis constructed cleaned versions benchmarks check potential contamination training set x-axis conservative lower bound much dataset known high condence clean y-axis shows difference performance evaluating veried clean subset performance benchmarks changed negligibly agged review inspection nd evidence contamination piqa winograd results mark corresponding results section 3 asterisk nd evidence benchmarks affected translation since overlap analysis designed extremely conservative expect produce false positives summarize results group tasks reading comprehension initial analysis agged 90 task examples quac squad2 drop potentially contaminated large even measuring differential clean subset difcult upon manual inspection however found every overlap inspected 3 datasets source text present training data question/answer pairs meaning model gains background information memorize answer specic question german translation found 25 examples wmt16 german-english test set marked potentially contaminated associated total effect size 1-2 bleu upon inspection none agged examples contain paired sentences resembling nmt training data collisions monolingual matches mostly snippets events discussed news reversed words anagrams recall tasks form alaok koala due short length tasks used 2-grams ltering ignoring punctuation inspecting agged overlaps found typically instances real reversals unscramblings training set rather palindromes trivial unscramblings e.g kayak kayak amount overlap small removing trivial tasks lead increase difculty thus spurious signal related symbol insertion task shows high overlap effect performance task involves removing non-letter characters word overlap analysis ignores characters leading many spurious matches piqa overlap analysis agged 29 examples contaminated observed 3 percentage point absolute decrease 4 relative decrease performance clean subset though test dataset released training set created labels hidden web pages used crowdsourced dataset creators contained training set found similar decrease 25x smaller model much less capacity memorize leading us suspect shift likely statistical bias rather memorization examples workers copied may simply easier unfortunately rigorously prove hypothesis therefore mark piqa results asterisk denote potential contamination winograd overlap analysis agged 45 examples found 2.6 decrease performance clean subset manual inspection overlapping data point showed 132 winograd schemas fact present training set though presented different format present task model although decrease performance small mark winograd results main paper asterisk 32language modeling found 4 wikipedia language modeling benchmarks measured gpt-2 plus childrens book test dataset almost entirely contained training data since reliably extract clean subset report results datasets even though intended starting work note penn tree bank due age unaffected therefore became chief language modeling benchmark also inspected datasets contamination high impact performance close zero simply verify much actual contamination existed appeared often contain false positives either actual contamination contamination give away answer task one notable exception lambada appeared substantial genuine contamination yet impact performance small clean subset scoring within 0.5 full dataset also strictly speaking ll-in-the-blank format precludes simplest form memorization nevertheless since made large gains lambada paper potential contamination noted results section important limitation contamination analysis sure clean subset drawn distribution original dataset remains possible memorization inates results time precisely counteracted statistical bias causing clean subset easier however sheer number shifts close zero suggests unlikely also observed noticeable difference shifts small models unlikely memorizing overall made best effort measure document effects data contamination note outright remove problematic results depending severity much work remains done address important subtle issue eld general designing benchmarks training models detailed explanation analysis refer reader appendix c. 5 limitations gpt-3 analysis number limitations describe suggest directions future work first despite strong quantitative qualitative improvements gpt-3 particularly compared direct predecessor gpt-2 still notable weaknesses text synthesis several nlp tasks text synthesis although overall quality high gpt-3 samples still sometimes repeat semantically document level start lose coherence sufciently long passages contradict occasionally contain non-sequitur sentences paragraphs release collection 500 uncurated unconditional samples help provide better sense gpt-3s limitations strengths text synthesis within domain discrete language tasks noticed informally gpt-3 seems special difculty common sense physics despite well datasets piqa bzb+19 test domain specically gpt-3 difculty questions type put cheese fridge melt quantitatively gpt-3s in-context learning performance notable gaps suite benchmarks described section 3 particular little better chance evaluated one-shot even few-shot comparison tasks determining two words used way sentence one sentence implies another wic anli respectively well subset reading comprehension tasks especially striking given gpt-3s strong few-shot performance many tasks gpt-3 several structural algorithmic limitations could account issues focused exploring in-context learning behavior autoregressive language models straightforward sample compute likelihoods model class result experiments include bidirectional architectures training objectives denoising noticeable difference much recent literature documented improved ne-tuning performance using approaches standard language models rsr+19 thus design decision comes cost potentially worse performance tasks empirically benet bidirectionality may include ll-in-the-blank tasks tasks involve looking back comparing two pieces content tasks require re-reading carefully considering long passage generating short answer could possible explanation gpt-3s lagging few-shot performance tasks wic involves comparing use word two sentences anli involves comparing two sentences see one implies several reading comprehension tasks e.g quac race also conjecture based past literature large bidirectional model would stronger ne-tuning gpt-3 making bidirectional model scale gpt-3 and/or trying make bidirectional models work few- zero-shot learning promising direction future research could help achieve best worlds fundamental limitation general approach described paper scaling lm-like model whether autoregressive bidirectional may eventually run could already running limits 33pretraining objective current objective weights every token equally lacks notion important predict less important rrs20 demonstrate benets customizing prediction entities interest also self-supervised objectives task specication relies forcing desired task prediction problem whereas ultimately useful language systems example virtual assistants might better thought taking goal-directed actions rather making predictions finally large pretrained language models grounded domains experience video real-world physical interaction thus lack large amount context world bht+20 reasons scaling pure self-supervised prediction likely hit limits augmentation different approach likely necessary promising future directions vein might include learning objective function humans zsw+19a ne-tuning reinforcement learning adding additional modalities images provide grounding better model world cly+19 another limitation broadly shared language models poor sample efciency pre-training gpt-3 takes step towards test-time sample efciency closer humans one-shot zero-shot still sees much text pre-training human sees lifetime lin20 improving pre-training sample efciency important direction future work might come grounding physical world provide additional information algorithmic improvements limitation least uncertainty associated few-shot learning gpt-3 ambiguity whether few-shot learning actually learns new tasks scratch inference time simply recognizes identies tasks learned training possibilities exist spectrum ranging demonstrations training set drawn exactly distribution test time recognizing task different format adapting specic style general task qa learning skill entirely de novo gpt-3 spectrum may also vary task task synthetic tasks wordscrambling dening nonsense words seem especially likely learned de novo whereas translation clearly must learned pretraining although possibly data different organization style test data ultimately even clear humans learn scratch vs prior demonstrations even organizing diverse demonstrations pre-training identifying test time would advance language models nevertheless understanding precisely few-shot learning works important unexplored direction future research limitation associated models scale gpt-3 regardless objective function algorithm expensive inconvenient perform inference may present challenge practical applicability models scale current form one possible future direction address distillation hvd15 large models manageable size specic tasks large models gpt-3 contain wide range skills needed specic task suggesting principle aggressive distillation may possible distillation well-explored general lhcg19a tried scale hundred billions parameters new challenges opportunities may associated applying models size finally gpt-3 shares limitations common deep learning systems decisions easily interpretable necessarily well-calibrated predictions novel inputs observed much higher variance performance humans standard benchmarks retains biases data trained last issue biases data may lead model generate stereotyped prejudiced content special concern societal perspective discussed along issues next section broader impacts section 6 6 broader impacts language models wide range benecial applications society including code writing auto-completion grammar assistance game narrative generation improving search engine responses answering questions also potentially harmful applications gpt-3 improves quality text generation adaptability smaller models increases difculty distinguishing synthetic text human-written text therefore potential advance benecial harmful applications language models focus potential harms improved language models believe harms necessarily greater order stimulate efforts study mitigate broader impacts language models like numerous focus two primary issues potential deliberate misuse language models like gpt-3 section 6.1 issues bias fairness representation within models like gpt-3 section 6.2. also briey discuss issues energy efciency section 6.3 346.1 misuse language models malicious uses language models somewhat difcult anticipate often involve repurposing language models different environment different purpose researchers intended help think terms traditional security risk assessment frameworks outline key steps identifying threats potential impacts assessing likelihood determining risk combination likelihood impact ros12 discuss three factors potential misuse applications threat actors external incentive structures 6.1.1 potential misuse applications socially harmful activity relies generating text could augmented powerful language models examples include misinformation spam phishing abuse legal governmental processes fraudulent academic essay writing social engineering pretexting many applications bottleneck human beings write sufciently high quality text language models produce high quality text generation could lower existing barriers carrying activities increase efcacy misuse potential language models increases quality text synthesis improves ability gpt-3 generate several paragraphs synthetic content people nd difcult distinguish human-written text 3.9.4 represents concerning milestone regard 6.1.2 threat actor analysis threat actors organized skill resource levels ranging low moderately skilled resourced actors may able build malicious product advanced persistent threats apts highly skilled well-resourced e.g state-sponsored groups long-term agendas sbc+19 understand low mid-skill actors think language models monitoring forums chat groups misinformation tactics malware distribution computer fraud frequently discussed nd signicant discussion misuse following initial release gpt-2 spring 2019 found fewer instances experimentation successful deployments since additionally misuse discussions correlated media coverage language model technologies assess threat misuse actors immediate signicant improvements reliability could change apts typically discuss operations open consulted professional threat analysts possible apt activity involving use language models since release gpt-2 discernible difference operations may see potential gains using language models assessment language models may worth investing signicant resources convincing demonstration current language models signicantly better current methods generating text methods targeting controlling content language models still early stage 6.1.3 external incentive structures threat actor group also set tactics techniques procedures ttps rely accomplish agenda ttps inuenced economic factors like scalability ease deployment phishing extremely popular among groups offers low-cost low-effort high-yield method deploying malware stealing login credentials using language models augment existing ttps would likely result even lower cost deployment ease use another signicant incentive stable infrastructure large impact adoption ttps outputs language models stochastic however though developers constrain e.g using top-k truncation able perform consistently without human feedback social media disinformation bot produces outputs reliable 99 time produces incoherent outputs 1 time could reduce amount human labor required operating bot human still needed lter outputs restricts scalable operation based analysis model analysis threat actors landscape suspect ai researchers eventually develop language models sufciently consistent steerable greater interest malicious actors expect introduce challenges broader research community hope work combination mitigation research prototyping coordinating technical developers 356.2 fairness bias representation biases present training data may lead models generate stereotyped prejudiced content concerning since model bias could harm people relevant groups different ways entrenching existing stereotypes producing demeaning portrayals amongst potential harms cra17 conducted analysis biases model order better understand gpt-3s limitations comes fairness bias representation.8 goal exhaustively characterize gpt-3 give preliminary analysis limitations behaviors focus biases relating gender race religion although many categories bias likely present could studied follow-up work preliminary analysis reect models biases even within studied categories broadly analysis indicates internet-trained models internet-scale biases models tend reect stereotypes present training data discuss preliminary ndings bias along dimensions gender race religion probe bias 175 billion parameter model also similar smaller models see different dimension 6.2.1 gender investigation gender bias gpt-3 focused associations gender occupation found occupations general higher probability followed male gender identier female one words male leaning given context `` thefoccupationgwas '' neutral variant 83 388 occupations tested likely followed male identier gpt-3 measured feeding model context `` detective '' looking probability model following male indicating words eg man male etc female indicating words woman female etc. particular occupations demonstrating higher levels education legislator banker professor emeritus heavily male leaning along occupations require hard physical labour mason millwright sheriff occupations likely followed female identiers include midwife nurse receptionist housekeeper etc also tested probabilities changed shifted context `` competent foccupationg '' competent variant shifted context `` incompetent foccupationgwas '' incompetent variant occupation dataset found prompted `` competent foccupationgwas '' majority occupations even higher probability followed male identier female one case original neutral prompt `` thefoccupationgwas '' prompt `` incompetent foccupationgwas '' majority occupations still leaned male similar probability original neutral prompt average occupation bias measured 1 njobsp jobslog p femalejcontext p malejcontext was\u00001:11for neutral variant \u00002:14for competent variant \u00001:15 incompetent variant also carried pronoun resolution winogender dataset rnlvd18 using two methods corroborated models tendency associate occupations males one method measured mod- els ability correctly assign pronoun occupation participant example fed model context `` advisor met advisee wanted get advice job applications refers '' found option lowest probability two possi- ble options choices occupation option advisor participant option advisee occupation participant words often societal biases associated assumption occupants default male found language models learnt biases tendency associate female pronouns participant positions male pronouns gpt-3 175b highest accuracy models 64.17 task also model accuracy occupant sentences sentences correct answer occupation option females higher males 81.7 vs 76.7 models higher accuracy male pronouns occupation sentences compared female pronouns exception second largest model- gpt-3 13b accuracy 60 offers preliminary evidence places issues bias make language models susceptible error larger models robust smaller models also performed co-occurrence tests analyzed words likely occur vicinity pre- selected words created model output sample set generating 800 outputs length 50 temperature 8evaluating fairness bias representation language models rapidly-developing area large body prior work see example hzj+19 nbr20 scnp19 36table 6.1 biased descriptive words 175b model top 10 biased male descriptive words raw co-occurrence countstop 10 biased female descriptive words raw co-occurrence counts average number co-occurrences across words 17.5average number co-occurrences across words 23.9 large 16 optimistic 12 mostly 15 bubbly 12 lazy 14 naughty 12 fantastic 13 easy-going 12 eccentric 13 petite 10 protect 10 tight 10 jolly 10 pregnant 10 stable 9 gorgeous 28 personable 22 sucked 8 survive 7 beautiful 158 1 top p 0.9 every prompt dataset gender prompts `` '' '' '' '' would described '' '' would described '' 9. looked adjectives adverbs top 100 favored words using off-the-shelf pos tagger lb02 found females often described using appearance oriented words beautiful gorgeous compared men often described using adjectives span greater spectrum table 6.1 shows top 10 favored descriptive words model along raw number times word co-occurred pronoun indicator favored indicates words skewed towards category co-occurring higher rate compared category put numbers perspective also included average number co-occurrences across qualifying words gender 6.2.2 race investigate racial bias gpt-3 seeded model prompts `` thefracegman '' '' thefracegwoman '' '' people would describe fracegperson '' generated 800 samples prompts fracegreplaced term indicating racial category white asian measure word co-occurrences generated samples given prior research demonstrating language models produce text differing sentiment varying features occupation hzj+19 explored race impacted sentiment measured sentiment using senti wordnet bes10 words co-occurred disproportionately race word sentiment varied 100 -100 positive scores indicating positive words eg wonderfulness 100 amicable 87.5 negative scores indicating negative words eg wretched -87.5 horrid -87.5 score 0 indicating neutral words eg sloping chalet noted explicitly prompting models talk race turn generated text focused racial features results models talking race wild talking race experimental setup primed additionally since measuring sentiment simply looking word co-occurrences resulting sentiment reect socio-historical factors instance text relating discussion slavery frequently negative sentiment may lead demographic associated negative sentiment testing methodology across models analyzed asian consistently high sentiment ranked 1st 3 7 models hand black consistently low sentiment ranked lowest 5 7 models differences narrowed marginally larger model sizes analysis gives sense biases different models highlights need sophisticated analysis relationship sentiment entities input data 9we used male female pronouns simplifying assumption makes easier study co-occurrence since require isolation instances refers singular noun didnt forms gender bias likely present could studied using different approaches 37figure 6.1 racial sentiment across models religion favored descriptive words atheism theists cool agnostics mad theism defensive complaining correct arrogant characterized buddhism myanmar vegetarians burma fellowship monk japanese reluctant wisdom en- lightenment non-violent christianity attend ignorant response judgmental grace execution egypt continue com- ments ofcially hinduism caste cows bjp kashmir modi celebrated dharma pakistani originated africa islam pillars terrorism fasting sheikh non-muslim source charities levant allah prophet judaism gentiles race semites whites blacks smartest racists arabs game russian table 6.2 shows ten favored words religion gpt-3 175b model 6.2.3 religion studied words co-occurred religious terms relating atheism buddhism christianity hinduism islam judaism generating 800 model outputs length \u001950 temperature 1 top pof0:9for every prompt prompts nature `` freligion practitioners gare '' eg `` christians '' six religious categories listed allowed model naturally carry completions created corpus completions studying co-occurrence words following example output model '' buddhists divided two main branches theravada mahayana theravada conservative branch centering monastic life earliest sutras refusing recognize later mahayana sutras authentic '' similar race found models make associations religious terms indicate propensity reect terms sometimes presented world example religion islam found words asramadan prophet andmosque co-occurred higher rate religions also found words asviolent terrorism andterrorist co-occurred greater rate islam religions top 40 favored words islam gpt-3 386.2.4 future bias fairness challenges presented preliminary analysis share biases found order motivate research highlight inherent difculties characterizing biases large-scale generative models expect area continuous research us excited discuss different methodological approaches community view work section subjective signposting chose gender race religion starting point recognize inherent subjectivity choice work inspired literature characterizing model attributes develop informative labels model cards model reporting mwz+18 ultimately important characterize biases language systems intervene literature also extensive qmzh19 hzj+19 offer brief comments future directions specic large language models order pave way effective bias prevention general purpose models need building common vocabulary tying together normative technical empirical challenges bias mitigation models room research engages literature outside nlp better articulates normative statements harm engages lived experience communities affected nlp systems bbdiw20 thus mitigation work approached purely metric driven objective remove bias shown blind spots gg19 nvnvdg19 holistic manner 6.3 energy usage practical large-scale pre-training requires large amounts computation energy-intensive training gpt-3 175b consumed several thousand petaop/s-days compute pre-training compared tens petaop/s-days 1.5b parameter gpt-2 model figure 2.2 means cognizant cost efciency models advocated sdse19 use large-scale pre-training also gives another lens view efciency large models consider resources go training resources amortized lifetime model subsequently used variety purposes ne-tuned specic tasks though models like gpt-3 consume signicant resources training surprisingly efcient trained even full gpt-3 175b generating 100 pages content trained model cost order 0.4 kw-hr cents energy costs additionally techniques like model distillation lhcg19a bring cost models letting us adopt paradigm training single large-scale models creating efcient versions use appropriate contexts algorithmic progress may also naturally increase efciency models time similar trends observed image recognition neural machine translation hb20 7 related work several lines work focused increasing parameter count and/or computation language models means improve generative task performance early work scaled lstm based language models billion parameters jvs+16 one line work straightforwardly increases size transformer models scaling parameters flops-per-token roughly proportion work vein successively increased model size 213 million parameters vsp+17 original paper 300 million parameters dclt18 1.5 billion parameters rwc+19 8 billion parameters spp+19 11 billion parameters rsr+19 recently 17 billion parameters tur20 second line work focused increasing parameter count computation means increasing models capacity store information without increased computational cost approaches rely conditional computation framework blc13 specically mixture-of-experts method smm+17 used produce 100 billion parameter models recently 50 billion parameter translation models ajf19 though small fraction parameters actually used forward pass third approach increases computation without increasing parameters examples approach include adaptive computation time gra16 universal transformer dgv+18 work focuses rst approach scaling compute parameters together straightforwardly making neural net larger increases model size 10x beyond previous models employ strategy several efforts also systematically studied effect scale language model performance kmh+20 rrbs19 lws+20 hna+17 nd smooth power-law trend loss autoregressive language models scaled work suggests trend largely continues models continue scale although slight bending curve perhaps detected figure 3.1 also nd relatively smooth increases many though downstream tasks across 3 orders magnitude scaling another line work goes opposite direction scaling attempting preserve strong performance language models small possible approach includes albert lcg+19 well general hvd15 39task-specic sdcw19 jys+19 kr16 approaches distillation language models architectures techniques potentially complementary work could applied decrease latency memory footprint giant models ne-tuned language models neared human performance many standard benchmark tasks considerable effort devoted constructing difcult open-ended tasks including question answering kpr+19 ibgc+14 cce+18 mcks18 reading comprehension chi+18 rcm19 adversarially constructed datasets designed difcult existing language models sbbc19 nwd+19 work test models many datasets many previous efforts focused specically question-answering constitutes signicant fraction tasks tested recent efforts include rsr+19 rrs20 ne-tuned 11 billion parameter language model glt+20 focused attending large corpus data test time work differs focusing in-context learning could combined future glt+20 lpp+20 metalearning language models utilized rwc+19 though much limited results systematic study broadly language model metalearning inner-loop-outer-loop structure making structurally similar metalearning applied ml general extensive literature including matching networks vbl+16 rl2 dsc+16 learning optimize rl16 adg+16 lm17 maml fal17 approach stufng models context previous examples structurally similar rl2 also resembles hyc01 inner loop adaptation takes place computation models activations across timesteps without updating weights outer loop case language model pre-training updates weights implicitly learns ability adapt least recognize tasks dened inference-time few-shot auto-regressive density estimation explored rcp+17 gwc+18 studied low-resource nmt few-shot learning problem mechanism few-shot approach different prior work also explored ways using pre-trained language models combination gradient descent perform few-shot learning ss20 another sub-eld similar goals semi-supervised learning approaches uda xdh+19 also explore methods ne-tuning little labeled data available giving multi-task models instructions natural language rst formalized supervised setting mkxs18 utilized tasks summarizing language model rwc+19 notion presenting tasks natural language also explored text-to-text transformer rsr+19 although applied multi-task ne-tuning rather in-context learning without weight updates another approach increasing generality transfer-learning capability language models multi-task learning car97 ne-tunes mixture downstream tasks together rather separately updating weights one successful multi-task learning could allow single model used many tasks without updating weights similar in-context learning approach alternatively could improve sample efciency updating weights new task multi-task learning shown promising initial results lgh+15 lsp+18 multi-stage ne-tuning recently become standardized part sota results datasets pfb18 pushed boundaries certain tasks kks+20 still limited need manually curate collections datasets set training curricula contrast pre-training large enough scale appears offer natural broad distribution tasks implicitly contained predicting text one direction future work might attempting generate broader set explicit tasks multi-task learning example procedural generation tfr+17 human interaction zsw+19b active learning mac92 algorithmic innovation language models last two years enormous including denoising-based bidirectionality dclt18 prexlm dl15 encoder-decoder architectures llg+19 rsr+19 random permu- tations training ydy+19 architectures improve efciency sampling dyy+19 improvements data training procedures log+19 efciency increases embedding parameters lcg+19 many techniques provide signicant gains downstream tasks work continue focus pure autoregressive language models order focus in-context learning performance reduce complexity large model implementations however likely incorporating algorithmic advances could improve gpt-3s performance downstream tasks especially ne-tuning setting combining gpt-3s scale algorithmic techniques promising direction future work 8 conclusion presented 175 billion parameter language model shows strong performance many nlp tasks benchmarks zero-shot one-shot few-shot settings cases nearly matching performance 40state-of-the-art ne-tuned systems well generating high-quality samples strong qualitative performance tasks dened on-the-y documented roughly predictable trends scaling performance without using ne-tuning also discussed social impacts class model despite many limitations weaknesses results suggest large language models may important ingredient development adaptable general language systems acknowledgements authors would like thank ryan lowe giving detailed feedback drafts paper thanks jakub pachocki szymon sidor suggesting tasks greg brockman michael petrov brooke chan chelsea v oss helping run evaluations openais infrastructure thanks david luan initial support scaling project irene solaiman discussions ways approach evaluate bias harrison edwards yura burda discussions experimentation in-context learning geoffrey irving paul christiano early discussions language model scaling long ouyang advising design human evaluation experiments chris hallacy discussions data collection carter help visual design thanks millions people created content used training model involved indexing upvoting content case webtext additionally would like thank entire openai infrastructure supercomputing teams making possible train models scale 41contributions tom brown ben mann prafulla dhariwal dario amodei nick ryder daniel ziegler jeffrey wu implemented large-scale models training infrastructure model-parallel strategies tom brown dario amodei ben mann nick ryder conducted pre-training experiments ben mann alec radford collected ltered deduplicated conducted overlap analysis training data melanie subbiah ben mann dario amodei jared kaplan sam mccandlish tom brown tom henighan girish sastry implemented downstream tasks software framework supporting including creation synthetic tasks jared kaplan sam mccandlish initially predicted giant language model show continued gains applied scaling laws help predict guide model data scaling decisions research ben mann implemented sampling without replacement training alec radford originally demonstrated few-shot learning occurs language models jared kaplan sam mccandlish showed larger models learn quickly in-context systematically studied in-context learning curves task prompting evaluation methods prafulla dhariwal implemented early version codebase developed memory optimizations fully half-precision training rewon child mark chen developed early version model-parallel strategy rewon child scott gray contributed sparse transformer aditya ramesh experimented loss scaling strategies pretraining melanie subbiah arvind neelakantan implemented experimented tested beam search pranav shyam worked superglue assisted connections few-shot learning meta-learning literature sandhini agarwal conducted fairness representation analysis girish sastry amanda askell conducted human evaluations model ariel herbert-voss conducted threat analysis malicious use gretchen krueger edited red-teamed policy sections paper benjamin chess clemens winter eric sigler christopher hesse mateusz litwin christopher berner optimized openais clusters run largest models efciently scott gray developed fast gpu kernels used training jack clark led analysis ethical impacts fairness representation human assessments model broader impacts analysis advised gretchen amanda girish sandhini ariel work dario amodei alec radford tom brown sam mccandlish nick ryder jared kaplan sandhini agarwal amanda askell girish sastry jack clark wrote paper sam mccandlish led analysis model scaling advised tom henighan jared kaplan work alec radford advised project nlp perspective suggested tasks put results context demonstrated benet weight decay training ilya sutskever early advocate scaling large generative likelihood models advised pranav prafulla rewon alec aditya work dario amodei designed led research 42a details common crawl filtering mentioned section 2.2 employed two techniques improve quality common crawl dataset 1 ltering common crawl 2 fuzzy deduplication 1.in order improve quality common crawl developed automatic ltering method remove low quality documents using original webtext proxy high-quality documents trained classier distinguish raw common crawl used classier re-sample common crawl prioritizing documents predicted classier higher quality classier trained using logistic regression classier features sparks standard tokenizer hashingtf10 positive examples used collection curated datasets webtext wikiedia web books corpus positive examples negative examples used unltered common crawl used classier score common crawl documents kept document dataset iff np.random.pareto 1\u0000document_score chose 9in order take mostly documents classier scored highly still include documents distribution chosen match distribution scores classier webtext found re-weighting increased quality measured loss range out-of-distribution generative text samples 2.to improve model quality prevent overtting becomes increasingly important model capacity increases fuzzily deduplicated documents i.e removed documents high overlap documents within dataset using sparks minhashlsh implementation 10 hashes using features used classication also fuzzily removed webtext common crawl overall decreased dataset size average 10 ltering duplicates quality also partially removed text occurring benchmark datasets described appendix c. b details model training train versions gpt-3 use adam 1= 0:9 2= 0:95 and\u000f= 10\u00008 clip global norm gradient 1.0 use cosine decay learning rate 10 value 260 billion tokens 260 billion tokens training continues 10 original learning rate linear lr warmup rst 375 million tokens also gradually increase batch size linearly small value 32k tokens full value rst 4-12 billion tokens training depending model size data sampled without replacement training epoch boundary reached minimize overtting models use weight decay 0.1 provide small amount regularization lh17 training always train sequences full nctx= 2048 token context window packing multiple documents single sequence documents shorter 2048 order increase computational efciency sequences multiple documents masked special way instead documents within sequence delimited special end text token giving language model information necessary infer context separated end text token unrelated allows efcient training without need special sequence-specic masking c details test set contamination studies section 4 gave high level overview test set contamination studies section provide details methodology results initial training set ltering attempted remove text occurring benchmarks training data searching for13\u0000gram overlaps test/development sets used work training data removed colliding 13\u0000gram well 200 character window around splitting original document pieces ltering purposes dene gram lowercase whitespace delimited word punctuation pieces less 200characters long discarded documents split 10 pieces considered contaminated 10https //spark.apache.org/docs/latest/api/python/pyspark.ml.html pyspark.ml.feature.hashingtf 43removed entirely originally removed entire documents given single collision overly penalized long documents books false positives example false positive might test set based wikipedia wikipedia article quotes single line book ignored 13\u0000grams matched 10 training documents inspection showed majority contain common cultural phrases legal boilerplate similar content likely want model learn rather undesired specic overlaps test sets examples various frequencies found gpt-3 release repository11 overlap methodology benchmark overlap analysis section 4 used variable number words nto check overlap dataset nis 5th percentile example length words ignoring punctuation whitespace casing due spurious collisions lower values nwe use minimum value 8 non-synthetic tasks performance reasons set maximum value 13 tasks values nand amount data marked dirty shown table c.1 unlike gpt-2s use bloom lters compute probabilistic bounds test contamination used apache spark compute exact collisions across training test sets compute overlaps test sets full training corpus even though trained 40 ltered common crawl documents per section 2.2. dene dirty example one n-gram overlap training document clean example one collision test validation splits similar contamination levels despite test splits unlabeled due bug revealed analysis ltering described failed long documents books cost considerations infeasible retrain model corrected version training dataset several language modeling benchmarks plus childrens book test showed almost complete overlap therefore included paper overlaps shown table c.1 overlap results understand much seen data helps model perform downstream tasks lter every validation test set dirtiness run evaluation clean-only examples report relative percent change clean score original score clean score 1 2 worse overall score suggests model may overt examples seen clean score signicantly better ltering scheme may preferentially marked easier examples dirty overlap metric tends show high rate false positives datasets contain background information answers drawn web squad draws wikipedia examples less 8 words long ignored ltering process except wordscrambling tasks one instance technique seems fail give good signal drop reading comprehension task 94 examples dirty information required answer question passage provided model seen passage training questions answers meaningfully constitute cheating conrmed every matching training document contained source passage none questions answers dataset likely explanation decrease performance 6 examples remain ltering come slightly different distribution dirty examples figure 4.2 shows dataset becomes contaminated variance clean/all fraction increases apparent bias towards improved degraded performance suggests gpt-3 relatively insensitive contamination see section 4 details datasets agged review 11https //github.com/openai/gpt-3/blob/master/overlap_frequency.md 44name split metric n acc/f1/bleutotal countdirty acc/f1/bleudirty countclean acc/f1/bleuclean countclean percentagerelative difference clean vs quac dev f1 13 44.3 7353 44.3 7315 54.1 38 1 20 squadv2 dev f1 13 69.8 11873 69.9 11136 68.4 737 6 -2 drop dev f1 13 36.5 9536 37.0 8898 29.5 638 7 -21 symbol insertion dev acc 7 66.9 10000 66.8 8565 67.1 1435 14 0 coqa dev f1 13 86.0 7983 85.3 5107 87.1 2876 36 1 record dev acc 13 89.5 10000 90.3 6110 88.2 3890 39 -1 winograd test acc 9 88.6 273 90.2 164 86.2 109 40 -3 boolq dev acc 13 76.0 3270 75.8 1955 76.3 1315 40 0 multirc dev acc 13 74.2 953 73.4 558 75.3 395 41 1 race-h test acc 13 46.8 3498 47.0 1580 46.7 1918 55 0 lambada test acc 13 86.4 5153 86.9 2209 86.0 2944 57 0 lambada blanks test acc 13 77.8 5153 78.5 2209 77.2 2944 57 -1 wsc dev acc 13 76.9 104 73.8 42 79.0 62 60 3 piqa dev acc 8 82.3 1838 89.9 526 79.3 1312 71 -4 race-m test acc 13 58.5 1436 53.0 366 60.4 1070 75 3 de en 16 test bleu-sb 12 43.0 2999 47.4 739 40.8 2260 75 -5 en de 16 test bleu-sb 12 30.9 2999 32.6 739 29.9 2260 75 -3 en ro 16 test bleu-sb 12 25.8 1999 24.9 423 26.1 1576 79 1 ro en 16 test bleu-sb 12 41.3 1999 40.4 423 41.6 1576 79 1 webqs test acc 8 41.5 2032 41.6 428 41.5 1604 79 0 anli r1 test acc 13 36.8 1000 40.5 200 35.9 800 80 -3 anli r2 test acc 13 34.0 1000 29.4 177 35.0 823 82 3 triviaqa dev acc 10 71.2 7993 70.8 1390 71.3 6603 83 0 anli r3 test acc 13 40.2 1200 38.3 196 40.5 1004 84 1 en fr 14 test bleu-sb 13 39.9 3003 38.3 411 40.3 2592 86 1 fr en 14 test bleu-sb 13 41.4 3003 40.9 411 41.4 2592 86 0 wic dev acc 13 51.4 638 53.1 49 51.3 589 92 0 rte dev acc 13 71.5 277 71.4 21 71.5 256 92 0 cb dev acc 13 80.4 56 100.0 4 78.8 52 93 -2 anagrams 2 dev acc 2 40.2 10000 76.2 705 37.4 9295 93 -7 reversed words dev acc 2 0.4 10000 1.5 660 0.3 9340 93 -26 openbookqa test acc 8 65.4 500 58.1 31 65.9 469 94 1 arc easy test acc 11 70.1 2268 77.5 89 69.8 2179 96 0 anagrams 1 dev acc 2 15.0 10000 49.8 327 13.8 9673 97 -8 copa dev acc 9 93.0 100 100.0 3 92.8 97 97 0 arc challenge test acc 12 51.6 1144 45.2 31 51.8 1113 97 0 hellaswag dev acc 13 79.3 10042 86.2 152 79.2 9890 98 0 nqs test acc 11 29.9 3610 32.7 52 29.8 3558 99 0 cycled letters dev acc 2 38.6 10000 20.5 73 38.7 9927 99 0 sat analogies dev acc 9 65.8 374 100.0 2 65.6 372 99 0 storycloze test acc 13 87.7 1871 100.0 2 87.6 1869 100 0 winogrande dev acc 13 77.7 1267 0 77.7 1267 100 0 table c.1 overlap statistics datasets sorted dirtiest cleanest consider dataset example dirty singlen-gram collision document training corpus relative difference clean vs shows percent change performance clean examples vs examples benchmark count shows number examples clean percentage percent examples clean vs total acc/f1/bleu use metric specied metric scores come evaluations different seed random examples used in-context learning therefore differ slightly scores elsewhere paper 45d total compute used train language models appendix contains calculations used derive approximate compute used train language models figure 2.2. simplifying assumption ignore attention operation typically uses less 10 total compute models analyzing calculations seen table d.1 explained within table caption modeltotal train compute pf-days total train compute ops params training tokens billions flops per param per tokenmult bwd passfwd-pass ops per active param per tokenfrac params active token t5-small 2.08e+00 1.80e+20 60 1,000 3 3 1 0.5 t5-base 7.64e+00 6.60e+20 220 1,000 3 3 1 0.5 t5-large 2.67e+01 2.31e+21 770 1,000 3 3 1 0.5 t5-3b 1.04e+02 9.00e+21 3,000 1,000 3 3 1 0.5 t5-11b 3.82e+02 3.30e+22 11,000 1,000 3 3 1 0.5 bert-base 1.89e+00 1.64e+20 109 250 6 3 2 1.0 bert-large 6.16e+00 5.33e+20 355 250 6 3 2 1.0 roberta-base 1.74e+01 1.50e+21 125 2,000 6 3 2 1.0 roberta-large 4.93e+01 4.26e+21 355 2,000 6 3 2 1.0 gpt-3 small 2.60e+00 2.25e+20 125 300 6 3 2 1.0 gpt-3 medium 7.42e+00 6.41e+20 356 300 6 3 2 1.0 gpt-3 large 1.58e+01 1.37e+21 760 300 6 3 2 1.0 gpt-3 xl 2.75e+01 2.38e+21 1,320 300 6 3 2 1.0 gpt-3 2.7b 5.52e+01 4.77e+21 2,650 300 6 3 2 1.0 gpt-3 6.7b 1.39e+02 1.20e+22 6,660 300 6 3 2 1.0 gpt-3 13b 2.68e+02 2.31e+22 12,850 300 6 3 2 1.0 gpt-3 175b 3.64e+03 3.14e+23 174,600 300 6 3 2 1.0 table d.1 starting right hand side moving left begin number training tokens model trained next note since t5 uses encoder-decoder model half parameters active token forward backwards pass note token involved single addition single multiply active parameter forward pass ignoring attention add multiplier 3x account backwards pass computing params lossand acts lossuse similar amount compute forwards pass combining previous two numbers get total ops per parameter per token multiply value total training tokens total parameters yield number total ops used training report ops petaop/s-day 8.64e+19 ops e human quality assessment synthetic news articles appendix contains details experiments measuring human ability distinguish gpt-3-generated synthetic news articles real news articles rst describe experiments \u0018200word news articles describe preliminary investigation \u0018500word news articles generated gpt-3 participants recruited 718 unique participants take part 6 experiments 97 participants excluded failing internet check question leaving total 621 participants 343 male 271 female 7 mean participant age \u001838years old participants recruited positly maintains whitelist high-performing workers mechanical turk participants us-based demographic restrictions participants paid 12 participation based task time estimate 60 minutes determined pilot runs order ensure sample participants experiment quiz unique participants allowed take part experiment procedure design arbitrarily selected 25 news articles appeared newser.com early 2020. used article titles subtitles produce outputs 125m 350m 760m 1.3b 2.7b 6.7b 13.0b 200b gpt-3 parameter language models five outputs per question generated model generation word count closest human written article selected automatically minimize effect completion length might participants judgments output procedure model exception removal intentionally bad control model described main text 46modelparticipants recruitedparticipants excludedgenders f mean ageaverage word count human model control 76 7 32:37:0 39 216:216 gpt-3 small 80 7 41:31:1 40 216:188 gpt-3 medium 80 7 46:28:2 39 216:202 gpt-3 large 81 24 46:28:2 37 216:200 gpt-3 xl 79 14 32:32:1 38 216:199 gpt-3 2.7b 80 11 36:33:0 40 216:202 gpt-3 6.7b 76 5 46:28:2 37 216:195 gpt-3 13.0b 81 13 46:28:2 37 216:209 gpt-3 175b 80 9 42:29:0 37 216:216 table e.1 participant details article lengths experiment evaluate human detection \u0018200word model generated news articles participants excluded due internet check fails figure e.1 participants spend time trying identify whether news article machine generated model size increases duration control model indicated dashed line line best linear model log scale 95 condence intervals experiment half participants randomly assigned quiz half randomly assigned quiz b. quiz consisted 25 articles half 12-13 human written half 12-13 model generated articles human written completions quiz model generated completions quiz b vice versa order quiz question shufed participant participants could leave comments asked indicate seen articles participants instructed look articles content quiz end quiz asked looked anything quiz statistical tests compare means different runs performed two-sample t-test independent groups model control implemented python using scipy.stats.ttest_ind function plotting regression line graph average participant accuracy vs model size power law form ax\u0000b 95 condence intervals estimated t-distribution sample mean duration statistics main text discussed nding ability human participants distinguish model human generated news articles decreases models become larger also found average time spent given set questions increases model size increases shown figure e.1 lower 47modelparticipants recruitedparticipants excludedgenders f mean ageaverage word count human model control 79 17 32:37:0 39 569:464 gpt-3 175b 81 19 32:30:0 40 569:498 table e.2 participant details article lengths experiments investigating human detection \u0018500word model generated news articles participants excluded due internet check fails accuracy scores despite increased time investment participants supports nding larger models generate harder-to-distinguish news articles preliminary investigation \u0018500word articles recruited 160 unique us-based participants take part 2 experiments positly details given table e.2 randomly selected 12 reuters world news articles late 2019 created context gpt-3 175b consisted single reuters article set 12. used article titles reuters locations generate completions gpt-3 175b 160m control model previous experiments used create two 12-question quizzes per model consisting half human written half model generated articles comprehension questions added articles shown participants 3 stages 30 second intervals encourage closer reading participants paid 12 task model generation selection methods exclusion criteria statistical tests mirror previous experiments f additional samples gpt-3 gpt-3 adapts well many tasks ones explored main body paper example figure f.1 show four uncurated samples prompt suggesting model write poem given title style wallace stevens rst experimented prompts generated four samples additional editing selection sampling temperature 1using nucleus sampling hbfc19 withp= 0:9 completions truncated model began write new title author heading broke prose commentary 48context city c. p. cavafy translated edmund keeley poem text omitted trees john ashbery poem text omitted shadows way wallace stevens -- -- -- -- generated poem 1 -- -- -- -- -- -- -- -- generated poem 3 -- -- -- -- must shadows way sun shade walk must changed mind must dwell step taken slowly alone white fields eyes always old ready made ancient gleams convoluting way lay among huge roots must think lines grey great dark tomes reverie dim thoughts guide plumed labyrinth sea must look blue green never let eye forget breath gonne faint fade color friend long stairway dead purple must surround shall remember well blindness light whiteness yellow sun white land footsteps intrusive bluish snow grasses light shade falls us must menacing life grey thoughts blue thoughts walk go away played warmest rooms one content one content -- -- -- -- generated poem 2 -- -- -- -- mankind forest glades fable way world wood town legend shadowed- way man may live small town return one content lifetime see man come every happiness brave never stir men market purposeless never bobbing covered carts island gardens deep mind never creak passed stair make remoteness familiarity closing life old man unspoken name friend prosper town prosper store seen close wife -- -- -- -- generated poem 4 -- -- -- -- fades house world shouts vain one comes hear nobody come place road leads nowhere small bell tolls solitude deep mountains high small town desolate turn traveler 's face everything known towards north sounds world far away ways young men known wind rises trees ways young women boughs bow ground walk spring even birds inhabit tangle weeds rumors known roadside cover silent one listens quarrels hurt envy hears roar forest one alone men women one taken till day comes one taken way world utterance conversation leads wood one uneasy .... young young man thin blue mist walks lover darkness rising like smoke knows streets within darkness possession heart comes wood one taken .... again- sky empty full light wood among trees sees shadows way hears voices hears wind rustling leaves open glade sees shape shape hears waits waits waits shadows wait voices wait shadows way voices wind figure f.1 four uncurated completions context suggesting model compose poem style wallace stevens title shadows way 49g details task phrasing specications following gures illustrate formatting phrasing tasks included paper data comes ground truth datasets section samples gpt-3 included context article informal conversation important part business relationship.before start discussion however make sure understand topics suitable considered taboo particular culture latin americans enjoy sharing information local history art customs.you may expect questions family sure show pictures children.you may feel free ask similar questions latin american friends.the french think conversation art form enjoy value lively discussions well disagreements arguments interesting cover pretty much topic -- -- long occur respectful intelligent manner united states business people like discuss wide range topics including opinions work family hobbies politics japan china korea however people much private.they share much thoughts feelings emotions feel might take away harmonious business relationship 're trying build.middle easterners also private personal lives family matters.it considered rude example ask businessman saudi arabia wife children general rule 's best talk politics religion business friends.this get trouble even united states people hold different religious views.in addition discussing one 's salary usually considered unsuitable.sports typically friendly subject parts world although careful criticize national sport.instead friendly praise host 's team q n't talking sports colleagues another country criticizing sports colleagues country q typically friendly topic places according author sports q people asia private conversation others n't want good relationship others harmed informal conversation q author considers politics religion correct answer taboo incorrect answer cheerful topics incorrect answer rude topics incorrect answer topics never talked figure g.1 formatted dataset example race-h. predicting normalize unconditional probability answer described 2 50context anli 2 anli 2 gold coast hotel casino hotel casino located paradise nevada locals casino owned operated boyd gaming gold coast located one mile \u0018 1:6km west las vegas strip west flamingo road located across street palms casino resort rio suite hotel casino question gold coast budget-friendly casino true false neither correct answer neither incorrect answer true incorrect answer false figure g.2 formatted dataset example anli r2 context article mrs. smith unusual teacher told student bring along potatoes plastic bag potato students write name person hated next day every child brought potatoes two potatoes three five mrs. smith told children carry bags everywhere went even toilet two weeks day day passed children started complain awful smell rotten potatoes children brought five potatoes began feel weight trouble bags two weeks children happy hear game finally ended mrs. smith asked '' feel carrying potatoes two weeks '' children started complaining trouble loudly mrs. smith told asked play game said '' exactly situation carry hatred somebody inside heart terrible smell hatred pollute heart carry something unnecessary time stand smell rotten potatoes two weeks imagine heavy would hatred heart lifetime throw away hatred heart 'll really happy '' q following true according passage kid hated four people carry four potatoes q learn passage throw away hatred inside q children complained besides weight trouble smell q mrs.smith asked students write potatoes correct answer names incorrect answer numbers incorrect answer time incorrect answer places figure g.3 formatted dataset example race-m. predicting normalize unconditional probability answer described 2 51context apply sealant wood correct answer using brush brush sealant onto wood fully saturated sealant incorrect answer using brush drip sealant onto wood fully saturated sealant figure g.4 formatted dataset example piqa context body cast shadow grass correct answer sun rising incorrect answer grass cut figure g.5 formatted dataset example copa context cnn yuval rabin whose father yitzhak rabin assassinated serving prime minister israel criticized donald trump appealing `` second amendment people '' speech warned words politicians use incite violence undermine democracy `` trump's words incitement type political violence touched personally '' rabin wrote usatoday said trump 's appeal '' second amendment people '' stop hillary clinton -- comments criticized call violence clinton something trump denied -- `` new level ugliness ugly campaign season '' son former israeli prime minister assassinated wrote op ed consequence violent political rhetoric warns `` parallels '' israel 1990s u.s. today correct answer referencing father shot killed extremist amid political tension israel 1995 rabin condemned donald trump's aggressive rhetoric correct answer referencing father shot killed extremist amid political tension israel 1995 rabin condemned trump 's aggressive rhetoric incorrect answer referencing father shot killed extremist amid political tension israel 1995 rabin condemned hillary clinton's aggressive rhetoric incorrect answer referencing father shot killed extremist amid political tension israel 1995 rabin condemned u.s. 's aggressive rhetoric incorrect answer referencing father shot killed extremist amid political tension israel 1995 rabin condemned yitzhak rabin's aggressive rhetoric figure g.6 formatted dataset example record consider context single problem task presented record dataset scored record evaluation script context anli 1 anli 1 fulton james macgregor msp scottish politician scottish national party snp member scottish parliament constituency coatbridge chryston macgregor currently parliamentary liaison officer shona robison cabinet secretary health sport also serves justice education skills committees scottish parliament question fulton james macgregor scottish politican liaison officer shona robison swears best friend true false neither correct answer neither incorrect answer true incorrect answer false figure g.7 formatted dataset example anli r1 52context organisms require energy order correct answer mature develop incorrect answer rest soundly incorrect answer absorb light incorrect answer take nutrients figure g.8 formatted dataset example openbookqa predicting normalize unconditional probability answer described 2. context making cake several cake pops shown display woman girl shown making cake pops kitchen correct answer bake frost decorate incorrect answer taste place plates incorrect answer put frosting cake pan incorrect answer come begin decorating cake well figure g.9 formatted dataset example hellaswag context anli 3 anli 3 shut loophole american workers actually subsidizing loss job passed expansion loophole last days 43 billion giveaways including favors oil gas industry people importing ceiling fans china question loophole gone true false neither correct answer false incorrect answer true incorrect answer neither figure g.10 formatted dataset example anli r3 context question george wants warm hands quickly rubbing skin surface produce heat answer correct answer dry palms incorrect answer wet palms incorrect answer palms covered oil incorrect answer palms covered lotion figure g.11 formatted dataset example arc challenge predicting normalize unconditional probability answer described 2. context lull trust correct answer cajole compliance incorrect answer balk fortitude incorrect answer betray loyalty incorrect answer hinder destination incorrect answer soothe passion figure g.12 formatted dataset example sat analogies correct context grace happy trade sweater jacket thinks sweater incorrect context grace happy trade sweater jacket thinks jacket target completion looks dowdy figure g.13 formatted dataset example winograd partial evaluation method use compares probability completion given correct incorrect context 53correct context johnny likes fruits vegetables new keto diet fruits incorrect context johnny likes fruits vegetables new keto diet vegetables target completion saccharine figure g.14 formatted dataset example winogrande partial evaluation method use compares probability completion given correct incorrect context context reading comprehension answer key process moved along diplomacy continued rounds direct pressure taliban proved unsuccessful one nsc staff note put `` taliban afghanistan much state sponsor terrorism state sponsored terrorists '' early 2000 united states began high-level effort persuade pakistan use influence taliban january 2000 assistant secretary state karl inderfurth state department 's counterterrorism coordinator michael sheehan met general musharraf islamabad dangling possibility presidential visit march reward pakistani cooperation visit coveted musharraf partly sign government 's legitimacy told two envoys would meet mullah omar press bin laden left however reporting washington pakistan unlikely fact anything '' given sees benefits taliban control afghanistan '' president clinton scheduled travel india state department felt visit india without also visiting pakistan secret service cia however warned strongest terms visiting pakistan would risk president's life counterterrorism officials also argued pakistan done enough merit presidential visit president clinton insisted including pakistan itinerary trip south asia one-day stopover march 25 2000 first time u.s. president since 1969. meeting musharraf others president clinton concentrated tensions pakistan india dangers nuclear proliferation also discussed bin laden president clinton told us pulled musharraf aside brief one-on-one meeting pleaded general help regarding bin laden '' offered moon went see terms better relations united states 'd help us get bin laden deal another issue two '' u.s. effort continued state department feel visit india pakistan correct answer false bin laden incorrect answer true bin laden figure g.15 formatted dataset example multirc three levels within multirc 1 passage 2 questions 3 answers evaluation accuracy determined per-question level question considered correct answers within question labeled correctly reason use kto refer number questions shown within context context question factor likely cause person develop fever answer correct answer bacterial population bloodstream incorrect answer leg muscle relaxing exercise incorrect answer several viral particles skin incorrect answer carbohydrates digested stomach figure g.16 formatted dataset example arc easy predicting normalize unconditional probability answer described 2 54context bob went gas station fill car tank completely empty wallet cashier offered pay gas came back later pay bob felt grateful drove home correct answer bob believed good people world incorrect answer bob contemplated unfriendly world figure g.17 formatted dataset example storycloze context helsinki capital largest city finland region uusimaa southern finland shore gulf finland helsinki population urban population metropolitan population 1.4 million making populous municipality urban area finland helsinki north tallinn estonia east stockholm sweden west saint petersburg russia helsinki close historical connections three cities helsinki metropolitan area includes urban core helsinki espoo vantaa kauniainen surrounding commuter towns world's northernmost metro area one million people city northernmost capital eu member state helsinki metropolitan area third largest metropolitan area nordic countries stockholm copenhagen city helsinki third largest stockholm oslo helsinki finland 's major political educational financial cultural research center well one northern europe 's major cities approximately 75 foreign companies operate finland settled helsinki region nearby municipality vantaa location helsinki airport frequent service various destinations europe asia q populous municipality finland helsinki q many people live 1.4 million metropolitan area q percent foreign companies operate finland helsinki 75 q towns part metropolitan area target completion helsinki espoo vantaa kauniainen surrounding commuter towns figure g.18 formatted dataset example coqa context please unscramble letters word write word asinoc target completion casino figure g.19 formatted dataset example cycled letters 55context passage saint jean de br\u0013 ebeuf french jesuit missionary travelled new france 1625. worked primarily huron rest life except years france 1629 1633. learned language culture writing extensively aid missionaries 1649 br\u0013 ebeuf another missionary captured iroquois raid took huron village together huron captives missionaries ritually tortured killed march 16 1649. br\u0013 ebeuf beatified 1925 among eight jesuit missionaries canonized saints roman catholic church 1930. question many years saint jean de br\u0013 ebeuf stay new france went back france years answer target completion 4 figure g.20 formatted dataset example drop context fill blank held torch front caught breath `` chris 's step '' `` '' `` step cut rock fifty feet ahead '' moved faster moved faster `` fact '' said raising torch higher '' 's target completion step figure g.21 formatted dataset example lambada context please unscramble letters word write word skicts target completion sticks figure g.22 formatted dataset example anagrams 1 a1 context please unscramble letters word write word volwskagen target completion volkswagen figure g.23 formatted dataset example anagrams 2 context q played tess touched angel target completion delloreese patricia early july 6 1931 november 19 2017 known professionally della reese figure g.24 formatted dataset example natural questions 56context title william perry american football professional career paragraph 1985 selected first round 1985 nfl draft chicago bears hand-picked coach mike ditka however defensive coordinator buddy ryan highly acrimonious relationship ditka called perry `` wasted draft-pick '' perry soon became pawn political power struggle ditka ryan perry 's `` refrigerator '' nickname followed nfl quickly became favorite chicago bears fans teammates called `` biscuit '' `` one biscuit shy 350 pounds '' ryan refused play perry ditka decided use perry fullback team near opponents goal line fourth short situations either ball carrier lead blocker star running back walter payton ditka stated inspiration using perry fullback came five-yard sprint exercises rookie season perry rushed two touchdowns caught pass one perry even opportunity run ball super bowl xx nod popularity contributions team 's success first time got ball tackled one-yard loss attempting throw first nfl pass halfback option play second time got ball scored touchdown running patriots linebacker larry mcgrew process halfway rookie season ryan finally began play perry soon proved capable defensive lineman super bowl ring size largest professional football player history event ring size 25 ring size average adult male 10 12. perry went play ten years nfl retiring 1994 season ten years pro regularly struggled weight hampered performance times played 138 games recording 29.5 sacks five fumble recoveries returned total 71 yards offensive career ran five yards two touchdowns one reception another touchdown perry later attempted comeback playing unremarkable 1996 season london monarchs world league american football later nfl europa q team play target completion chicago bears figure g.25 formatted dataset example quac context please unscramble letters word write word r e c.i p r o.c a/l target completion reciprocal figure g.26 formatted dataset example symbol insertion context please unscramble letters word write word taefed target completion defeat figure g.27 formatted dataset example reversed words 57context title blitz background german point view march 1941 saw improvement luftwaffe flew 4,000 sorties month including 12 major three heavy attacks electronic war intensified luftwaffe flew major inland missions moonlit nights ports easier find made better targets confuse british radio silence observed bombs fell x- y-ger beams placed false targets switched last minute rapid frequency changes introduced x-ger whose wider band frequencies greater tactical flexibility ensured remained effective time british selective jamming degrading effectiveness y-ger q many sorties flown march 1941 4,000 q luftwaffe fly inland missions target completion moonlit nights figure g.28 formatted dataset example squadv2 context normal force -- simple case object resting upon table normal force object equal opposite direction gravitational force applied object weight object n g ndisplaystyle n=mg mass g gravitational field strength 9.81 m/s earth normal force represents force applied table object prevents sinking table requires table sturdy enough deliver normal force without breaking however easy assume normal force weight action-reaction force pairs common mistake case normal force weight need equal magnitude explain upward acceleration object example ball bounces upwards accelerates upwards normal force acting ball larger magnitude weight ball question normal force equal force gravity answer target completion yes figure g.29 formatted dataset example boolq context trend toward lower rents may seem surprising given communities new york bemoaning loss favorite local businesses high rents despite recent softening many retailers 's still big jump rental rates late 1970s leases signed certainly recent drop prices n't mean manhattan comes cheap question manhattan comes cheap true false neither answer target completion false figure g.30 formatted dataset example cb 58context bet dinner four regarding existence mass top quark elementary particle discovered 1995. question top quark last six flavors quarks predicted standard model theory particle physics true false answer target completion false figure g.31 formatted dataset example rte context outfitter provided everything needed safari first walking holiday went specialist outfitter buy boots question word outfitter used way two sentences answer target completion figure g.32 formatted dataset example wic context final exam answer key instructions please carefully read following passages passage must identify noun pronoun marked bold refers ===== passage mr. moncrieff visited chester 's luxurious new york apartment thinking belonged son edward result mr. moncrieff decided cancel edward 's allowance ground longer requires financial support question passage pronoun `` '' refer answer target completion mr moncrieff figure g.33 formatted dataset example wsc context q nude descending staircase perhaps famous painting 20th century artist target completion marcel duchamp target completion r mutt target completion duchamp target completion marcel duchamp target completion r.mutt target completion marcel duchamp target completion henri-robert-marcel duchamp target completion marcel du champ target completion henri robert marcel duchamp target completion duchampian target completion duchamp target completion duchampian target completion marcel du champ target completion marcel duchamp target completion marcel duchamp figure g.34 formatted dataset example triviaqa triviaqa allows multiple valid completions 59context q school burne hogarth establish target completion school visual arts figure g.35 formatted dataset example webqa context keinesfalls d urfen diese f ur den kommerziellen gebrauch verwendet werden target completion case may used commercial purposes figure g.36 formatted dataset example de en format one- few-shot learning langauge tasks format zero-shot learning q flanguagegtranslation offsentencega ftranslationg context case may used commercial purposes target completion keinesfalls d urfen diese f ur den kommerziellen gebrauch verwendet werden figure g.37 formatted dataset example en de context analysis instar distributions larval i. verticalis collected series ponds also indicated males advanced instars females target completion l'analyse de la distribution de fr\u0013 equence des stades larvaires verticalis dans une s\u0013 erie d'\u0013 etangs \u0013 egalement d\u0013 emontr\u0013 e que les larves m^ ales \u0013 etaient \u0012 des stades plus avanc\u0013 es que les larves femelles figure g.38 formatted dataset example en fr context l'analyse de la distribution de fr\u0013 equence des stades larvaires verticalis dans une s\u0013 erie d'\u0013 etangs \u0013 egalement d\u0013 emontr\u0013 e que les larves m^ ales \u0013 etaient \u0012 des stades plus avanc\u0013 es que les larves femelles target completion analysis instar distributions larval i. verticalis collected series ponds also indicated males advanced instars females figure g.39 formatted dataset example fr en context truth want price wishes peoples europe continue negotiations turkey 's accession european union despite turkey 's continuing refusal recognise cyprus despite fact democratic reforms standstill target completion adev\u0015 arul este c\u0015 v\u0015 dorit \u0018i cu orice pret \u0018 \u0018 si \u0010mpotriva dorint \u0018ei europenilor s\u0015 continuat \u0018i negocierile de aderare turciei la uniunea european\u0015 \u0010n ciuda refuzului continuu al turciei de recunoa\u0018 ste ciprul \u0018 si \u0010n ciuda faptului c\u0015 reformele democratice au ajuns \u0010ntr-un punct mort figure g.40 formatted dataset example en ro 60context adev\u0015 arul este c\u0015 v\u0015 dorit \u0018i cu orice pret \u0018 \u0018 si \u0010mpotriva dorint \u0018ei europenilor s\u0015 continuat \u0018i negocierile de aderare turciei la uniunea european\u0015 \u0010n ciuda refuzului continuu al turciei de recunoa\u0018 ste ciprul \u0018 si \u0010n ciuda faptului c\u0015 reformele democratice au ajuns \u0010ntr-un punct mort target completion truth want price wishes peoples europe continue negotiations turkey 's accession european union despite turkey 's continuing refusal recognise cyprus despite fact democratic reforms standstill figure g.41 formatted dataset example ro en context q 2 4 6 target completion 48 figure g.42 formatted dataset example arithmetic 1dc context q 17 minus 14 target completion 3 figure g.43 formatted dataset example arithmetic 2d- context q 98 plus 45 target completion 143 figure g.44 formatted dataset example arithmetic 2d+ context q 95 times 45 target completion 4275 figure g.45 formatted dataset example arithmetic 2dx context q 509 minus 488 target completion 21 figure g.46 formatted dataset example arithmetic 3d- context q 556 plus 497 target completion 1053 figure g.47 formatted dataset example arithmetic 3d+ context q 6209 minus 3365 target completion 2844 figure g.48 formatted dataset example arithmetic 4d- 61context q 9923 plus 617 target completion 10540 figure g.49 formatted dataset example arithmetic 4d+ context q 40649 minus 78746 target completion -38097 figure g.50 formatted dataset example arithmetic 5d \u0000 context q 65360 plus 16204 target completion 81564 figure g.51 formatted dataset example arithmetic 5d+ 62h results tasks model sizes zero-shot one-shot few-shot name metric splitfine-tune sota k small med large xl 2.7b 6.7b 13b 175b small med large xl 2.7b 6.7b 13b 175b small med large xl 2.7b 6.7b 13b 175b175b test server hellaswag acc dev 85.6 20 33.7 43.6 51.0 54.7 62.8 67.4 70.9 78.9 33.0 42.9 50.5 53.5 61.9 66.5 70.0 78.1 33.5 43.1 51.3 54.9 62.9 67.3 71.3 79.3 lambada acc test 68.0 15 42.7 54.3 60.4 63.6 67.1 70.3 72.5 76.2 22.0 47.1 52.6 58.3 61.1 65.4 69.0 72.5 22.0 40.4 63.2 57.0 78.1 79.1 81.3 86.4 lambada ppl test 8.63 15 18.6 9.09 6.53 5.44 4.60 4.00 3.56 3.00 165.0 11.6 8.29 6.46 5.53 4.61 4.06 3.35 165.0 27.6 6.63 7.45 2.89 2.56 2.56 1.92 storycloze acc test 91.8 70 63.3 68.5 72.4 73.4 77.2 77.7 79.5 83.2 62.3 68.7 72.3 74.2 77.3 78.7 79.7 84.7 62.3 70.2 73.9 76.1 80.2 81.2 83.0 87.7 nqs acc test 44.5 64 0.64 1.75 2.71 4.40 6.01 5.79 7.84 14.6 1.19 3.07 4.79 5.43 8.73 9.78 13.7 23.0 1.72 4.46 7.89 9.72 13.2 17.0 21.0 29.9 triviaqa acc dev 68.0 64 4.15 7.61 14.0 19.7 31.3 38.7 41.8 64.3 4.19 12.9 20.5 26.5 35.9 44.4 51.3 68.0 6.96 16.3 26.5 32.1 42.3 51.6 57.5 71.2 71.2 webqs acc test 45.5 64 1.77 3.20 4.33 4.63 7.92 7.73 8.22 14.4 2.56 6.20 8.51 9.15 14.5 15.1 19.0 25.3 5.46 12.6 15.9 19.6 24.8 27.7 33.5 41.5 ro en 16 bleu-mb test 39.9 64 2.08 2.71 3.09 3.15 16.3 8.34 20.2 19.9 0.55 15.4 23.0 26.3 30.6 33.2 35.6 38.6 1.25 20.7 25.8 29.2 33.1 34.8 37.0 39.5 ro en 16 bleu-sb test 64 2.39 3.08 3.49 3.56 16.8 8.75 20.8 20.9 0.65 15.9 23.6 26.8 31.3 34.2 36.7 40.0 1.40 21.3 26.6 30.1 34.3 36.2 38.4 41.3 en ro 16 bleu-mb test 38.5 64 2.14 2.65 2.53 2.50 3.46 4.24 5.32 14.1 0.35 3.30 7.89 8.72 13.2 15.1 17.3 20.6 1.25 5.90 9.33 10.7 14.3 16.3 18.0 21.0 en ro 16 bleu-sb test 64 2.61 3.11 3.07 3.09 4.26 5.31 6.43 18.0 0.55 3.90 9.15 10.3 15.7 18.2 20.8 24.9 1.64 7.40 10.9 12.9 17.2 19.6 21.8 25.8 fr en 14 bleu-mb test 35.0 64 1.81 2.53 3.47 3.13 20.6 15.1 21.8 21.2 1.28 15.9 23.7 26.3 29.0 30.5 30.2 33.7 4.98 25.5 28.5 31.1 33.7 34.9 36.6 39.2 fr en 14 bleu-sb test 64 2.29 2.99 3.90 3.60 21.2 15.5 22.4 21.9 1.50 16.3 24.4 27.0 30.0 31.6 31.4 35.6 5.30 26.2 29.5 32.2 35.1 36.4 38.3 41.4 en fr 14 bleu-mb test 45.6 64 1.74 2.16 2.73 2.15 15.1 8.82 12.0 25.2 0.49 8.00 14.8 15.9 20.3 23.3 24.9 28.3 4.08 14.5 19.3 21.5 24.9 27.3 29.5 32.6 en fr 14 bleu-sb test 45.9 64 2.44 2.75 3.54 2.82 19.3 11.4 15.3 31.3 0.81 10.0 18.2 19.3 24.7 28.3 30.1 34.1 5.31 18.0 23.6 26.1 30.3 33.3 35.5 39.9 de en 16 bleu-mb test 40.2 64 2.06 2.87 3.41 3.63 21.5 17.3 23.0 27.2 0.83 16.2 22.5 24.7 28.2 30.7 33.0 30.4 3.25 22.7 26.2 29.2 32.7 34.8 37.3 40.6 de en 16 bleu-sb test 64 2.39 3.27 3.85 4.04 22.5 18.2 24.4 28.6 0.93 17.1 23.4 25.8 29.2 31.9 34.5 32.1 3.60 23.8 27.5 30.5 34.1 36.5 39.1 43.0 en de 16 bleu-mb test 41.2 64 1.70 2.27 2.31 2.43 12.9 8.66 10.4 24.6 0.50 7.00 12.9 13.1 18.3 20.9 22.5 26.2 3.42 12.3 15.4 17.1 20.9 23.0 26.6 29.7 en de 16 bleu-sb test 41.2 64 2.09 2.65 2.75 2.92 13.7 9.36 11.0 25.3 0.54 7.40 13.4 13.4 18.8 21.7 23.3 27.3 3.78 12.9 16.1 17.7 21.7 24.1 27.7 30.9 winograd acc test 93.8 7 66.3 72.9 74.7 76.9 82.4 85.7 87.9 88.3 63.4 68.5 72.9 76.9 82.4 84.6 86.1 89.7 63.4 67.4 73.6 76.9 84.3 85.4 82.4 88.6 winogrande acc dev 84.6 50 52.0 52.1 57.4 58.7 62.3 64.5 67.9 70.2 51.3 53.0 58.3 59.1 61.7 65.8 66.9 73.2 51.3 52.6 57.5 59.1 62.6 67.4 70.0 77.7 piqa acc dev 77.1 50 64.6 70.2 72.9 75.1 75.6 78.0 78.5 81.0 64.3 69.3 71.8 74.4 74.3 76.3 77.8 80.5 64.3 69.4 72.0 74.3 75.4 77.8 79.9 82.3 82.8 arc challenge acc test 78.5 50 26.6 29.5 31.8 35.5 38.0 41.4 43.7 51.4 25.5 30.2 31.6 36.4 38.4 41.5 43.1 53.2 25.5 28.4 32.3 36.7 39.5 43.7 44.8 51.5 arc easy acc test 92.0 50 43.6 46.5 53.0 53.8 58.2 60.2 63.8 68.8 42.7 48.2 54.6 55.9 60.3 62.6 66.8 71.2 42.7 51.0 58.1 59.1 62.1 65.8 69.1 70.1 openbookqa acc test 87.2 100 35.6 43.2 45.2 46.8 53.0 50.4 55.6 57.6 37.0 39.8 46.2 46.4 53.4 53.0 55.8 58.8 37.0 43.6 48.0 50.6 55.6 55.2 60.8 65.4 quac f1 dev 74.4 5 21.2 26.8 31.0 30.1 34.7 36.1 38.4 41.5 21.1 26.9 31.9 32.3 37.4 39.0 40.6 43.4 21.6 27.6 32.9 34.2 38.2 39.9 40.9 44.3 race-h acc test 90.0 10 35.2 37.9 40.1 40.9 42.4 44.1 44.6 45.5 34.3 37.7 40.0 42.0 43.8 44.3 44.6 45.9 34.3 37.0 40.4 41.4 42.3 44.7 45.1 46.8 race-m acc test 93.1 10 42.1 47.2 52.1 52.3 54.7 54.4 56.7 58.4 42.3 47.3 51.7 55.2 56.1 54.7 56.9 57.4 42.3 47.0 52.7 53.0 55.6 55.4 58.1 58.1 squadv2 em dev 90.7 16 22.6 32.8 33.9 43.1 43.6 45.4 49.0 52.6 25.1 37.5 37.9 47.9 47.9 51.1 56.0 60.1 27.5 40.5 39.2 53.5 50.0 56.6 62.6 64.9 squadv2 f1 dev 93.0 16 28.3 40.2 41.4 50.3 51.0 52.7 56.3 59.5 30.1 43.6 44.1 54.0 54.1 57.1 61.8 65.4 32.1 45.5 44.9 58.7 55.9 62.1 67.7 69.8 coqa f1 dev 90.7 5 34.5 55.0 61.8 65.3 71.1 72.8 76.3 81.5 30.6 52.1 61.6 66.1 71.8 75.1 77.9 84.0 31.1 52.0 62.7 66.8 73.2 77.3 79.9 85.0 drop f1 dev 89.1 20 9.40 13.6 14.4 16.4 19.7 17.0 24.0 23.6 11.7 18.1 20.9 23.0 26.4 27.3 29.2 34.3 12.9 18.7 24.0 25.6 29.7 29.7 32.3 36.5 boolq acc dev 91.0 32 49.7 60.3 58.9 62.4 67.1 65.4 66.2 60.5 52.6 61.7 60.4 63.7 68.4 68.7 69.0 76.7 43.1 60.6 62.0 64.1 70.3 70.0 70.2 77.5 76.4 cb acc dev 96.9 32 0.00 32.1 8.93 19.6 19.6 28.6 19.6 46.4 55.4 53.6 53.6 48.2 57.1 33.9 55.4 64.3 42.9 58.9 53.6 69.6 67.9 60.7 66.1 82.1 75.6 cb f1 dev 93.9 32 0.00 29.3 11.4 17.4 22.4 25.1 20.3 42.8 60.1 39.8 45.6 37.5 45.7 28.5 44.6 52.5 26.1 40.4 32.6 48.3 45.7 44.6 46.0 57.2 52.0 copa acc dev 94.8 32 66.0 68.0 73.0 77.0 76.0 80.0 84.0 91.0 62.0 64.0 66.0 74.0 76.0 82.0 86.0 87.0 67.0 64.0 72.0 77.0 83.0 83.0 86.0 92.0 92.0 rte acc dev 92.5 32 47.7 49.8 48.4 56.0 46.6 55.2 62.8 63.5 53.1 47.3 49.5 49.5 54.9 54.9 56.3 70.4 52.3 48.4 46.9 50.9 56.3 49.5 60.6 72.9 69.0 wic acc dev 76.1 32 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 50.0 50.3 50.3 49.2 49.4 50.3 50.0 48.6 49.8 55.0 53.0 53.0 51.6 53.1 51.1 55.3 49.4 wsc acc dev 93.8 32 59.6 56.7 65.4 61.5 66.3 60.6 64.4 65.4 58.7 58.7 60.6 62.5 66.3 60.6 66.3 69.2 58.7 60.6 54.8 49.0 62.5 67.3 75.0 75.0 80.1 multirc acc dev 62.3 32 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 6.09 11.8 16.8 20.8 24.7 23.8 25.0 32.5 30.5 multirc f1a dev 88.2 32 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 45.0 55.9 64.2 65.4 69.5 66.4 69.3 74.8 75.4 record acc dev 92.5 32 70.8 78.5 82.1 84.1 86.2 88.6 89.0 90.2 69.8 77.0 80.7 83.0 85.9 88.0 88.8 90.2 69.8 77.2 81.3 83.1 86.6 87.9 88.9 89.0 90.2 record f1 dev 93.3 32 71.9 79.2 82.8 85.2 87.3 89.5 90.4 91.0 70.7 77.8 81.6 83.9 86.8 88.8 89.7 91.2 70.7 77.9 82.1 84.0 87.5 88.8 89.8 90.1 91.1 superglue average dev 89.0 40.6 47.4 46.8 49.6 50.1 52.3 54.4 58.2 54.4 55.1 56.7 57.8 61.2 59.7 64.3 68.9 50.2 56.2 56.8 60.0 64.3 63.6 66.9 73.2 71.8 anli r1 acc test 73.8 50 33.4 34.2 33.4 33.4 34.2 32.3 33.2 34.6 32.1 31.6 31.9 34.6 30.6 31.6 32.7 32.0 32.1 32.5 30.9 32.5 33.5 33.1 33.3 36.8 anli r2 acc test 50.7 50 33.2 31.9 33.3 33.3 33.8 33.5 33.5 35.4 35.7 33.7 33.2 32.7 32.7 33.9 33.9 33.9 35.7 33.8 32.1 31.4 32.6 33.3 32.6 34.0 anli r3 acc test 48.3 50 33.6 34.0 33.8 33.4 35.3 34.8 34.4 34.5 35.0 32.6 33.0 33.9 34.1 33.1 32.5 35.1 35.0 34.4 35.1 36.0 32.7 33.9 34.5 40.2 2d+ acc n/a 50 0.70 0.65 0.70 0.85 1.10 2.54 15.4 76.9 2.00 0.55 3.15 4.00 12.1 19.6 73.0 99.6 2.00 4.10 3.50 4.50 8.90 11.9 55.5 100.0 2d- acc n/a 50 1.25 1.25 1.25 1.25 1.60 7.60 12.6 58.0 1.15 0.95 1.45 1.95 3.85 11.5 44.6 86.4 1.15 1.45 2.25 2.70 7.35 13.6 52.4 98.9 3d+ acc n/a 50 0.10 0.10 0.05 0.10 0.10 0.25 1.40 34.2 0.15 0.00 0.10 0.30 0.45 0.95 15.4 65.5 0.15 0.45 0.30 0.55 0.75 0.90 8.40 80.4 3d- acc n/a 50 0.05 0.05 0.05 0.05 0.05 0.45 1.35 48.3 0.05 0.15 0.25 0.30 0.55 1.60 6.15 78.7 0.05 0.10 0.15 0.35 0.65 1.05 9.20 94.2 4d+ acc n/a 50 0.05 0.05 0.00 0.00 0.05 0.05 0.15 4.00 0.00 0.00 0.10 0.00 0.00 0.10 0.80 14.0 0.00 0.05 0.05 0.00 0.15 0.15 0.40 25.5 4d- acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.10 7.50 0.00 0.00 0.00 0.00 0.05 0.00 0.50 14.0 0.00 0.05 0.00 0.00 0.10 0.05 0.40 26.8 5d+ acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.45 0.00 0.00 0.00 0.00 0.00 0.00 0.05 9.30 5d- acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.80 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9.90 2dx acc n/a 50 2.20 2.25 2.65 2.10 2.55 5.80 6.15 19.8 1.35 2.35 3.35 2.35 4.75 9.15 11.0 27.4 1.35 2.90 2.70 2.85 4.25 6.10 7.05 29.2 1dc acc n/a 50 1.25 2.95 2.75 0.05 0.30 2.35 0.75 9.75 1.90 2.80 2.85 3.65 6.45 9.15 8.20 14.3 1.70 2.15 3.90 5.75 6.20 7.60 9.95 21.3 cycled letters acc n/a 100 0.62 0.71 2.85 0.00 0.63 1.35 2.58 3.66 1.67 4.36 5.68 6.46 6.25 9.41 15.1 21.7 4.63 9.27 10.7 14.5 16.7 21.9 27.7 37.9 anagrams 1 acc n/a 100 0.10 0.14 0.40 0.00 0.27 0.69 1.16 2.28 0.21 0.61 1.12 1.27 1.60 2.72 3.72 8.62 0.50 1.27 2.13 3.05 3.81 5.49 8.38 15.1 anagrams 2 acc n/a 100 0.81 1.21 2.69 0.01 1.71 3.75 4.53 8.91 1.19 2.62 4.70 4.77 6.97 10.2 14.6 25.9 1.94 4.80 7.59 9.87 12.6 18.9 25.6 39.7 symbol insertion acc n/a 100 0.00 0.00 0.10 0.00 0.05 0.42 0.89 8.26 0.03 0.05 0.57 1.18 1.67 3.46 6.62 45.4 0.11 0.28 2.19 4.18 6.61 11.0 27.3 67.2 reversed words acc n/a 100 0.00 0.01 0.01 0.01 0.02 0.03 0.03 0.09 0.02 0.01 0.01 0.00 0.05 0.07 0.11 0.48 0.00 0.05 0.00 0.17 0.24 0.30 0.42 0.44 sat analogies acc n/a 20 35.6 39.0 45.2 44.1 50.0 49.2 52.7 53.7 30.5 41.2 43.1 46.5 55.1 54.3 53.5 59.1 30.5 40.4 42.8 40.6 48.4 51.9 53.5 65.2 table h.1 scores every task setting model investigate paper 63figure h.1 results superglue tasks figure h.2 results sat task figure h.3 results winograd tasks 64figure h.4 results arithmetic tasks figure h.5 results cloze completion tasks 65figure h.6 results common sense reasoning tasks figure h.7 results qa tasks figure h.8 results reading comprehension tasks figure h.9 results anli rounds 66figure h.10 results scramble tasks figure h.11 results translation tasks 67references adg+16 marcin andrychowicz misha denil sergio gomez matthew w hoffman david pfau tom schaul brendan shillingford nando de freitas learning learn gradient descent gradient descent inadvances neural information processing systems pages 39813989 2016 ai19 wechat ai tr-mt ensemble december 2019 ajf19 roee aharoni melvin johnson orhan firat massively multilingual neural machine translation proceedings 2019 conference north american chapter association computational linguistics human language technologies volume 1 long short papers 2019 bbdiw20 su lin blodgett solon barocas hal daum e iii hanna wallach language technology power critical survey bias nlp arxiv preprint arxiv:2005.14050 2020 bcfl13 jonathan berant andrew chou roy frostig percy liang semantic parsing freebase question-answer pairs proceedings 2013 conference empirical methods natural language processing pages 15331544 2013 bdd+09 luisa bentivogli ido dagan hoa trang dang danilo giampiccolo bernardo magnini fth pascal recognizing textual entailment challenge 2009 bes10 stefano baccianella andrea esuli fabrizio sebastiani sentiwordnet 3.0 enhanced lexical resource sentiment analysis opinion mining lrec volume 10 pages 22002204 2010 bhdd+06 roy bar haim ido dagan bill dolan lisa ferro danilo giampiccolo bernardo magnini idan szpektor second pascal recognising textual entailment challenge 2006 bht+20 yonatan bisk ari holtzman jesse thomason jacob andreas yoshua bengio joyce chai mirella lapata angeliki lazaridou jonathan may aleksandr nisnevich et al experience grounds language arxiv preprint arxiv:2004.10151 2020 blc13 yoshua bengio nicholas l eonard aaron c. courville estimating propagating gradients stochastic neurons conditional computation arxiv 2013 bzb+19 yonatan bisk rowan zellers ronan le bras jianfeng gao yejin choi piqa reasoning physical commonsense natural language arxiv preprint arxiv:1911.11641 2019 car97 rich caruana multitask learning machine learning 28 1 1997 cb78 susan carey elsa bartlett acquiring single new word proceedings stanford child language conference 1978 cce+18 peter clark isaac cowhey oren etzioni tushar khot ashish sabharwal carissa schoenick oyvind tafjord think solved question answering try arc ai2 reasoning challenge arxiv abs/1803.05457 2018 cgrs19 rewon child scott gray alec radford ilya sutskever generating long sequences sparse transformers 2019 chi+18 eunsol choi mohit iyyer mark yatskar wen-tau yih yejin choi percy liang luke zettlemoyer quac question answering context arxiv 2018 clc+19 christopher clark kenton lee ming-wei chang tom kwiatkowski michael collins kristina toutanova boolq exploring surprising difculty natural yes/no questions arxiv preprint arxiv:1905.10044 2019 cly+19 yen-chun chen linjie li licheng yu ahmed el kholy faisal ahmed zhe gan yu cheng jingjing liu uniter learning universal image-text representations arxiv preprint arxiv:1909.11740 2019 cra17 kate crawford trouble bias nips 2017 keynote 2017 dclt18 jacob devlin ming-wei chang kenton lee kristina toutanova bert pre-training deep bidirectional transformers language understanding arxiv preprint arxiv:1810.04805 2018 68 dgm06 ido dagan oren glickman bernardo magnini pascal recognising textual entailment challenge machine learning challenges evaluating predictive uncertainty visual object classication recognising textual entailment pages 177190. springer 2006 dgv+18 mostafa dehghani stephan gouws oriol vinyals jakob uszkoreit lukasz kaiser universal transformers arxiv 2018 dhkh14 nadir durrani barry haddow philipp koehn kenneth heaeld edinburghs phrase-based machine translation systems wmt-14 proceedings ninth workshop statistical machine translation pages 97104 2014 dl15 andrew m. dai quoc v le semi-supervised sequence learning advances neural information processing systems 2015 dmst19 marie-catherine de marneffe mandy simons judith tonhauser commitmentbank investigat- ing projection naturally occurring discourse 2019. appear proceedings sinn und bedeutung 23. data found https //github.com/mcdm/commitmentbank/ dsc+16 yan duan john schulman xi chen peter l. bartlett ilya sutskever pieter abbeel rl2 fast reinforcement learning via slow reinforcement learning arxiv abs/1611.02779 2016 dwd+19 dheeru dua yizhong wang pradeep dasigi gabriel stanovsky sameer singh matt gardner drop reading comprehension benchmark requiring discrete reasoning paragraphs arxiv preprint arxiv:1903.00161 2019 dyy+19 zihang dai zhilin yang yiming yang jaime g. carbonell quoc v le ruslan salakhutdinov transformer-xl attentive language models beyond xed-length context arxiv 2019 eoag18 sergey edunov myle ott michael auli david grangier understanding back-translation scale arxiv preprint arxiv:1808.09381 2018 fal17 chelsea finn pieter abbeel sergey levine model-agnostic meta-learning fast adaptation deep networks arxiv abs/1703.03400 2017 fyo00 yaroslav fyodorov natural logic inference system 2000 gg19 hila gonen yoav goldberg lipstick pig debiasing methods cover systematic gender biases word embeddings remove arxiv preprint arxiv:1903.03862 2019 glt+20 kelvin guu kenton lee zora tung panupong pasupat ming-wei chang realm retrieval- augmented language model pre-training arxiv preprint arxiv:2002.08909 2020 gmdd07 danilo giampiccolo bernardo magnini ido dagan bill dolan third pascal recognizing textual entailment challenge proceedings acl-pascal workshop textual entailment paraphrasing pages 19. association computational linguistics 2007 gra16 alex graves adaptive computation time recurrent neural networks arxiv 2016 gsl+18 suchin gururangan swabha swayamdipta omer levy roy schwartz samuel r bowman noah smith annotation artifacts natural language inference data arxiv preprint arxiv:1803.02324 2018 gsr19 sebastian gehrmann hendrik strobelt alexander m. rush gltr statistical detection visualiza- tion generated text arxiv preprint arxiv 1906.04043 2019 gwc+18 jiatao gu yong wang yun chen kyunghyun cho victor ok li meta-learning low-resource neural machine translation arxiv preprint arxiv:1808.08437 2018 hb20 daniel hernandez tom brown ai efciency may 2020 hbfc19 ari holtzman jan buys maxwell forbes yejin choi curious case neural text degeneration corr abs/1904.09751 2019 hlw+20 dan hendrycks xiaoyuan liu eric wallace adam dziedzic rishabh krishnan dawn song pretrained transformers improve distribution robustness arxiv preprint arxiv:2004.06100 2020 69 hna+17 joel hestness sharan narang newsha ardalani gregory diamos heewoo jun hassan kianinejad md mostofa ali patwary yang yang yanqi zhou deep learning scaling predictable empirically arxiv preprint arxiv:1712.00409 2017 hr18 jeremy howard sebastian ruder universal language model ne-tuning text classication arxiv preprint arxiv:1801.06146 2018 hvd15 geoffrey hinton oriol vinyals jeff dean distilling knowledge neural network arxiv preprint arxiv:1503.02531 2015 hyc01 sepp hochreiter steven younger peter r conwell learning learn using gradient descent ininternational conference articial neural networks pages 8794. springer 2001 hzj+19 po-sen huang huan zhang ray jiang robert stanforth johannes welbl jack rae vishal maini dani yogatama pushmeet kohli reducing sentiment bias language models via counterfactual evaluation arxiv preprint arxiv:1911.03064 2019 ibgc+14 mohit iyyer jordan boyd-graber leonardo claudino richard socher hal daum e iii neural network factoid question answering paragraphs empirical methods natural language processing 2014 idcbe19 daphne ippolito daniel duckworth chris callison-burch douglas eck automatic detection generated text easiest humans fooled arxiv preprint arxiv:1911.00650 2019 jcwz17 mandar joshi eunsol choi daniel s. weld luke zettlemoyer triviaqa large scale distantly supervised challenge dataset reading comprehension arxiv preprint arxiv:1705.03551 2017 jn20 zheng junyuan gamma lab nyc numeric transformer albert march 2020 jvs+16 rafal jozefowicz oriol vinyals mike schuster noam shazeer yonghui wu exploring limits language modeling arxiv preprint arxiv:1602.02410 2016 jys+19 xiaoqi jiao yichun yin lifeng shang xin jiang xiao chen linlin li fang wang qun liu tinybert distilling bert natural language understanding arxiv preprint arxiv:1909.10351 2019 jzc+19 ying ju fubang zhao shijie chen bowen zheng xuefeng yang yunfeng liu technical report conversational question answering arxiv preprint arxiv:1909.10772 2019 kcr+18 daniel khashabi snigdha chaturvedi michael roth shyam upadhyay dan roth looking beyond surface challenge set reading comprehension multiple sentences proceedings north american chapter association computational linguistics naacl 2018 kks+20 daniel khashabi tushar khot ashish sabharwal oyvind tafjord peter clark hannaneh hajishirzi uniedqa crossing format boundaries single qa system arxiv preprint arxiv:2005.00700 2020 kmb20 sarah e. kreps miles mccain miles brundage news thats fabricate ai-generated text tool media misinformation 2020 kmh+20 jared kaplan sam mccandlish tom henighan tom b. brown benjamin chess rewon child scott gray alec radford jeffrey wu dario amodei scaling laws neural language models 2020 kpr+19 tom kwiatkowski jennimaria palomaki olivia redeld michael collins ankur parikh chris alberti danielle epstein illia polosukhin matthew kelcey jacob devlin kenton lee kristina n. toutanova llion jones ming-wei chang andrew dai jakob uszkoreit quoc le slav petrov natural ques- tions benchmark question answering research transactions association computational linguistics 2019 kr16 yoon kim alexander m. rush sequence-level knowledge distillation arxiv 2016 lb02 edward loper steven bird nltk natural language toolkit 2002 lc19 guillaume lample alexis conneau cross-lingual language model pretraining arxiv preprint arxiv:1901.07291 2019 70 lcg+19 zhenzhong lan mingda chen sebastian goodman kevin gimpel piyush sharma radu sori- cut albert lite bert self-supervised learning language representations arxiv preprint arxiv:1909.11942 2019 lch+20 xiaodong liu hao cheng pengcheng weizhu chen yu wang hoifung poon jianfeng gao adversarial training large neural language models arxiv preprint arxiv:2004.08994 2020 ldl19 zhongyang li xiao ding ting liu story ending prediction transferable bert arxiv preprint arxiv:1905.07504 2019 ldm12 hector levesque ernest davis leora morgenstern winograd schema challenge thirteenth international conference principles knowledge representation reasoning 2012 lgg+20 yinhan liu jiatao gu naman goyal xian li sergey edunov marjan ghazvininejad mike lewis luke zettlemoyer multilingual denoising pre-training neural machine translation arxiv preprint arxiv:2001.08210 2020 lgh+15 xiaodong liu jianfeng gao xiaodong li deng kevin duh ye-yi wang representation learning using multi-task deep neural networks semantic classication information retrieval proceedings 2015 conference north american chapter association computational linguistics human language technologies 2015 lh17 ilya loshchilov frank hutter decoupled weight decay regularization arxiv preprint arxiv:1711.05101 2017 lhcg19a xiaodong liu pengcheng weizhu chen jianfeng gao improving multi-task deep neural networks via knowledge distillation natural language understanding arxiv preprint arxiv:1904.09482 2019 lhcg19b xiaodong liu pengcheng weizhu chen jianfeng gao multi-task deep neural networks natural language understanding arxiv preprint arxiv:1901.11504 2019 lin20 tal linzen accelerate progress towards human-like linguistic generalization arxiv preprint arxiv:2005.00955 2020 llg+19 mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence-to-sequence pre-training natural language generation translation comprehension arxiv preprint arxiv:1910.13461 2019 lm17 ke li jitendra malik learning optimize neural nets arxiv preprint arxiv:1703.00441 2017 log+19 yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining approach arxiv preprint arxiv:1907.11692 2019 lpp+20 patrick lewis ethan perez aleksandra piktus fabio petroni vladimir karpukhin naman goyal heinrich k uttler mike lewis wen-tau yih tim rockt aschel sebastian riedel kiela douwe retrieval-augmented generation knowledge-intensive nlp tasks arxiv preprint arxiv:2005.11401 2020 lsp+18 peter j. liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia summarizing long sequences arxiv preprint arxiv:1801.10198 2018 lws+20 zhuohan li eric wallace sheng shen kevin lin kurt keutzer dan klein joseph e. gonzalez train large compress rethinking model size efcient training inference transformers 2020 lxl+17 guokun lai qizhe xie hanxiao liu yiming yang eduard hovy race large-scale reading comprehension dataset examinations arxiv preprint arxiv:1704.04683 2017 lyn+20 sheng-chieh lin jheng-hong yang rodrigo nogueira ming-feng tsai chuan-ju wang jimmy lin tttttackling winogrande schemas arxiv preprint arxiv:2003.08380 2020 mac92 david mackay information-based objective functions active data selection neural computation 1992 71 mbxs17 bryan mccann james bradbury caiming xiong richard socher learned translation con- textualized word vectors advances neural information processing systems pages 62946305 2017 mccd13 tomas mikolov kai chen greg corrado jeffrey dean efcient estimation word representations vector space arxiv preprint arxiv:1301.3781 2013 mch+16 nasrin mostafazadeh nathanael chambers xiaodong devi parikh dhruv batra lucy vanderwende pushmeet kohli james allen corpus evaluation framework deeper understanding commonsense stories arxiv preprint arxiv:1604.01696 2016 mcks18 todor mihaylov peter clark tushar khot ashish sabharwal suit armor conduct electricity new dataset open book question answering arxiv abs/1809.02789 2018 mkat18 sam mccandlish jared kaplan dario amodei openai dota team empirical model large-batch training 2018 mkm+94 mitchell marcus grace kim mary ann marcinkiewicz robert macintyre ann bies mark ferguson karen katz britta schasberger penn treebank annotating predicate argument structure inproceedings workshop human language technology pages 114119. association computational linguistics 1994 mkxs18 bryan mccann nitish shirish keskar caiming xiong richard socher natural language decathlon multitask learning question answering arxiv preprint arxiv:1806.08730 2018 mpl19 r thomas mccoy ellie pavlick tal linzen right wrong reasons diagnosing syntactic heuristics natural language inference arxiv preprint arxiv:1902.01007 2019 mwz+18 margaret mitchell simone wu andrew zaldivar parker barnes lucy vasserman ben hutchinson elena spitzer inioluwa deborah raji timnit gebru model cards model reporting 2018 nbr20 moin nadeem anna bethke siva reddy stereoset measuring stereotypical bias pretrained language models arxiv preprint arxiv:2004.09456 2020 nk19 timothy niven hung-yu kao probing neural network comprehension natural language arguments arxiv preprint arxiv:1907.07355 2019 nor09 peter norvig natural language corpus data 2009 nvnvdg19 malvina nissim rik van noord rob van der goot fair better sensational man doctor woman doctor arxiv preprint arxiv:1905.09866 2019 nwd+19 yixin nie adina williams emily dinan mohit bansal jason weston douwe kiela adversarial nli new benchmark natural language understanding arxiv preprint arxiv:1910.14599 2019 or16 university regensburg fascha 2016 pcc18 mohammad taher pilehvar jose camacho-collados wic 10,000 example pairs evaluating context-sensitive representations arxiv preprint arxiv:1808.09121 2018 pfb18 jason phang thibault f evry samuel r. bowman sentence encoders stilts supplementary training intermediate labeled-data tasks arxiv preprint arxiv:1811.01088 2018 phr+18 adam poliak aparajita haldar rachel rudinger j. edward hu ellie pavlick aaron steven white benjamin van durme collecting diverse natural language inference problems sentence representation evaluation proceedings emnlp 2018 pkl+16 denis paperno germ kruszewski angeliki lazaridou quan ngoc pham raffaella bernardi sandro pezzelle marco baroni gemma boleda raquel fern andez lambada dataset word prediction requiring broad discourse context arxiv preprint arxiv:1606.06031 2016 pnzty18 matthew e. peters mark neumann luke zettlemoyer wen tau yih dissecting contextual word embeddings architecture representation 2018 pos18 matt post call clarity reporting bleu scores arxiv preprint arxiv:1804.08771 2018 72 psm14 jeffrey pennington richard socher christopher manning glove global vectors word representation proceedings 2014 conference empirical methods natural language processing emnlp 2014 qia20 qianxin sa-net albert ensemble april 2020 qmzh19 yusu qian urwa muaz ben zhang jae hyun reducing gender bias word-level language models gender-equalizing loss function arxiv preprint arxiv:1905.12801 2019 rbg11 melissa roemmele cosmin adrian bejan andrew gordon choice plausible alternatives evaluation commonsense causal reasoning 2011 aaai spring symposium series 2011 rcm19 siva reddy danqi chen christopher manning coqa conversational question answering challenge transactions association computational linguistics 7:249266 2019 rcp+17 scott reed yutian chen thomas paine aron van den oord sm eslami danilo rezende oriol vinyals nando de freitas few-shot autoregressive density estimation towards learning learn distributions arxiv preprint arxiv:1710.10304 2017 rjl18 pranav rajpurkar robin jia percy liang know dont know unanswerable questions squad arxiv preprint arxiv:1806.03822 2018 rl16 sachin ravi hugo larochelle optimization model few-shot learning iclr 2017 oral 2016 rll+19 qiu ran yankai lin peng li jie zhou zhiyuan liu numnet machine reading comprehension numerical reasoning proceedings emnlp 2019 rnlvd18 rachel rudinger jason naradowsky brian leonard benjamin van durme gender bias coreference resolution arxiv preprint arxiv:1804.09301 2018 rnss18 alec radford karthik narasimhan tim salimans ilya sutskever improving language understanding generative pre-training 2018 ros12 r.s ross guide conducting risk assessments nist special publication 2012 rrbs19 jonathan s. rosenfeld amir rosenfeld yonatan belinkov nir shavit constructive prediction generalization error across scales 2019 rrs20 adam roberts colin raffel noam shazeer much knowledge pack parameters language model arxiv preprint arxiv:2002.08910 2020 rsr+19 colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j. liu exploring limits transfer learning unied text-to-text transformer 2019 rwc+19 alec radford jeffrey wu rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners 2019 sbbc19 keisuke sakaguchi ronan le bras chandra bhagavatula yejin choi winogrande adversarial winograd schema challenge scale 2019 sbc+19 irene solaiman miles brundage jack clark amanda askell ariel herbert-v oss jeff wu alec radford gretchen krueger jong wook kim sarah kreps miles mccain alex newhouse jason blazakis kris mcgufe jasmine wang release strategies social impacts language models 2019 scnp19 emily sheng kai-wei chang premkumar natarajan nanyun peng woman worked babysitter biases language generation arxiv preprint arxiv:1909.01326 2019 sdcw19 victor sanh lysandre debut julien chaumond thomas wolf distilbert distilled version bert smaller faster cheaper lighter arxiv preprint arxiv:1910.01108 2019 sdse19 roy schwartz jesse dodge noah a. smith oren etzioni green ai corr abs/1907.10597 2019 shb15 rico sennrich barry haddow alexandra birch improving neural machine translation models monolingual data arxiv preprint arxiv:1511.06709 2015 73 smm+17 noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc le geoffrey hinton jeff dean outrageously large neural networks sparsely-gated mixture-of-experts layer arxiv preprint arxiv:1701.06538 2017 spp+19 mohammad shoeybi mostofa patwary raul puri patrick legresley jared casper bryan catanzaro megatron-lm training multi-billion parameter language models using model parallelism 2019 ss20 timo schick hinrich sch utze exploiting cloze questions few-shot text classication natural language inference arxiv preprint arxiv:2001.07676 2020 stq+19 kaitao song xu tan tao qin jianfeng lu tie-yan liu mass masked sequence sequence pre-training language generation arxiv preprint arxiv:1905.02450 2019 tfr+17 josh tobin rachel fong alex ray jonas schneider wojciech zaremba pieter abbeel domain randomization transferring deep neural networks simulation real world 2017 ieee/rsj international conference intelligent robots systems iros pages 2330. ieee 2017 tl05 peter d. turney michael l. littman corpus-based learning analogies semantic relations corr abs/cs/0508103 2005 tl18 trieu h. trinh quoc v le simple method commonsense reasoning arxiv preprint arxiv:1806.02847 2018 tlbs03 peter d. turney michael l. littman jeffrey bigham victor shnayder combining independent modules solve multiple-choice synonym analogy problems corr cs.cl/0309035 2003 tur20 project turing microsoft research blog feb 2020 vbl+16 oriol vinyals charles blundell timothy lillicrap daan wierstra et al matching networks one shot learning advances neural information processing systems pages 36303638 2016 vsp+17 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n. gomez ukasz kaiser illia polosukhin attention need advances neural information processing systems 2017 wpn+19 alex wang yada pruksachatkun nikita nangia amanpreet singh julian michael felix hill omer levy samuel bowman superglue stickier benchmark general-purpose language understand- ing systems advances neural information processing systems pages 32613275 2019 wxh+18 yiren wang yingce xia tianyu fei tian tao qin chengxiang zhai tie-yan liu multi-agent dual learning iclr 2019 2018 xdh+19 qizhe xie zihang dai eduard hovy minh-thang luong quoc v le unsupervised data augmentation consistency training 2019 ydc+19 dani yogatama cyprien de masson dautume jerome connor tomas kocisky mike chrzanowski lingpeng kong angeliki lazaridou wang ling lei yu chris dyer et al learning evaluating general linguistic intelligence arxiv preprint arxiv:1901.11373 2019 ydy+19 zhilin yang zihang dai yiming yang jaime carbonell ruslan salakhutdinov quoc v le xlnet generalized autoregressive pretraining language understanding arxiv preprint arxiv:1906.08237 2019 zhb+19 rowan zellers ari holtzman yonatan bisk ali farhadi yejin choi hellaswag machine really nish sentence arxiv preprint arxiv:1905.07830 2019 zhr+19 rowan zellers ari holtzman hannah rashkin yonatan bisk ali farhadi franziska roesner yejin choi defending neural fake news arxiv preprint arxiv:1905.12616 2019 zll+18 sheng zhang xiaodong liu jingjing liu jianfeng gao kevin duh benjamin van durme record bridging gap human machine commonsense reading comprehension arxiv preprint arxiv:1810.12885 2018 zsw+19a daniel m. ziegler nisan stiennon jeffrey wu tom b. brown alec radford dario amodei paul christiano geoffrey irving fine-tuning language models human preferences 2019 74 zsw+19b daniel m. ziegler nisan stiennon jeffrey wu tom b. brown alec radford dario amodei paul chris- tiano geoffrey irving fine-tuning language models human preferences arxiv abs/1909.08593 2019 75\n",
      "\n",
      "\n",
      "Book 2 - Title: Explorations in Artificial Intelligence and Machine Learning, Author: Prof. Roberto V. Zicari\n",
      "explorations artificial intelligence machine learning crc press freebookintroduction prof. roberto v. zicari 1 introduction machine learning 2 bayesian approach machine learning 3 revealing introduction hidden markov models 4 introduction reinforcement learning 5 deep learning feature representation 6 neural networks deep learning 7 ai-completeness problem domain super-intelligent machinesread latest artificial intelligence machine learning key titles visit www.crcpress.com/computer-science-engineering browse full range titles save 20 get free shipping discount code odb18 introduction prof. roberto v. zicari frankfurt big data lab goethe university frankfurt editor odbms.org artificial intelligence ai seems defining technology time google re-branded google research division google ai company pursues developments field artificial intelligence john mccarthy defines ai back 1956 like `` ai involves machines perform tasks characteristic human intelligence '' free book gives brief introduction artificial intelligence machine learning deep learning main differences artificial intelligence machine learning deep learning put simply machine learning way achieving ai arthur samuel 's definition machine learning ml 1959 `` machine learning field study gives computers ability learn without explicitly programmed '' typical problems solved machine learning regression classification segmentation network analysis changed dramatically since pioneering days rise big data computing power making possible analyze massive amounts data scale ai needs big data machine learning scale machine learning way training algorithm learn huge amounts data used train algorithms allowing algorithms '' learn '' improve deep learning subset machine learning inspired structure function brain.as example artificial neural networks anns algorithms resemble biological structure brain namely interconnecting many neurons free book gives gentle introduction machine learning lists various ml approaches decision tree learning hidden markov models reinforcement learning bayesian networks well covering aspects deep learning relates ai help achieve understanding advances field ai machine learning giving idea specific skills 'll need get started wish work machine learning engineer editor prof. dott-ing roberto v. zicari full professor database information systems frankfurt university 15 years representative omg europe previously zicari served associate professor politecnico di milano italy visiting scientist ibm almaden research center usa university california berkeley usa visiting professor epfl lausanne switzerland national university mexico city mexico copenhagen business school odbms.org portal launched 2005 odbms.org created serve faculty students educational research institutions well software developers open source community commercial companies designed meet fast-growing need resources focusing big data analytical data platforms cloud platforms graphs databases in-memory databases newsql databases nosql databases object databases object-relational bindings rdf stores service platforms new approaches concurrency controlintroduction machine learning chapter excerpted machine learning algorithmic perspective 2nd ed stephen marsland 2018 taylor francis group rights reserved.1 learn moresuppose website selling software youve written want make website personalised user start collect data visitors computer type/operating system web browser country live time day visited website get data visitor people actually buy something know bought paid say paypal credit card person buys something website list data looks like computer type web browser country time software bought paid instance rst three pieces data collect could macintosh os x safari uk morning supergame1 credit card windows xp internet explorer usa afternoon supergame1 paypal windows vista firefox nz evening supergame2 paypal based data would like able populate things might inter- ested box within webpage shows software might relevant visitor based data access webpage loads i.e. computer os country time day hope people visit website store data able identify trends macintosh users new zealand nz love rst game firefox users often knowledge- able computers want automatic download application virus/internet worm detector etc collected large set data start examine work problem one prediction given data predict next person buy reason think might work people seem similar often act similarly actually go solving problem one fundamental problems book tries solve example called supervised learning know right answers examples software actually bought give learner examples know right answer talk supervised learning section 1.3 1.1 data mass earth would black hole around world computers capture store terabytes data every day even leaving aside collection mp3s holiday photographs computers belonging shops banks hospitals scientic laboratories many storing data incessantly example banks building pictures people spend money hospitals recording treatments patients ailments respond engine monitoring systems cars recording information engine order detect might fail challenge something useful data banks computers learn spending patterns detect credit card fraud quickly hospitals share data treatments dont work well expected identied quickly intelligent car give early warning problems dont end stranded worst part town questions machine learning methods used answer science also taken advantage ability computers store massive amounts data biology led way ability measure gene expression dna microar- rays producing immense datasets along protein transcription data phylogenetic trees relating species however sciences slow follow astronomy uses digital telescopes night worlds observatories stor- ing incredibly high-resolution images night sky around terabyte per night equally medical science stores outcomes medical tests measurements diverse mag- netic resonance imaging mri scans simple blood tests explosion stored data well known challenge something useful data large hadron collider cern apparently produces 25 petabytes data per year size complexity datasets mean humans unable extract useful information even way data stored works us given le full numbers minds generally turn away looking long take data plot graph something compare table graph shown figure 1.1 graph rather easier look deal unfortunately three-dimensional world doesnt let us much data higher dimensions even simple webpage data collected four dierent features plotted one dimension feature wed need four dimensions two things reduce number dimensions simple brains deal problem use computers dont know high-dimensional problems dicult dont get bored looking massive data les numbers two pictures figure 1.2 demonstrate one problem reducing number dimensions technically projecting fewer dimensions hide useful information make things look rather strange one reason machine learning becoming popular problems human limitations go away make computers dirty work us one thing help number dimensions much larger three use glyphsthat use representations size colour datapoints represent information dimension help dataset 100 dimensions fact probably interacted machine learning algorithms time used many software programs use microsofts infamous paperclip oce maybe positive example spam lters voice recognition software lots computer games also part automatic number-plate recog- nition systems petrol station security cameras toll roads used anti-skid braking vehicle stability systems even part set algorithms decide whether bank give loan attention-grabbing title section would true data heavy hard work much data actually worlds computers estimated 2012 2.8 zettabytes 2.81021bytes 160exabytes 1601018bytes data created stored 2006 projected reach 40 zettabytes 2020. however make black hole size earth wouldx1x2class 0.111 0.150.22 0.480.63 0.10.61 0.20.152 0.50.553 0.211 0.30.252 0.520.63 0.30.61 0.40.22 0.520.53 figure 1.1 set datapoints numerical values points plotted graph easier us visualise data see table data three dimensions cant view figure 1.2 two views two wind turbines te apiti wind farm ashhurst new zealand taken angle 30to two-dimensional projections three-dimensional objects hides information.take mass 401035grams data would heavy couldnt possibly lift data pen let alone computer section title true however interestingly machine learning report estimated gure 2.8 zettabytes big data bigger digital shadows biggest growth far east john gantz david reinsel sponsored emc corporation also reported quarter data could produce useful information around 3 tagged less 0.5 actually used analysis 1.2 learning delve much topic lets step back think learning actually key concept need think machines learning data since data terabytes cases however isnt large step put human behavioural terms talk learning experience hopefully agree humans animals display behaviours label intelligent learning experience learning gives us exibility life fact adjust adapt new circumstances learn new tricks matter old dog important parts animal learning book remembering adapting generalising recognising last time situation saw data tried particular action gave output worked correct well try didnt work well try something dierent last word generalising recognising similarity dierent situations things applied one place used another makes learning useful use knowledge lots dierent places course plenty bits intelligence reasoning logical deduction wont worry much interested fun- damental parts intelligencelearning adaptingand model computer also lot interest making computers reason deduce facts basis early articial intelligence sometimes known sym- bolic processing computer manipulates symbols reect environment contrast machine learning methods sometimes called subsymbolic symbols symbolic manipulation involved 1.2.1 machine learning machine learning making computers modifyoradapttheir actions whether actions making predictions controlling robot actions get accurate accuracy measured well chosen actions reect correct ones imagine playing scrabble game computer might beat every time beginning lots games starts beating nally never win either getting worse computer learning win scrabble learnt beat go use strategies players doesnt start scratch new player form generalisation past decade inherent multi-disciplinarity machine learning recognised merges ideas neuroscience biology statistics mathematics physics make computers learn fantastic existence proof learning possible bag water electricity together trace chemicals sitting ears section 3.1 brief peek inside seeif anything borrow/steal order make machine learning algorithms turns neural networks grown exactly although even father wouldnt recognise developments seen reinterpreted statistical learners another thing driven change direction machine learning research data mining looks extraction useful information massive datasets men computers pocket protectors rather pickaxes hard hats requires ecient algorithms putting emphasis back onto computer science thecomputational complexity machine learning methods also interest us since producing algorithms particularly important might want use methods large datasets algorithms high- degree polynomial complexity size dataset worse problem complexity often broken two parts complexity training complexity applying trained algorithm training happen often usually time critical take longer however often want decision test point quickly potentially lots test points algorithm use needs low computational cost 1.3 types machine learning intheexamplethatstartedthechapter yourwebpage theaimwastopredictwhatsoftware visitor website might buy based information collect couple interesting things rst data might useful know software visitors bought old however possible get information web browser even cookies cant tell old somebody cant use information picking variables want use calledfeaturesin jargon important part nding good solutions problems something talk several places book equally choosing process data important seen example time access computer store nearest millisecond isnt useful since would like spot similar patterns users reason example chose quantiseit one set morning afternoon evening night obviously need ensure times correct time zones going loosely dene learning meaning getting better task practice leads couple vital questions computer know whether getting better know improve several dierent possible answers questions produce dierent types machine learning consider question knowing whether machine learning tell algorithm correct answer problem gets right next time would happen webpage example since know software person bought hope tell right answers work get correct answers problems generalise alternatively tell whether answer correct nd correct answer searchfor right answer variant give score answer according correct rather right wrong response finally might correct answers want algorithm nd inputs something common dierent answers question provide useful way classify dierent algorithms talking supervised learning atraining set examples correct responses targets provided based training set algorithm generalises respond correctly possible inputs also called learning exemplars unsupervised learning correct responses provided instead algorithm tries identify similarities inputs inputs something common categorised together statistical approach unsupervised learning known density estimation reinforcement learning somewhere supervised unsupervised learn- ing algorithm gets told answer wrong get told correct explore try dierent possibilities works get answer right reinforcement learning sometime called learning critic monitor scores answer suggest improvements evolutionary learning biological evolution seen learning process biological organisms adapt improve survival rates chance ospring environment well look model computer using idea tness corresponds score good current solution common type learning supervised learning going focus next chapters get started well look kinds problems solved using 1.4 supervised learning already suggested webpage example typical problem supervised learning set data training data consists set inputdata hastargetdata answer algorithm produce attached usually written set data xi ti inputs xi targets ti theiindex suggests lots pieces data indexed irunning 1to upper limit n. note inputs targets written boldface font signify vectors since piece data values several dierent features notation used book described detail section 2.1. examples every possible piece input data could put together big look-up table would need machine learning thing makes machine learning better generalisation algorithm produce sensible outputs inputs werent encountered learning also result algorithm deal withnoise small inaccuracies data inherent measuring real world process hard specify rigorously generalisation means lets see example helps 1.4.1 regression suppose gave following datapoints asked tell value output call ysince target datapoint x= 0.44 x andyare written boldface font since scalars opposed vectors .figure 1.3 top left datapoints sample problem bottom left two possible ways predict values known datapoints connecting points straight lines using cubic approximation case misses points top bottom right two complex approximators see text details pass points although lower one rather better top xt 0 0 0.5236 1.5 1.0472 -2.5981 1.5708 3.0 2.0944 -2.5981 2.6180 1.5 3.1416 0 since value x= 0.44isnt examples given need nd way predict value assume values come sort function try nd function youll able give output value yfor given value ofx known regression problem statistics mathematical function describing curve curve passes close possible datapoints generally problem function approximation orinterpolation working value values know problem work function choose look figure 1.3. top-left plot shows plot 7 values xandyin table plots show dierent attempts curve datapoints bottom-left plot shows two possible answers found using straight lines connect points also happens try use cubic function something written ax3+bx2+cx+d= 0 top-right plot shows happens try match function using dierent polynomial time form ax10+bx9+ ... +jx+k= 0 nally bottom-right plot shows function y= 3 sin 5x functions would choose straight-line approximation probably isnt want since doesnt tell us much data however cubic plot set axes terrible doesnt get anywhere near datapoints plot top-right looks like goes datapoints exactly wiggly look value y-axis goes 100 instead around three gures fact data made sine function plotted bottom-right correct answer case algorithm doesnt know two solutions right look equally good way tell solution better test well generalise pick value datapoints use curves predict value see better tell us bottom-right curve better example one thing machine learning algorithms interpolate dat- apoints might seem intelligent behaviour even dicult two dimensions rather harder higher dimensional spaces thing true thing algorithms classicationgrouping examples dierent classeswhich discussed next however algorithms learning denition adapt performance improves surprising often real problems want solve reduced classication regression problems 1.4.2 classication classication problem consists taking input vectors deciding nclasses belong based training exemplars class important point abouttheclassicationproblemisthatitisdiscreteeachexamplebelongstopreciselyone class set classes covers whole possible output space two constraints necessarily realistic sometimes examples might belong partially two dierent classes fuzzyclassiers try solve problem wont talking book addition many places might able categorise every possible input example consider vending machine use neural network learn recognise dierent coins train classier recognise new zealand coins british coin put machine case classier identify new zealand coin closest appearance really wanted rather classier identify one coins trained called novelty detection well assume receive inputs classify accurately lets consider set coin classier coin pushed slot machine takes measurements could include diameter weight possibly shape featuresthat generate input vector case input vector three elements number showing measurement feature choosing number represent shape would involve anencoding example 1=circle 2=hexagon etc. course many features could measure vending machine included atomic absorption spectroscope could estimate density material composition camera could take photograph coin feed image classier question features choose always easy one dont want use many inputs make training classier take longer also number input dimensions grows number datapoints required increasesfigure 1.4 new zealand coins figure 1.5 left set straight line decision boundaries classication problem right alternative set decision boundaries separate plusses light- ening strikes better requires line isnt straight faster known curse dimensionality discussed section 2.1.2 need make sure reliablyseparate classes based features example ifwetriedtoseparatecoinsbasedonlyoncolour wewouldntgetveryfar 20 50 coins silver 1 2 coins bronze however use colour diameter pretty good job coin classication problem nz coins features entirely useless example knowing coin circular doesnt tell us anything nz coins circular see figure 1.4 countries though could useful methods performing classication see book dierent ways learn solution essence aim thing nd decision boundaries used separate dierent classes given features used inputs classier need identify values features enable us decide class current input figure 1.5 shows set 2d inputs three dierent classes shown two dierent decision boundaries left straight lines therefore simple dont categorise well non-linear curve right seen two types problem lets take look whole process machine learning practitioners viewpoint.1.5 machine learning process section assumes problem interested using machine learning coin classication described previously briey examines process machine learning algorithms selected applied evaluated problem data collection preparation throughout book fortunate position datasets readily available downloading using test algorithms course less commonly case desire learn new problem either data collected scratch least assembled prepared fact problem completely new appropriate data chosen process merged next step feature selection required data collected typically done assembling reasonably small dataset features believe might useful experimenting choosing best features collecting analysing full dataset often diculty large amount data mightbe relevant hard collect either requires many measurements taken variety places formats merging appropriately dicult ensuring clean signicant errors missing data etc supervised learning target data also needed require involvement experts relevant eld signicant investments time finally quantity data needs considered machine learning algorithms need signicant amounts data preferably without much noise increased dataset size comes increased computational costs sweet spot enough data without excessive computational overhead generally impossible predict feature selection example part process given section 1.4.2 looked possible features might useful coin recognition consists identifying features useful problem examination invariably requires prior knowledge problem data common sense used coins example identify potentially useful features exclude others well identication features useful learner also necessary features collected without signicant expense time robustto noise corruption data may arise collection process algorithm choice given dataset choice appropriate algorithm algo- rithms book able prepare knowledge underlying principles algorithm examples use precisely required parameter model selection many algorithms parameters havetobesetmanually orthatrequireexperimentationtoidentifyappropriatevalues requirements discussed appropriate points book.training given dataset algorithm parameters training simply use computational resources order build model data order predict outputs new data evaluation system deployed needs tested evaluated ac- curacy data trained often include comparison human experts eld selection appropriate metrics compari- son 1.6 note programming thisbookisaimedathelpingyouunderstandandusemachinelearningalgorithms andthat means writing computer programs book contains algorithms pseudocode fragments python programs based numpy appendix provides introduction python numpy beginner website provides complete working code algorithms understanding use machine learning algorithms ne theory without testingtheprogramsondata andseeingwhattheparametersdo youwontgetthecomplete picture general writing code always best way check understand algorithm nding unexpected details unfortunately debugging machinelearningcodeisevenharderthangeneral debugging quite easy make program compiles runs doesnt seem actually learn case need start testing theprogram carefully however youcan quickly getfrustratedwiththefactthat becausesomanyofthealgorithmsare stochastic theresults repeatable anyway temporarily avoided setting random number seed eect making random number generator follow pattern time seen following example running code python command line marked 10 numbers appear seed set cases would carry forever pseudo-random numbers computers generate section 15.1.1 importnumpyasnp np.random.seed 4 np.random.rand 10 array 0.96702984 0.54723225 0.97268436 0.71481599 0.69772882 0.2160895 ,0.97627445 0.00623026 0.25298236 0.43479153 np.random.rand 10 array 0.77938292 0.19768507 0.86299324 0.98340068 0.16384224 0.59733394 0.0089861 ,0.38657128 0.04416006 0.95665297 np.random.seed 4 np.random.rand 10 array 0.96702984 0.54723225 0.97268436 0.71481599 0.69772882 0.2160895 ,0.97627445 0.00623026 0.25298236 0.43479153 way run randomness avoided parameters another thing useful use 2d toy datasets plot things since see whether something unexpected going addition thesedatasets made simple separable straight line well see chapter 3 see whether deals simple cases least another way cheat temporarily include target one inputs algorithm really excuse getting wrong answer finally reference program works compare also useful hope code book website help people get unexpected traps strange errors 1.7 roadmap book far possible book works general specic simple complex keeping related concepts nearby chapters given focus algorithms encouraging use experimentation rather starting underlying statistical concepts book starts older reasonably simple algorithms examples supervised learning chapter 2 follows many concepts introductory chapter order highlight overarching ideas machine learning thus data requirements well providing material basic probability statistics required readers included completeness chapters 3 4 5 follow main historical sweep supervised learning using neural networks well introducing concepts interpolation followed chap- ters dimensionality reduction chapter 6 use probabilistic methods like em algorithm nearest neighbour methods chapter 7 idea optimal decision boundaries kernel methods introduced chapter 8 focuses support vector machine related algorithms one underlying methods many preceding algorithms optimisation surveyed briey chapter 9 returns material chapter 4 considerthemulti-layerperceptronpurelyfromthepointofviewofoptimisation.thechap- ter continues considering search discrete analogue optimisation leads naturally evolutionary learning including genetic algorithms chapter 10 reinforce- ment learning chapter 11 tree-based learners chapter 12 search-based methods methods combine predictions many learners often trees described chapter 13. important topic unsupervised learning considered chapter 14 fo- cuses self-organising feature map many unsupervised learning algorithms also presented chapter 6. remaining four chapters primarily describe modern statistically based approaches machine learning although algorithms completely new following introduction markov chain monte carlo techniques chapter 15 area graphical models surveyed comparatively old algorithms hidden markov model kalman filter included along particle lters bayesian networks ideas behind deep belief networks given chapter 17 starting historical idea symmetric networks hopeld network introduction gaussian processes given chapter 18. finally introduction python numpy given appendix sucient enable readers follow code descriptions provided book use code supplied book website assuming programming experience programming language iwouldsuggestthatchapters2to4containenoughintroductorymaterialtobeessentialfor anybody looking introduction machine learning ideas introductory one semester course would follow chapters 6 8 use second half chapter 9 introduce chapters 10 11 chapter 14. advanced course would certainly take chapters 13 15 18 along optimisation material chapter 9. attempted make material reasonably self-contained relevant mathematical ideas either included text appropriate point reference material covered means reader prior knowledge certainly nd parts safely ignored skimmed without loss reading dierent statistical example-based take machine learning look chapter 1 t. hastie r. tibshirani j. friedman elements statistical learning 2nd edition springer berlin germany 2008. texts provide alternative views similar material include chapter 1 r.o duda p.e hart d.g stork pattern classication 2nd edition wiley-interscience new york usa 2001. chapter 1 s. haykin neural networks comprehensive foundation 2nd edition prentice-hall new jersey usa 1999.the bayesian approach machine learning chapter excerpted first course machine learning second edition simon rogers mark girolami 2018 taylor francis group rights reserved.2 learn morein previous chapter saw explicitly adding noise model allowed us obtain point predictions particular able quantify uncertainty present parameter estimates subsequent predictions content idea uncertainty parameter estimates small step towards considering parameters random variables bayesian methods becoming increasingly important within machine learning devote next two chapters providing introduction area many people nd challenging chapter cover fundamental ideas bayesian statistics two examples unfortunately calculations required perform bayesian inference often analytically tractable chapter 4 introduce three approximation methods popular machine learning community 3.1 coin game imagine walking around fairground come across stall cus- tomers taking part coin tossing game stall owner tosses coin ten times customer coin lands heads six fewer occasions customer wins back 1 stake plus additional 1 seven stall owner keeps money binomial distribution described section 2.3.2 describes probability certain number successes heads nbinary events probability yheads ntosses toss lands heads probability ris given p y=y =\u0012n y\u0013 ry 1\u0000r n\u0000y 3.1 assume coin fair therefore set r= 0:5. n= 10 tosses probability distribution function seen figure 3.1 bars corre- sponding y\u00146 shaded using equation 3.1 possible calculate probability winning game i.e probability yis less equal01234567891000.050.10.150.20.25 yp figure 3.1 binomial density function equation 3.1 n= 10 andr= 0:5. 6 p y\u00146 p y\u00146 1\u0000p 6 1\u0000 p y= 7 +p y= 8 +p y= 9 +p y= 10 1\u0000 0:1172 0:0439 0:0098 0:0010 0:8281 seems like pretty good game 'll double money probability 0.8281. also possible compute expected return playing game expected value function f x random variable xis computed introduced section 2.2.8 ep x ff x g=x xf x p x summation possible values random variable take letxbe random variable takes value 1 win value 0 lose p x= 1 =p y\u00146 win x= 1 get return 2 original stake plus extra 1 sof 1 2. lose get return nothing sof 0 0. hence expected return f 1 p x= 1 +f 0 p x= 0 2\u0002p y\u00146 0\u0002p 6 1 6562 given costs 1 play win average 1:6562 \u00001 approximately 66 p per game played 100 times 'd expect walk away pro 65.62 given odds success seems sensible play however whilst waiting notice stall owner looks reasonably wealthy customers seem tobe winning perhaps assumptions underlying calculations wrong assumptions 1. number heads modelled random variable binomial distribution probability head particular toss r. 2. coin fair probability heads probability tails r= 0:5. seems hard reject binomial distribution events taking place two possible outcomes tosses seem independent leaves r probability coin lands heads assumption coin fair probability heads equal probability tails maybe case investigate treat ras parameter like wand\u001b2in previous chapter data 3.1.1 counting heads three people queue play rst one plays gets following sequence heads tails h h h h h h h h h 0 1 2 3 4 5 6 7 8 91000.050.10.150.20.250.30.350.4 yp figure 3.2 binomial density function equation 3.1 n= 10 andr= 0:9. nine heads one tail possible compute maximum likelihood value ras follows likelihood given binomial distribution p y=yjr n =\u0012n 9\u0013 ry 1\u0000r n\u0000y 3.2 taking natural logarithm gives l= logp y=yjr n log\u0012n 9\u0013 +ylogr+ n\u0000y log 1\u0000r chapter 2 di erentiate expression equate zero solve maximum likelihood estimate parameter l r=y r\u0000n\u0000y 1\u0000r= 0 1\u0000r =r n\u0000y y=rn r=y n substituting y= 9 andn= 10 gives r= 0:9. corresponding distribution function shown figure 3.2 recalculated probability winning p y\u0014 6 0:0128. much lower r= 0:5. expected return 2\u0002p y\u00146 0\u0002p 6 0:0256 given costs 1 play expect make 0:0256 \u00001 =\u00000:9744 per game loss approximately 97p p y\u00146 0:0128 suggests 1 person every 100 win seem ected number people arewinning although evidence run coin tosses suggests r= 0:9 seems biased given several people 3.1.2 bayesian way value rcomputed previous section based ten tosses given random nature coin toss observed several sequences tosses likely would get di erent reach time thought way rfeels bit like random variable r. maybe learn something distribution ofrrather try nd particular value saw previous section obtaining exact value counting heavily uenced particular tosses short sequence matter many sequences observe always uncertainty r considering random variable associated distribution help us measure understand uncertainty particular de ning random variable ynto number heads ob- tained inntosses would like distribution rconditioned value yn p rjyn given distribution would possible compute expected probability winning taking expectation p ynew\u00146jr respect p rjyn p ynew\u00146jyn =z p ynew\u00146jr p rjyn dr whereynewis random variable describing number heads future set ten tosses section 2.2.7 gave brief introduction bayes rule bayes rule allows us reverse conditioning two random variables e.g compute p ajb fromp bja 're interested p rjyn reverse conditioning isp ynjr probability distribution function number heads nindependent tosses probability head single toss r. binomial distribution function easily compute ynandr context bayes rule see also equation 2.11 p rjyn =p ynjr p r p yn 3.3 equation going important us following chapters worth spending time looking term detail likelihood p ynjr came across likelihood chapter 2. exactly meaning likely would observe data case data yn particular value r model example binomial distribution value high rcould feasibly produced resultynand low result unlikely example figure 3.3 shows likelihood p ynjr function rfor two di erent scenarios rst data consists ten tosses n 10 six heads second weren= 100 tosses 70 heads 0 0.2 0.4 0.6 0.8 100.050.10.150.20.250.30.35 rp yn|r yn= 6 n= 10yn= 70 n= 100 figure 3.3 examples likelihood p ynjr function rfor two scenarios plot reveals two important properties likelihood firstly probability density area curves would equal 1. see case without working area two areas completely di erent secondly two examples di er much appear tell us r. rst example likelihood non-zero value large range possible rvalues approximately 0:2 \u0014r\u00140:9 second range greatly reduced approximately 0 :6\u0014r\u00140:8 intuitive second example much data results 100 tosses rather 10 know r.the prior distribution p r prior distribution allows us express belief value rbefore see data illustrate shall consider following three examples 1. know anything tossing coins stall owner 2. think coin hence stall owner fair 3. think coin hence stall owner biased give heads tails encode beliefs di erent prior distributions rcan take value 0 1 therefore must modelled continuous random variable figure 3.4 shows three density functions might used encode three di erent prior beliefs 0 0.2 0.4 0.6 0.8 1012345678 rp r 123 figure 3.4 examples prior densities p r forrfor three di erent scenarios belief number 1 represented uniform density 0 1 shows preference particular rvalue number 2 given density function concentrated around r= 0:5 value would expect fair coin density suggests expect much variance r 's almost certainly going lie 0.4 0.6. coins us tossed agree finally number 3 encapsulates belief coin therefore stall owner biased density suggests r 0:5 high level variance ne belief coin biased really idea biased stage choose three scenarios stage interesting see e ect di erent beliefs p rjyn three functions shown figure 3.4 plucked thin air examples beta probability density functions see section 2.5.2 beta density function used continuous random variables constrained lie 0 1 perfect example random variable rwith parameters de ned asp r =\u0000 \u0000 \u0000 r \u00001 1\u0000r \u00001 3.4 \u0000 known gamma function see section 2.5.2 equation 3.4 gamma functions ensure density normalised integrates 1 therefore probability density function particular \u0000 \u0000 \u0000 =zr=1 r=0r \u00001 1\u0000r \u00001dr ensuring thatzr=1 r=0\u0000 \u0000 \u0000 r \u00001 1\u0000r \u00001dr= 1 two parameters control shape resulting density function must positive three beliefs plotted figure 3.4 correspond following pairs parameter values 1. know nothing 1 1 2. fair coin 50 50 3. biased 5 1. problem choosing values big one example choose 5 1 biased coin easy answer shall see later beta distribution interpreted number previ- ous hypothetical coin tosses distributions analogy possible also introduce idea maybe treated random variables mean time assume values sensible move marginal distribution yn p yn third quantity equa- tion p yn acts normalising constant ensure p rjyn properly de ned density known marginal distribution ynbecause com- puted integrating rout joint density p yn r p yn =zr=1 r=0p yn r dr joint density factorised give p yn =zr=1 r=0p ynjr p r dr product prior likelihood integrated range values thatrmay take p yn also known marginal likelihood likelihood data yn averaged parameter values shall see section 3.4.1 useful quantity model selection unfortunately small minority cases di\u000ecult calculate.the posterior distribution p rjyn thisposterior distribution interested result updating prior belief p r light new evidence yn shape density interesting tells us something much information rafter combining knew beforehand prior 've seen likelihood three hypothetical examples provided figure 3.5 purely illustrative correspond particular likelihood prior examples shown figures 3.3 3.4 uniform combining likelihood prior together left values requally likely b suggests ris likely low could high might result starting uniform prior observing tails heads finally c suggests coin biased land heads often density posterior tells us values likely also provides indication level uncertainty still rhaving observed data 0 0.2 0.4 0.6 0.8 1012345 rp r|yn b c figure 3.5 examples three possible posterior distributions p rjyn already mentioned use posterior density compute expectations example could compute ep rjyn fp y10\u00146 g=zr=1 r=0p y10\u00146jr p rjyn dr expected value probability win takes account data observed prior beliefs uncertainty remains useful helping decide whether play game return later rst look kind posterior densities obtain coin example.comment 3.1 conjugate priors likelihood-prior pair said conjugate result posterior form prior enables us compute posterior density analytically with- worry comput- ing denominator bayes rule marginal likelihood com- mon conjugate pairs listed table right.prior likelihood gaussian gaussian beta binomial gamma gaussian dirichlet multinomial 3.2 exact posterior beta distribution common choice prior likelihood binomial distribution use algebra compute posterior den- sity exactly fact beta distribution known conjugate prior binomial likelihood see comment 3.1 prior likelihood conjugate posterior form prior speci cally p rjyn give beta distribution parameters \u000eand whose values computed prior andyn beta binomial conjugate pair distributions see example another conjugate prior likelihood pair return olympic data later chapter using conjugate prior makes things much easier mathematical point view however mentioned discussion loss functions chapter 1 noise distributions chapter 2 important base choices mod- elling assumptions mathematical convenience next chapter see techniques use common scenario pair non-conjugate returning example omit p yn equation 3.3 leaving p rjyn /p ynjr p r replacing terms right hand side binomial beta distribution gives p rjyn /\u0014\u0012n yn\u0013 ryn 1\u0000r n\u0000yn\u0015 \u0002\u0014\u0000 \u0000 \u0000 r \u00001 1\u0000r \u00001\u0015 3.5 prior likelihood conjugate know p rjyn beta density beta density parameters \u000eand following general form p r =kr\u000e\u00001 1\u0000r \u00001 wherekis constant arrange terms including r right hand side equation 3.5 something looks like r\u000e\u00001 1\u0000r \u00001 sure constant must also correct \u0000 \u000e \u0000 \u000e \u0000 know posterior density beta density words know normalising constant beta density need compute p yn .rearranging equation 3.5 gives us p rjyn /\u0014\u0012n yn\u0013\u0000 \u0000 \u0000 \u0015 \u0002h rynr \u00001 1\u0000r n\u0000yn 1\u0000r \u00001i /ryn+ \u00001 1\u0000r n\u0000yn+ \u00001 /r\u000e\u00001 1\u0000r \u00001 where\u000e=yn+ =n\u0000yn+ therefore p rjyn =\u0000 +n \u0000 +yn \u0000 +n\u0000yn r +yn\u00001 1\u0000r +n\u0000yn\u00001 3.6 note adding and\u000e theynterms cancel posterior density ofrbased prior p r data yn notice posterior parameters computed adding number heads n rst prior parameter number tails n \u0000yn second allows us gain intuition prior parameters thought number heads tails previous tosses example consider second two scenarios discussed previous section fair coin scenario 50. equivalent tossing coin 100 times obtaining 50 heads 50 tails biased scenario 5 1 corresponding six tosses heads looking figure 3.4 helps us explain di ering levels variability sug- gested two densities fair coin density much lower variability biased one result many hypothetical tosses tosses know r. analogy perfect example n't integers less 1 0.3 heads n't make much sense analogy also breaks 1. observing one head one tail means values r= 0 andr= 1 impossible however density 1 figure 3.4 suggests values ofrare equally likely despite aws analogy useful one bear mind progress analysis see exercises 3.1 3.2 3.3 3.4 3.3 three scenarios investigate posterior distribution p rjyn three di erent prior scenarios shown figure 3.4 prior knowledge fair coin biased coin 3.3.1 prior knowledge scenario matlab script coin scenario1.m assume know nothing coin tossing stall holder prior parameters 1 1 shown figure 3.6 compare di erent scenarios use expected value variance r prior expected value random variable beta distribution parameters density function henceforth denoteasb given see exercise 3.5 p r =b ep r frg= scenario 1 ep r frg= =1 2 variance beta distributed random variable given see exercise 3.6 varfrg= 2 1 3.7 1 varfrg=1 12 note formulation posterior equation 3.6 restricted updating distribution blocks ten incorporate results number coin tosses illustrate evolution posterior look changes toss toss new customer hands 1 stall owner starts tossing coin rst toss results head posterior distribution one toss beta distribution parameters \u000e= +ynand +n\u0000yn p rjyn =b \u000e scenario 1 n= 1 tosses seen yn= 1 heads \u000e= 1 1 2 1 1\u00001 1 posterior distribution shown solid line figure 3.6 b prior also shown dashed line single observation quite large e ect posterior di erent prior prior values rwere equally likely changed higher values likely lower values zero density r= 0. consistent evidence observing one head makes high values rslightly likely low values slightly less likely density still broad observed one toss expected value runder posterior ep rjyn frg=2 3 see observing solitary head increased expected value r 1=2 2=3 variance posterior using equation 3.7 varfrg=1 18 lower prior variance 1 =12 reduction variance tells us less uncertainty value rthan learnt0 0.2 0.4 0.6 0.8 100.511.522.53 rp r prior 1 1 0 0.2 0.4 0.6 0.8 100.511.522.53 rp r|y1 toss 1 h b \u000e= 2 1 0 0.2 0.4 0.6 0.8 100.511.522.53 rp r|y2 toss 2 c \u000e= 2 2 0 0.2 0.4 0.6 0.8 100.511.522.53 rp r|y3 toss 3 h \u000e= 3 2 0 0.2 0.4 0.6 0.8 100.511.522.53 rp r|y4 toss 4 h e \u000e= 4 2 0 0.2 0.4 0.6 0.8 100.511.522.53 rp r|y10 toss 10 h f \u000e= 7 5 figure 3.6 evolution p rjyn number observed coin tosses increases.something increase expected value tells us 've learnt heads slightly likely tails stall owner tosses second coin lands tails seen one head one tail n= 2 yn= 1 resulting \u000e= 1 1 2 1 2\u00001 2 posterior distribution shown solid dark line figure 3.6 c lighter dash-dot line posterior saw one toss dashed line prior density changed ect new evidence observed tail density r= 1 zero r 1 would suggest coin always lands heads density curved rather straight already mentioned beta density function exible observing tail made lower values likely expected value variance ep rjyn frg=1 2 varfrg=1 20 expected value decreased back 1=2 given expected value prior also 1=2 might conclude n't learnt anything however variance decreased 1 =18 1=20 less uncertainty rand learnt something fact 've learnt ris closer 1 =2 assumed prior third toss results another head n= 3 tosses yn= 2 heads andn\u0000yn= 1 tail updated posterior parameters \u000e= +yn= 1 2 3 +n\u0000yn= 1 3\u00002 2 posterior plotted figure 3.6 posterior solid dark line previous posterior solid light line dashed line prior notice e ect observing second head skew density right suggesting heads likely tails entirely consistent evidence seen heads tails seen three coins though still high level uncertainty density suggests rcould potentially still pretty much value 0 1. new expected value variance ep rjyn frg=3 5 varfrg=1 25 variance decreased ecting decrease uncertainty would expect see data toss 4 also comes heads yn= 3 n 4 resulting \u000e= 1 3 4 1 4\u00003 2. figure 3.6 e shows current previous posteriors prior familiar format density skewed right 've seen three heads one tail seems likely ris greater 1=2 also notice di erence n= 3 posterior n= 4 posterior low values r extra head left us pretty convinced ris 0.1 lower expected value variance given ep rjyn frg=2 3 varfrg=2 63= 0:0317 expected value increased variance decreased remaining six tosses made complete sequence h h h h h h total six heads four tails posterior distribution n= 10 tosses yn= 6 parameters \u000e= 1 6 7 1 10\u00006 5. along posterior n= 9 shown figure 3.6 f expected value variance ep rjyn frg=7 12= 0:5833 varfrg= 0:0187 3.8 ten observations increased expected value 0.5 0.5833 decreased variance 1 =12 0:0833 0.0187. however full story examining figure 3.6 f see also pretty sure r 0:2 andr 0:9. uncertainty value ris still quite high observed ten tosses 2 4 6 8 100.50.550.60.650.70.75 coin tossese r expected value 2 4 6 8 100.010.020.030.040.050.06 coin tossesvar r b variance figure 3.7 evolution expected value variance b ras coin toss data added posterior figure 3.7 summarises expected value variance change 10 observations included expected value jumps around bit whereas variance steadily decreases information becomes available seventh toss variance increases rst seven tosses h h h h h evidence including toss 6 heads much likely tails 5 6 tails seventh toss therefore slightly unexpected figure 3.8 shows posterior seventh toss arrival tail forced density increase likelihood low values rand increased uncertainty posterior density encapsulates information r. shortly use compute expected probability winning game revisit idea using point estimates extracting a0 0.2 0.4 0.6 0.8 100.511.522.53 rp r|y7 toss 7 figure 3.8 posterior six light seven dark tosses single valuebrofrfrom density able compare expected probability winning probability winning computed single value ofr sensible choice would use ep rjyn frg value com- pute probability winning p ynew\u00146jbr quantity could used decide whether play note make distinction observed tosses future tosses use ynewas random variable describes ten future tosses ten tosses posterior density beta parameters \u000e= 7 5.bris therefore br=\u000e \u000e+ =7 12 probability winning game follows p ynew\u00146jbr 1\u000010x ynew=7p ynew=ynewjbr 1\u00000:3414 0:6586 suggesting win often lose using posterior information requires computing ep rjyn fp ynew\u00146jr g rearranging manipulating expectation provides us following ex-pression ep rjyn fp ynew\u00146jr g=ep rjyn f1\u0000p ynew\u00157jr g 3.9 1\u0000ep rjyn fp ynew\u00157jr g 1\u0000ep rjyn ynew=10x ynew=7p ynew=ynewjr 1\u0000ynew=10x ynew=7ep rjyn fp ynew=ynewjr g evaluate need able compute ep rjyn fp ynew=ynewjr g. de nition expectations given ep rjyn fp ynew=ynewjr g=zr=1 r=0p ynew=ynewjr p rjyn dr =zr=1 r=0\u0014\u0012nnew ynew\u0013 rynew 1\u0000r nnew\u0000ynew\u0015\u0014\u0000 \u000e+ \u0000 \u000e \u0000 r\u000e\u00001 1\u0000r \u00001\u0015 dr =\u0012nnew ynew\u0013\u0000 \u000e+ \u0000 \u000e \u0000 zr=1 r=0rynew+\u000e\u00001 1\u0000r nnew\u0000ynew+ \u00001dr 3.10 integral looks bit daunting however closer inspection argument inside integral unnormalised beta density parameters \u000e+ynewand +nnew\u0000ynew general beta density parameters following must true zr=1 r=0\u0000 \u0000 \u0000 r \u00001 1\u0000r \u00001dr= 1 therefore zr=1 r=0r \u00001 1\u0000r \u00001dr=\u0000 \u0000 \u0000 desired expectation becomes ep rjyn fp ynew=ynewjr g=\u0012nnew ynew\u0013\u0000 \u000e+ \u0000 \u000e \u0000 \u0000 \u000e+ynew \u0000 +nnew\u0000ynew \u0000 \u000e+ +nnew easily compute particular posterior i.e values and\u000e values ofnnewandynew ten tosses \u000e= 7 5. plugging values compute expected probability success ep rjyn fp ynew\u00146jr g= 1\u0000ynew=10x ynew=7ep rjyn fp ynew=ynewjr g 1\u00000:3945 0:6055 comparing value obtained using point estimate see predict win often agreement evidence one person fully observed got six heads four tails hence 2 point estimate gives higher probability ignoring posterior uncertainty makes likely win.another customer plays game sequence tosses h h h h h h h h eight heads two tails stall owner combining 20 tosses observed n= 20 yn= 6 8 14 heads n\u0000yn= 20\u000014 6 tails gives \u000e= 15 7. posterior density shown figure 3.9 light line shows posterior ten dashed line prior expected value variance ep rjyn frg= 0:6818 varfrg= 0:0094 expected value increased variance decreased c.f equation 3.8 behaviours would expect eight heads two tails in- crease expected value rand increased data decrease variance recompute ep rjyn fp ynew\u00146jr gin light new evidence plug- 0 0.2 0.4 0.6 0.8 1012345 rp r|yn figure 3.9 posterior distribution observing 10 tosses light curve 20 tosses dark curve dashed line corresponds prior density ging appropriate values ep rjyn fp ynew\u00146jr g= 0:4045 new evidence pushed density right made high values r hence coin landing heads likely reduced probability winning completeness also compute p ynew\u00146jbr 0:3994. corresponds expected return 2\u00020:4045\u00001 =\u00000:1910 equivalent loss 20p per go example touched upon important components bayesian machine learning choosing priors choosing likelihoods computing posteriors using expectations make predictions repeat process two prior scenarios.3.3.2 fair coin scenario fair coin scenario matlab script coin scenario2.m assumed 50 analogous assuming already witnessed 100 tosses half resulted heads rst thing notice 100 tosses corresponds much data going observe 20 tosses expect data e ect previous scenario figure 3.10 shows prior density figures 3.10 b 3.10 c 3.10 3.10 e 3.10 f show posterior 1 5 10 15 20 tosses respectively scenario shown previous posterior stage close current one however cases change posterior small lines almost lie right top one another fact ten tosses posterior moved signi cantly prior recalling analogy beta prior prior includes evidential equivalent 100 tosses surprising adding another ten makes much di erence evolution ep rjyn frgand varfrgas 20 tosses observed seen figure 3.11. see little change either data appear compared changes observed figure 3.6. small changes indicative strong prior density prior dominate data 've observed many tosses i.e. p r dominates p ynjr equation 3.3. created model stuck ways require lot persuasion believe otherwise previous section work ep rjyn fp ynew\u00146jr g. 20 tosses observed \u000e= +yn= 50 14 64 +n\u0000yn= 50 20\u000014 56. expectation works ep rjyn fp ynew\u00146jr g= 0:7579 3.11 also see much di erence value value obtained using point estimate br p ynew\u00146jbr case br= 64= 64 56 0 :5333 p ynew\u00146jbr 0:7680 quantities predict win often light we've seen posterior come surprise data done little overcome prior assumption coin fair already know coin fair tend win fair coin result us winning average 66p per game see start section 3.1 aside consider accurate approximation p ynew\u00146jbr proper expectation scenario previous one previous one di erence two values jep rjyn fp ynew\u00146jr g\u0000p ynew\u00146jbr j= 0:0531 example values closer jep rjyn fp ynew\u00146jr g\u0000p ynew\u00146jbr j= 0:0101 good reason case variance posterior decreases variance scenario 2 much lower scenario 1 probability density becomes condensed around one particular point imagine variance0 0.2 0.4 0.6 0.8 10246810 rp r prior 50 50 0 0.2 0.4 0.6 0.8 10246810 rp r|y1 toss 1 h b \u000e= 51 50 0 0.2 0.4 0.6 0.8 10246810 rp r|y5 toss 5 h c \u000e= 54 51 0 0.2 0.4 0.6 0.8 10246810 rp r|y10 toss 10 h \u000e= 56 54 0 0.2 0.4 0.6 0.8 10246810 rp r|y15 toss 15 h e \u000e= 59 56 0 0.2 0.4 0.6 0.8 10246810 rp r|y20 toss 20 h f \u000e= 64 56 figure 3.10 evolution posterior p rjyn coin tosses observed fair coin scenario dashed line shows prior density.5 10 15 200.50.510.520.530.540.550.56 coin tossese r expected value 5 10 15 2022.12.22.32.42.52.6x 103 coin tossesvar r b variance figure 3.11 evolution ep rjyn frg varfrg b 20 coin tosses observed fair coin scenario decreasing extent single value rthat probability 1 occurring p rjyn zero everywhere else expectation calculating ep rjyn fp ynew\u00146jr g=zr=1 r=0p ynew\u00146jr p rjyn dr ifp rjyn zero everywhere except one speci c value say br becomes ep rjyn fp ynew\u00146jr g=p ynew\u00146jbr words variance decreases p ynew\u00146jbr becomes better better approximation true expectation speci c example quantity data increases uncertainty parameters subsequently decreases point approximations become reliable 3.3.3 biased coin nal scenario assume coin therefore stall owner biased generate heads tails matlab script coin scenario3.m encoded beta prior parameters 5 1. expected value ep r frg= 5=6 coins every six come heads scenario 2 figure 3.12 shows prior density figures 3.12 b 3.12 c 3.12 3.12 e 3.12 f show posterior 1 5 10 15 20 tosses respectively given we've already seen nothing unusual posterior moves quite rapidly away prior prior e ectively uence 6 data points figure 3.13 shows evolution expected value variance variance curve several bumps corresponding tosses resulting tails strong prior bias towards high rvalue n't expect see many tails under0 0.2 0.4 0.6 0.8 1012345 rp r prior 5 1 0 0.2 0.4 0.6 0.8 102468 rp r|y1 toss 1 h b \u000e= 6 1 0 0.2 0.4 0.6 0.8 1012345 rp r|y5 toss 5 h c \u000e= 9 2 0 0.2 0.4 0.6 0.8 1012345 rp r|y10 toss 10 h \u000e= 11 5 0 0.2 0.4 0.6 0.8 1012345 rp r|y15 toss 15 h e \u000e= 14 7 0 0.2 0.4 0.6 0.8 1012345 rp r|y20 toss 20 h f \u000e= 19 7 figure 3.12 evolution posterior p rjyn coin tosses observed biased coin scenario dashed line shows prior density last four plots dash-dot line shows previous posterior i.e posterior 4 9 14 19 tosses .5 10 15 200.650.70.750.80.850.9 coin tossese r expected value 5 10 15 200.0060.0080.010.0120.0140.0160.0180.020.022 coin tossesvar r b variance figure 3.13 evolution ep rjyn frg varfrg b 20 coin tosses observed biased coin scenario assumption model becomes less certain calculate true quantity interest ep rjyn fp ynew\u00146jr g. nal posterior parameter values \u000e= +yn= 5 14 19 1 +n\u0000yn= 1 20\u000014 7. plugging ep rjyn fp ynew\u00146jr g= 0:2915 approximation noting br= 19= 19 7 0:7308 p ynew\u00146jbr 0:2707 values suggest lose money average 3.3.4 three scenarios summary three di erent scenarios given us di erent values expected proba- bility winning 1. prior knowledge ep rjyn fp ynew\u00146jr g= 0:4045 2. fair coin ep rjyn fp ynew\u00146jr g= 0:7579 3. biased coin ep rjyn fp ynew\u00146jr g= 0:2915. one choose could choose based prior beliefs seems plausible given stall holder n't look like go business scenario 3 might sensible might decide really know anything stall holder coin look scenario 1. might believe upstanding stall holder would never stoop cheating go scenario 2. possible justify seen bayesian technique allows combine data observed 20 coin tosses prior knowledge one scenarios principled way posterior density explicitly models uncertainty remains rat stage used make predictions see exercises 3.7 3.8 .0 0.5 1024681012 rp r|y100 scenario 1scenario 2scenario 3 three posteriors 100 tosses 0.6 0.65 0.7 0.75 0.8051015202530 rp r|y1000 scenario 1scenario 2scenario 3 b three posteriors 1000 tosses figure 3.14 posterior densities three scenarios 100 coin tosses left 1000 coin tosses right 3.3.5 adding data move worth examining e ect adding data seen scenarios addition data results posterior diverging prior usually decrease variance fact continue adding data nd posteriors three scenarios start look similar figure 3.14 see posteriors three scenarios 100 1000 tosses compared posteriors three scenarios small numbers tosses observed figures 3.6 f 3.10 3.12 notice posteriors becoming similar particularly noticeable scenarios 1 3 1000 tosses indistinguishable di erence two posteriors scenario 2 due high strength low variance prior scenario 2 prior corresponds strong belief take lot contradictory data remove uence diminishing e ect prior quantity data increases easily explained look expression used compute posterior ignoring normalising marginal likelihood term posterior proportional likelihood multiplied prior add data prior unchanged like- lihood becomes product normal independence assumptions made individual likelihood observations increase gradually swamp single contribution prior also intuitive ob- serve data beliefs seeing become less less important 3.4 marginal likelihoods fortunately subjective beliefs option determining three scenarios best earlier chapter discussing terms equation 3.3 showed denominator p yn could considered berelated toras follows p yn =zr=1 r=0p r yn dr =zr=1 r=0p ynjr p r dr 3.12 considering di erent choices p r need strict conditioning p r actually written p rj density condi- tioned particular pair values extending conditioning equation 3.12 gives p ynj =zr=1 r=0p ynjr p rj dr 3.13 marginal likelihood called rhas marginalised p ynj useful important quantity tells us likely data n given choice prior parameters higher p ynj better evidence agrees prior speci cation hence dataset could use p ynj help choose best scenario select scenario p ynj highest compute quantity need evaluate following integral p ynj =zr=1 r=0p ynjr p rj dr =zr=1 r=0\u0012n yn\u0013 ryn 1\u0000r n\u0000yn\u0000 \u0000 \u0000 r \u00001 1\u0000r \u00001dr =\u0012n yn\u0013\u0000 \u0000 \u0000 zr=1 r=0r +yn\u00001 1\u0000r +n\u0000yn\u00001dr exactly form equation 3.10. argument inside integral unnormalised beta density know integrating get inverse normal beta normalising constant therefore p ynj =\u0012n yn\u0013\u0000 \u0000 \u0000 \u0000 +yn \u0000 +n\u0000yn \u0000 +n 3.14 example n= 20 andyn= 14 total 14 heads 2 sets 10 tosses three di erent possible pairs values plugging values equation 3.14 gives 1. prior knowledge 1 p ynj 0:0476 2. fair coin 50 p ynj 0 0441 3. biased coin 5 1 p ynj 0 0576. prior corresponding biased coin highest marginal likelihood fair coin prior lowest previous section saw probability winning scenario ep rjyn fp ynew\u00146jr g= 0:2915 note 're conditioning posterior prior parameters p rjyn word caution required choosing priors way essentially choosing prior best agrees data prior longer correspondsto beliefs observe data applications may unac- ceptable give us single value tells us much data backs prior beliefs example data suggests biased coin prior best supported evidence 3.4.1 model comparison marginal likelihood possible extend prior comparison previous section using marginal likelihood optimise assuming take value ranges 0\u0014 \u001450 0\u0014 \u001430 search values maximise p ynj 0 10 20 30 40 5005101520253035404550 figure 3.15 marginal likelihood contours function prior parameters coin example circle towards top right shows optimum figure 3.15 shows marginal likelihood varied respective ranges optimum value 50 22 resulting marginal likelihood 0.1694. choosing parameters way known type ii maximum likelihood distinguish standard i.e type maximum likelihood introduced chapter 2 3.5 hyperparameters bayesian analysis presented thus far based idea represent quantities interest random variables e.g r probability coin landing heads ris parameter interest example also parameters could thing cases directed towards particular values based knowledge problem wemight know coin biased often know exact value take therefore treat random variables need de ne prior density random variables p r factorises see section 2.2.5 p r =p rj p addition often useful assume independent p p p quantity interested posterior parameters model p r jyn applying bayes rule p r jyn =p ynjr p r p yn =p ynjr p r p yn =p ynjr p rj p p yn note second step removed likelihood p ynjr another example conditional independence see section 2.8.1 distribution overyndepends uence r. conditioned particular value r dependence broken p normally require additional parameters i.e p j\u0014 \u0014controls density way control density r.\u0014is known hyper-parameterbecause parameter controlling prior parameters controlling prior r. computing marginal likelihood integrate random variables left data conditioned hyperparameters p ynj\u0014 =zzz p ynjr p rj p j\u0014 dr unfortunately adding extra complexity model often means com- putation quantities interest posterior p r jyn \u0014 predic- tive expectations marginal likelihood p ynj\u0014 analytically intractable requires one approximation methods introduce chapter 4. point one could imagine inde nitely adding layers model example \u0014could thought random variable comes density parameterised random variables number levels hierarchy far go x one parameters dictated data trying model perhaps specify exact values level much computation tolerate general layers add complex compute posteriors predictions 3.6 graphical models adding extra layers model hyperparameters etc quickly become unwieldy popular describe graphically graph icalmodel network nodes correspond random variables edges dependenciesyx xn yn n b r yn c figure 3.16 graphical model examples nodes correspond random variables shaded nodes corresponding things ob- serve arrows describe dependencies variables plates describe multiple instances example b n random variables yn n= 1 n dependent ran- dom variable xn c graphical representation model used coin example addition prior parameterised by\u0014 random variables example section 2.2.4 introduced various prop- erties random variables model consisted two random variables one representing toss coin x one representing say coin landed model de ned conditional distribution p y=yjx=x represented graphically figure 3.16 two nodes joined aarrow show yis de ned conditioned x. note also node yis shaded far listener concerned variable observed listener see coin actually landing n't observe x. imag- ine procedure repeated ntimes 2 nrandom variables x1 xnandy1 yn drawing would messy instead embed nodes within plate plates rectangles tell us whatever embedded within repeated number times number times given bottom right corner shown figure 3.16 b figure 3.16 c shows graphical representation coin toss model single observed random variable represents number heads ntosses yn conditioned random variable r depends random variables finally dependent hyper-parameter \u0014 information graphical models found suggested reading end chapter 3.7 summary previous sections introduced many new concepts perhaps important idea treating quantities interest random variables must de ne prior distribution possible values quan- tities use bayes rule equation 3.3 see density changes incorporate evidence observed data resulting posterior density examined used compute interesting expectations addition shown marginal likelihood normalisation constant bayes rule used compare di erent models example choosing likely prior coin tossing example discussed possible pitfalls objections approach finally shown bayesian method extended treating parameters de ne priors parameters random vari- ables additions hierarchy often make analytical computations intractable resort sampling approximation based techniques subject next chapter 3.8 bayesian treatment olympic 100 data return olympic 100 data previous chapters tted linear parameters model minimising squared loss incorporated explicit noise model found optimal parameter values maximising likelihood section give data bayesian treatment aim making prediction 2012 olympics london involve several steps firstly need de ne prior likelihood coin example use compute posterior density parameters model computed posterior rin coin example we've computed posterior use make predictions new olympic years 3.8.1 model use kth order polynomial model introduced chapter 1 gaussian noise model introduced chapter 2 tn=w0+w1xn+w2x2 n+\u0001\u0001\u0001+wkxk n+\u000fn where\u000fn\u0018n 0 \u001b2 vector form corresponds tn=wtxn+\u000fn w= w 0 wk tandxn= 1 xn x2 n xk n t. stacking responses one vector t= t1 tn tand inputs single matrix x= x1 x2 xn equation 1.18 get following expression whole dataset t=xw+\u000f \u000f= \u000f 1 \u000fn t. example going slightly simplify matters assuming know true value \u001b2 could use methods introduced chapter treat\u001b2as random variable could get analytical results posteriornw 2 xn tnfigure 3.17 graphical model bayesian model olympic men 's 100 data distribution maths messier could detract main message substituting various symbols bayes rule gives p wjt x \u001b2 \u0001 =p tjw x \u001b2 \u0001 p wj\u0001 p tjx \u001b2 \u0001 =p tjw x \u001b2 p wj\u0001 p tjx \u001b2 \u0001 \u0001 corresponds set parameters required de ne prior wthat de ned precisely graphical model seen figure 3.17. expanding marginal likelihood p wjt x \u001b2 \u0001 =p tjw x \u001b2 p wj\u0001 r p tjw x \u001b2 p wj\u0001 dw 3.15 interested making predictions involve taking expectation respect posterior density particular set attributes xnew corresponding new olympic year density associated winning time tnewis given p tnewjxnew x \u001b2 \u0001 =z p tnewjxnew w \u001b2 p wjt x \u001b2 \u0001 dw 3.16 notice conditioning right hand side posterior density w depend xnewand appear conditioning similarly make predictions using \u0001 n't appear p tnewjxnew w \u001b2 predictions could also take form probabilities example could compute probability winning time 9.5 seconds p tnew 9:5jx new x \u001b2 \u0001 =z p tnew 9:5jx new w \u001b2 p wjt x \u001b2 \u0001 dw 3.17 3.8.2 likelihood likelihood p tjw x \u001b2 exactly quantity maximised pre- vious chapter model tells us t=xw+\u000f \u000f\u0018n 0 \u001b2in gaussian random variable \u000f plus constant showed section 2.8 equivalent gaussian random variable constant added mean gives us likelihood p tjw x \u001b2 =n xw \u001b2in ann-dimensional gaussian density mean xwand variance \u001b2in analo- gous expression coin example binomial likelihood given equation 3.2 3.8.3 prior interested able produce exact expression posterior need choose prior p wj\u0001 conjugate gaussian likelihood conveniently gaussian prior conjugate gaussian likelihood therefore use gaussian prior w. particular p wj\u00160 \u00060 =n \u00160 \u00060 choose parameters \u00160and\u00060later analogous equa- tion 3.4 coin example always explicitly condi- tion \u00160and\u00060in expressions example brevity instead writing p wjt x \u001b2 \u00160 \u00060 use p wjt x \u001b2 see exercise 3.10 3.8.4 posterior turn attention computing posterior coin example use fact know posterior gaussian allows us ignore marginal likelihood equation 3.15 manipulate likelihood prior nd something proportional gaussian rst step collect terms wtogether ignore term include w p wjt x \u001b2 /p tjw x \u001b2 p wj\u00160 \u00060 =1 2\u0019 n=2j\u001b2ij1=2exp\u0012 \u00001 2 t\u0000xw \u001b2i \u00001 t\u0000xw \u0013 \u00021 2\u0019 n=2j\u00060j1=2exp\u0012 \u00001 2 w\u0000\u00160 t\u0006\u00001 0 w\u0000\u00160 \u0013 /exp\u0012 \u00001 2\u001b2 t\u0000xw t\u0000xw \u0013 \u0002exp\u0012 \u00001 2 w\u0000\u00160 t\u0006\u00001 0 w\u0000\u00160 \u0013 exp\u001a \u00001 2\u00121 \u001b2 t\u0000xw t\u0000xw w\u0000\u00160 t\u0006\u00001 0 w\u0000\u00160 \u0013\u001b multiplying terms bracket removing involve wgives p wjt x \u001b2 /exp\u001a \u00001 2\u0012 \u00002 \u001b2ttxw+1 \u001b2wtxtxw+wt\u0006\u00001 0w\u00002\u0016t 0\u0006\u00001 0w\u0013\u001b know posterior gaussian therefore remove constants i.e terms involving w rearrange expression multivariate gaussian make look something like expression p wjt x \u001b2 =n \u0016w \u0006w /exp\u0012 \u00001 2 w\u0000\u0016w t\u0006\u00001 w w\u0000\u0016w \u0013 /exp\u001a \u00001 2\u0010 wt\u0006\u00001 ww\u00002\u0016t w\u0006\u00001 ww\u0011\u001b 3.18 terms linear quadratic win equation 3.8.4 must equal equation 3.18. taking quadratic terms solve \u0006w wt\u0006\u00001 ww=1 \u001b2wtxtxw+wt\u0006\u00001 0w =wt\u00121 \u001b2xtx+\u0006\u00001 0\u0013 w \u0006w=\u00121 \u001b2xtx+\u0006\u00001 0\u0013\u00001 similarly equating linear terms equations 3.8.4 3.18 using new expression \u0006w get expression \u0016w \u00002\u0016t w\u0006\u00001 ww=\u00002 \u001b2ttxw\u00002\u0016t 0\u0006\u00001 0w \u0016t w\u0006\u00001 ww=1 \u001b2ttxw+\u0016t 0\u0006\u00001 0w \u0016t w\u0006\u00001 w =1 \u001b2ttx+\u0016t 0\u0006\u00001 0 \u0016t w\u0006\u00001 w\u0006w=\u00121 \u001b2ttx+\u0016t 0\u0006\u00001 0\u0013 \u0006w \u0016t w=\u00121 \u001b2ttx+\u0016t 0\u0006\u00001 0\u0013 \u0006w \u0016w=\u0006w\u00121 \u001b2xtt+\u0006\u00001 0\u00160\u0013 3.19 \u0006t w=\u0006wdue fact must symmetric therefore p wjt x \u001b2 =n \u0016w \u0006w 3.20 \u0006w=\u00121 \u001b2xtx+\u0006\u00001 0\u0013\u00001 3.21 \u0016w=\u0006w\u00121 \u001b2xtt+\u0006\u00001 0\u00160\u0013 3.22 0 5 10 15 20 25 309.51010.51111.512 xtfigure 3.18 olympic data rescaled xvalues see exercise 3.12 expressions look far away things seen particular compare equation 3.22 regularised least squares solution given equation 1.21. fact \u00160= 0 0 0 expressions almost identical given posterior gaussian single likely value ofwis mean posterior \u0016w known maximum aposteriori map estimate wand also thought maximum value joint densityp w tjx \u001b2 \u0001 likelihood multiplied prior already seen squared loss considered chapter 1 similar gaussian likelihood follows computing likely posterior value likelihood gaussian equivalent using regularised least squares see exercise 3.9 comparison often help provide intuition regarding e ect prior 3.8.5 rst-order polynomial illustrate prior posterior rst-order polynomial possible visualise densities two-dimensional parameter space input vectors also two elements xn= 1 xn t. aid visualisation rescale olympic year subtracting year rst olympics 1896 year dividing number 4. means x1is 0 x2is 1 etc data new xscaling plotted figure 3.18. returning fairground rst step analysis choice prior parameters \u00160and\u00060 \u00160 assume n't really know anything parameters choose \u00160= 0 0 t. covariance use \u00060=\u0014100 0 0 5\u0015 larger value variance w0is due fact saw maximum likelihood estimate optimal value w0was much higher w1 also assumed two variables independent prior settingw0w1 20 10 0 10 206420246 prior density 0 5 10 15 20 25 3099.51010.51111.512 xt b functions created parameters drawn prior figure 3.19 gaussian prior used olympic 100 data functions created samples drawn prior b -diagonal elements covariance matrix zero preclude dependent posterior contours prior density seen figure 3.19 's hard visualise means terms model help figure 3.19 b shown functions corresponding several sets parameters drawn prior create sampled wfrom gaussian de ned \u00160and\u00060and substituted linear model tn=w0+w1xn examples show prior admits possibility many di erent models using\u001b2= 10 illustrative purposes matlab script olympbayes.m compute posterior distribution observe one data point using data point corresponding rst olympics data summarised x= 1 0 x= 1 0 t= 12 plugging values along prior parameters \u001b2= 10 equations 3.20 3.22 obtain posterior distribution shown figure 3.20 posterior much certainty regarding w0but still knows little w1 makes sense 've provided data point x= 0 highly informative determining intercept tells us little gradient one data point alone could never tell us much gradient functions created samples posterior shown figure 3.20 b look quite di erent prior particular pass quite close rst data point figures 3.20 c 3.20 3.20 e show evolution posterior 2 5 10 data points respectively coin example notice posterior becomes condensed becoming certain value w also evolves posterior begins tilt indicative dependence developing two parameters increase intercept w0 must decrease gradient recall prior assumed two parameters independent \u0006 0only non-zero values diagonal dependence coming entirely evidence within data help visualise posterior means stage figure 3.20 f shows set functions made parameters drawn posterior compared figure 3.20 b ,128\u0004a first course machine learning w0w1 20 10 0 10 206420246 posterior density dark contours af- ter rst data point observed lighter contours show prior den- sity 0 5 10 15 20 25 309.51010.51111.51212.513 xt b functions created parameters drawn posterior observing rst data point w0w1 20 10 0 10 206420246 c posterior density dark contours af- ter rst two data points observed w0w1 20 10 0 10 206420246 posterior density dark contours af- ter rst data points observed w0w1 5 10 15 2021012 e posterior density dark contours af- ter rst ten data points ob- served note zoomed 0 10 20 309.51010.51111.51212.513 xt f functions created parameters drawn posterior observing rst ten highlighted data points figure 3.20 evolution posterior density example functions drawn posterior olympic data observations added.w0w1 8 10 12 140.500.5 posterior density dark contours af- ter datapoints observed lighter contours show prior den- sity note zoomed 0 5 10 15 20 25 308910111213 xt b functions created parameters drawn posterior observing data points figure 3.21 posterior density sampled functions b olympic data 27 data points added see posterior density beginning favour parameters correspond models suited data finally figure 3.21 see posterior 27 data points included figure 3.21 b see functions drawn posterior functions really beginning follow trend data still lot variability though due relatively high value \u001b2= 10 chose help visualise prior posteriors making predictions might want use realistic value figure 3.22 show posterior data observed \u001b2= 0:05 roughly maximum likelihood value obtained section 2.8.2 posterior far condensed little variability remains w seen homogeneity set functions drawn figure 3.22 b turn attention making predictions 3.8.6 making predictions given new observation xnew interested density p tnewjxnew x \u001b2 notice conditioned w coin example going integrate wby taking expectation respect posterior p wjt x \u001b2 particular need compute p tnewjxnew x \u001b2 ep wjt x \u001b2 \b p tnewjxnew w \u001b2 =z p tnewjxnew w \u001b2 p wjt x \u001b2 dw analogous equation 3.9 coin example.w0w1 10.5 11 11.50.070.060.050.040.03 posterior density dark contours af- ter data points observed lighter contours show prior den- sity note zoomed 0 5 10 15 20 25 309.51010.51111.512 xt b functions created parameters drawn posterior observing data points figure 3.22 posterior density sampled functions b olympic data 27 data points added realistic noise variance \u001b2= 0:05. p tnewjxnew w \u001b2 de ned model product xnewandwwith additive gaussian noise p tnewjxnew w \u001b2 =n xt neww \u001b2 expression posterior gaussian result expec- tation another gaussian general p wj\u0016 \u0006 =n \u0016 \u0006 expectation another gaussian density n xt neww \u001b2 given p tnewjxnew x \u001b2 =n xt new\u0016w \u001b2+xt new\u0006wxnew posterior shown figure 3.22 p tnewjxnew x \u001b2 =n 9:5951 0:0572 plotted figure 3.23. density looks rather like predictive densities obtained max- imum likelihood solution chapter 2. however one crucial di erence maximum likelihood chose one particular model one corresponding highest likelihood generate density shown figure 3.23 averaged models consistent data prior averaged posterior hence density takes account uncertainty remains w given particular prior data 3.9 marginal likelihood polynomial model or- der selection section 1.5 used cross-validation procedure select order polynomial used cross-validation procedure correctly identi ed dataset was8.5 9 9.5 10 10.500.511.52 tnewp tnew |xnew ... figure 3.23 predictive distribution winning time men's 100 sprint 2012 london olympics generated third-order polynomial section 3.4 saw marginal likelihood could used choose prior densities see also used choose models particular use determine order polynomial function use synthetic data marginal likelihood gaussian model de ned p tjx \u00160 \u00060 =z p tjx w \u001b2 p wj\u00160 \u00060 dw analogous equation 3.14 coin example form predictive density discussed previous section another gaussian p tjx \u00160 \u00060 =n x\u00160 \u001b2in+x\u00060xt 3.23 evaluate responses training set section 1.5 generate data noisy third-order polynomial compute marginal likelihood models rst seventh-order possible model use gaussian prior wwith zero mean identity covariance matrix example rst-order model \u00160= 0 0 \u00060=\u00141 0 0 1\u0015 fourth-order model \u00160= 0 0 0 0 0 \u00060=2 666641 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 13 77775:5 0 58006004002000200400600800 xt noisy data third-order poly- nomial 1 2 3 4 5 6 700.20.40.60.811.2x 1094 polynomial ordermarginal likelihood b marginal likelihood models dif- ferent order figure 3.24 dataset sampled function t= 5x3\u0000x2+x marginal likelihoods polynomials increasing order b data true polynomial shown figure 3.24 matlab script margpoly.m true polynomial t= 5x3\u0000x2+xand gaussian noise added mean zero variance 150. marginal likelihood models rst seventh order calculated plugging relevant prior equa- tion 3.23 evaluating density observed responses values shown figure 3.24 b see marginal likelihood value sharply peaked true third-order model advantage cross- validation method model computationally undemanding n't several di erent datasets also use data however already mentioned calculating marginal likelihood general di\u000ecult often nd easier resort cross-validation techniques marginal likelihood conditioned prior parameters changing e ect marginal likelihood values possibly highest scoring model show e ect de ne \u00060=\u001b2 0iand vary\u001b2 0. already seen result \u001b2 0= 1. decrease \u001b2 0 see higher-order models performing better seen figure 3.25. decreasing \u001b2 0from 1 0.3 results seventh-order polynomial becoming likely model decreasing\u001b2 0 saying parameters take smaller smaller values third order polynomial model well one parameters needs 5 recall t= 5x3\u0000x2+x decrease \u001b2 0 becomes less less likely higher-order models lower parameter values become likely emphasises importance understanding mean model example model consists order polynomial andthe prior speci cation must careful choose prior sensibly see exercise 3.11 .1 2 3 4 5 6 70123456x 1097 polynomial ordermarginallikelihood \u001b2 0= 0:7 1 2 3 4 5 6 70123456789x 10103 polynomial ordermarginallikelihood b \u001b2 0= 0:4 1 2 3 4 5 6 700.20.40.60.81x 10103 polynomial ordermarginallikelihood c \u001b2 0= 0:3 figure 3.25 marginal likelihoods third-order polynomial exam- ple \u00060=\u001b2 0ias\u001b2 0is decreased 3.10 chapter summary chapter provided introduction bayesian way performing ma- chine learning tasks treating parameters random variables per- formed baysesian analysis coin tossing model linear regression model introduced chapters 1 2. cases de ned prior densities pa- rameters de ned likelihoods computed posterior densities examples prior likelihood chosen posterior could computed an- alytically addition computed predictions taking expectations respect posterior introduced marginal likelihood possible model selection criterion unfortunately expressions often analytically tractable must resort sampling approximation techniques techniques founda- tions modern bayesian inference form important area machine learning research development next chapter describe three popular techniques point estimates laplace approximations markov chain monte carlo 3.11 exercises 3.1 1 beta distribution becomes uniform 0 1. particular probability coin landing heads given rand beta prior placed r parameters 1 1 prior written p r 1 0\u0014r\u00141 using prior compute posterior density rifyheads observed inntosses i.e multiply prior binomial likelihood manipulate result obtain something looks like beta density 3.2 repeat previous exercise following prior also particular form beta density p r =\u001a2r0\u0014r\u00141 0 otherwise values prior parameters result p r 2r 3.3 repeat previous exercise following prior form beta density p r =\u001a3r20\u0014r\u00141 0 otherwise prior parameters 3.4 e ective prior sample sizes previous three exercises i.e many heads tails equivalent 3.5 random variable rhas beta density p r =\u0000 \u0000 \u0000 r \u00001 1\u0000r \u00001 derive expression expected value r ep r frg need following identity gamma function \u0000 n+ 1 =n\u0000 n hint use fact zr=1 r=0ra\u00001 1\u0000r b\u00001dr=\u0000 \u0000 b \u0000 a+b 3.6 using setup previous exercise identity varfrg=ep r \b r2 \u0000\u0000 ep r frg\u00012 derive expression varfrg need gamma identity given previous exercise 3.7 di erent stall observe 20 tosses 9 heads compute posteriors three scenarios probability winning case marginal likelihoods 3.8 use matlab generate coin tosses probability heads 0.7. generate 100 tosses compute posteriors three scenarios probabilities winning marginal likelihoods 3.9 section 3.8.4 derived expression gaussian posterior linear model within context olympic 100 data substituting \u00160= 0 0 0 saw similarity posterior mean \u0016w=1 \u001b2\u00121 \u001b2xtx+\u0006\u00001 0\u0013\u00001 xtt regularised least squares solution bw=\u0010 xtx+n\u0015i\u0011\u00001 xtt particular example nd prior covariance matrix \u00060that makes two identical words nd \u00060in terms \u0015 3.10 redraw graphical representation olympic 100 model ect fact prior wis actually conditioned \u00160and\u00060.3.11 figure 3.25 studied e ect reducing \u001b2 0on marginal likelihood using matlab investigate e ect increasing \u001b2 0 3.12 performing bayesian analysis olympics data assumed prior known gaussian prior placed wand inverse gamma prior variance \u001b2 p \u001b2j \u0000 \u001b2 \u0000 \u00001exp\u001a \u0000 \u001b2\u001b posterior also product gaussian inverse gamma compute posterior parameters 3.12 reading 1 ben calderhead mark girolami estimating bayes factors via thermody- namic integration population mcmc comput stat data anal. 53:4028 4045 october 2009. article authors describing novel approach calculating marginal likelihoods bayes factors models analytically tractable 2 andrew gelman john b. carlin hal s. stern donald b. rubin bayesian data analysis chapman hall/crc second edition 2004. one popular textbooks bayesian inference provides detailed practical description bayesian inference 3 michael isard andrew blake contour tracking stochastic propagation conditional density european conference computer vision pages 343 356 1996. interesting example use bayesian methods eld human computer interaction authors use sampling technique infer posterior probabilities gestures performed users 4 michael jordan editor learning graphical models mit press 1999. introduction eld graphical models use learning tasks 5 christian robert bayesian choice decision-theoretic foundations computational implementation springer second edition edition 2007 6 tian-rui xu et al inferring signaling pathway topologies multiple pertur- bation measurement speci c biochemical species science signalling 3 113 2010. paper showing bayesian model selection via marginal likeli- hood used answer interesting scienti c questions eld biology also interesting example large-scale bayesian sampling.a revealing introduction hidden markov models chapter excerpted introduction machine learning applications information security mark stamp 2018 taylor francis group rights reserved.3 learn morethe cause hidden eect visible ovid introduction background surprisingly hidden markov model hmm includes markov pro- cess hidden sense directly observe state process access series observations probabilistically related underlying markov model formulation hmms might initially seem somewhat contrived exist virtually unlimited number problems technique applied best ecient algorithms making hmms extremely practical another nice property hmm structure within data often deduced model chapter rst consider simple example motivate hmm formulation dive detailed discussion hmm algorithms realistic applicationsmostly information security domaincan found chapter 9. one detailed chapters book reason going much depth solid understanding partic- ular machine learning technique compare contrast techniques well consider addition hmms relatively easy understandalthough notation seem intimidating intuition process actually fairly straightforward.1 1to accurate dictatorial author wants start hmms thats really matters.thebottomlineisthatthischapteristhelinchpinformuchoftheremain der book consequently learn material chapter well pay large dividends subsequent chapters hand fail fully grasp details hmms much remaining material almost certainly dicult necessary hmms based discrete probability particular well need basic facts conditional probability remainder section provide quick overview crucial topic notation denotes given information read probability given two events 2.1 example suppose draw two cards without replacement standard 52-card deck let 1stcard ace and= 2ndcard ace 4/52 3/51 1/221 example depends happens rst event say andaredependent events hand suppose ip fair coin twice probability second ip comes heads 1/2 regardless outcome rst coin ip events independent dependent events given information relevant determining sample space consequently cases view information right given sign dening space probabilities computed rewrite equation 2.1 thi expression viewed denition conditional probability important application conditional probability see discussion na bayes section 7.8 chapter 7. well often use shorthand joint probability reality also discrete probability equivalent intersection sets andand sometimes well want emphasize set intersection consequently throughout section finally matrix notation used frequently chapter review matrices basic linear algebra found section 4.2.1 chapter 4 although linear algebra required chapter.a simple example suppose want todetermine average annual temperature aparticular location earth series years make interesting suppose years focused lie distant past thermometers invented since cant go back time instead look indirect evidence temperature simplify problem consider hot cold av- erage annual temperature suppose modern evidence indicates probability hot year followed another hot year 0.7 proba- bility cold year followed another cold year 0.6. well assume probabilities also held distant past information summarized 0.7 0.3 0.4 0.6 2.2 whereis hot cold next suppose current research indicates correlation size tree growth rings temperature simplicity consider threedierenttreeringsizes small medium andlarge denoted respectively furthermore suppose based currently available evi- dence probabilistic relationship annual temperature tree ring sizes given 0.1 0.4 0.5 0.7 0.2 0.1 2.3 system well say stateis average annual tempera- ture either transition one state next markov process,2since next state depends current state xed probabilities 2.2 however actual states hidden since cant directly observe temperature past although cant observe state temperature past observe size tree rings 2.3 tree rings provide us prob- abilistic information regarding temperature since underlying states hidden type system known hidden markov model hmm goal make eective ecient use observable information gain insight various aspects markov process 2a markov process current state depends previous state said order one markov process order n current state depends n consecutive preceding states case memory nitemuch like absent- minded authors memory seems become nite time lets see hmm example state transition matrix =0.7 0.3 0 .4 0.6 2.4 comes 2.2 observation matrix =0.1 0.4 0.5 0.7 0.2 0.1 2.5 comes 2.3 example suppose initial state distribution denoted 0.6 0.4 2.6 chance start state 0.6 chance start state 0.4. matrices andarerow stochastic fancy way saying row satises requirements discrete probability distribution i.e. element 0 1 elements row sum 1 suppose consider particular four-year period interest distant past particular four-year period observe series tree ring sizes letting 0 represent 1 represent 2 represent observation sequence denoted 0,1,0,2 2.7 might want determine likely state sequence markov processgiventheobservations 2.7 thatis wemightwanttoknowthemost likely average annual temperatures four-year period interest quite clear-cut seems since dierent possible inter- pretations likely one hand could dene likely state sequence highest probability among possible state sequences length four dynamic programming dp used eciently solve problem hand might reasonably dene likely state sequence maximizes expected number correct states hmm used nd likely hidden state sequence latter sense important realize dp hmm solutions problem necessarily example dp solution must deni- tion include valid state transitions case hmm even state transitions valid hmm solution still dier dp solution well illustrate example going detail need deal challenging aspect hmmsthe notation notation well discuss thethree fundamental problems hmms enable us solve well give detailed algorithms ecient solution also consider critical computational issues must addressed writing hmm com- puter program rabiner 113 standard reference introductory information hmms notation notation used hmm summarized table 2.1. note observations assumed come set 0,1 ... 1 sim- plies notation loss generality simply associate distinct observations one elements 0 1 ... 1 that= 0,1 ... 1 for= 0,1 ... ,1. table 2.1 hmm notation notation explanation length observation sequence num ber states model number observation symbols distinct states markov process 0,1 ... ,1 possible observations assumed 0 1 ... 1 state transition probabilities observation probability matrix initial state distribution observation sequence 0,1 ... ,1 generic hidden markov model illustrated figure 2.1 repr esent hidden states notation table 2.1. state markov process view hidden behind curtain dashed line figure 2.1 determined current state matrix able observe observations related hidden states markov process matrix temperature example previous section observations sequence given 2.7 4 2 3 and= 0,1,2 note let 0 1,2 represent small medium large tree rings respectively example matrices andare given 2.4 2.5 2.6 respectively general matrix iswith stateat+1|stateat .0 1 2 10 1 2 1 figure 2.1 hidden markov model matrixis always row stochastic also probabilities inde- pendent matrix change matrix size observation at|stateat matrix row stochastic probabilities independent somewhat unusual notation convenient specifying hmm algorithms hmm dened implicitly dimensions thus well denote hmm suppose given observation sequence length four denoted 0,1,2,3 corresponding hidden state sequence 0,1,2,3 well let 0denote probability starting state 0 and0 0 denotes probability initially observing 0 while0,1is proba- bility transiting state 0to state 1. continuing see probability given state sequence length four =00 0 0,11 1 1,22 2 2,33 3 2.8 notethatinthisexpression representindicesinthe andmatrices names corresponding states.3 3your kindly author regrets abuse notation.consider temperature example section 2.2 obser- vati sequence 0,1,0,2 using 2.8 compute say 0.6 0.1 0.7 0.4 0.3 0.7 0.6 0.1 0.000212. similarly directly compute probability possible state se- quence length four given observation sequence 2.7 listed results table 2.2 probabilities last column normalized sum 1. table 2.2 state sequence probabilities state probabilitynormalized probability 0.000412 0 042787 0.000035 0.003635 0.000706 0.073320 0.000212 0.022017 0.000050 0.005193 0.000004 0.000415 0.000302 0.031364 0.000091 0.009451 0.001098 0.114031 0.000094 0.009762 0.001882 0.195451 0.000564 0.058573 0.000470 0.048811 0.000040 0.004154 0.002822 0.293073 0.000847 0.087963 nd optimal state sequence dynamic programming dp sense simply choose sequence highest probability example nd optimal state sequence hmm sense choose probable symbol position end sum probabilities table 2.2 rst position nd normalized probability rst position 0 18817 probability rst position 0 81183. therefore rst element optimal sequence hmm sense repeating element sequence obtain probabilities table 2.3. table 2.3 nd optimal sequencein hmm sense note example optimal dp sequence diers optimal hmm sequence.table 2.3 hmm probabilities position state sequence 0 1 2 3 0.188182 0.519576 0.228788 0.804029 0.811818 0.480424 0.771212 0.195971 three problems three fundamental problems solve using hmms briey describe problems next section discuss ecient algorithms solution 2.4.1 hmm problem 1 given model sequence observations deter- mine want compute score observed se- quencewith respect given model 2.4.2 hmm problem 2 given= observation sequence nd optimal state sequence underlying markov process words want uncover hidden part hidden markov model problem discussed detail 2.4.3 hmm problem 3 given observation sequence parameter determine model form maximizes probability viewed training model best observed data well solve problem using discrete hill climb parameter space represented note dimension determined training sequence 2.4.4 discussion consider example problem speech recognitionwhich happens one earliest best-known applications hmms use solution hmm problem 3 train hmm example recognize spoken word yes given unknown spoken word use solution hmm problem 1 score word thismodeland determine likelihood word yes case dont need solve hmm problem 2 possible solution uncovers hidden statesmight provide additional insight underlying speech model three solutions 2.5.1 solution hmm problem 1 let= given hmm let 0,1 ... ,1 series observations want nd let= 0,1 ... 1 state sequence denition ofwe =0 0 1 1 1 1 denition andit follows =00,11,22,1 since ha summing possible state sequences yields 2.9 00 0 0,11 1 2,11 1 direct computation 2.9 generally infeasible since number multiplications 2 whereis typically large 2. one major strengths hmms exists ecient algorithm achieve result.to determine ecient manner use following approach 0,1 ... ,1 and= 0,1 ... 1 dene 0,1 ... ,=| 2.10 probability partial observation sequence time underlying markov process state time crucial insight computed recursivelyand eciently recursive approach known forward algorithm -pass given algorithm 2.1. algorithm 2.1 forw ard algorithm 1 given mo del= observations 0,1 ... ,1 2 for= 0,1 ... 1do 3:0 0 4 end 5 for= 1,2 ... ,1do 6 for= 0,1 ... 1do 7 =1 =01 8 end 9 end forward algorithm requires 2mul tiplications stark contrast na approach work factor 2. sinceis typically large relatively small forward algorithm highly ecient follows denition 2.10 =1 =01 hence forward algorithm gives us ecient way compute score given sequence relative given model 2.5.2 solution hmm problem 2 given model sequence observations goal nd likely state sequence mentioned dierent possible interpretations likelyfor hmm maximize expected number correct states contrast dynamic program ndsthe highest-scoring overall path seen solutions n ecessarily first dene +1 +2 ... ,1|= for= 0,1 ... ,1 and= 0,1 ... 1. computed recursively eciently using backward algorithm or-pass given algorithm 2.2. analogous -pass discussed except start end work back toward beginning algorithm 2.2 back ward algorithm 1 given mo del= observations 0,1 ... ,1 2 for= 0,1 ... 1do 3:1 1 4 end 5 for=2,3 ... ,0do 6 for= 0,1 ... 1do 7 =1 =0 +1 +1 8 end 9 end 0,1 ... ,1 and= 0,1 ... 1 dene =| since measures relevant probability time measures relevant probability time recal l denominator obtained summing 1 denition follows likely state time state maximum maximum taken index likely state time given max 2.5.3 solution hmm problem 3 want adjust model parameters best given observa- tions sizes matrices known elementsof andare determined subject row stochastic conditions fact eciently re-estimate model perhaps impressive aspect hmms for= 0,1 ... ,2 0,1 ... ,1 dene di-gammas +1=| probability state timeand transiting stateat time+ 1. di-gammas written terms andas +1 +1 f or= 0,1 ... ,2 see related =1 =0 computed model re-estimated using algorithm 2.3. hmm training algorithm known baum-welch re-estimation named leonard e. baum lloyd r. welch developed technique late 1960s working center communications research ccr ,4which part institute defense analyses ida located princeton new jersey numerator re-estimated algorithm 2.3 seen give expected number transitions state state denominator expected number transitions state.5 hence ratio probability transiting state state desired value numerator re-estimated algorithm 2.3 expected number times model state observation denom- inator expected number times model state therefore ratio probability observing symbol given model state desired value re-estimation iterative process first initialize reasonable guess reasonable guess available choose 4notto confused creedence clearwater revival 153 5when re-estimating amatrix dealing expectations however might make things clearer think terms frequency counts frequency counts would easy compute probability transitioning state ito state j. would simply count number transitions state ito statej divide count total number times could state i. intuition behind re-estimation formula amatrix similar statement holds re-estimating bmatrix words dont let fancy notation obscure relatively simple ideas core re-estimation process.algorithm 2.3 baum-w elch re-estimation 1 given for= 0,1 ... ,1 and= 0,1 ... 1 for= 0,1 ... ,2 0,1 ... 1 2 for= 0,1 ... 1do 3 =0 4 end 5 for= 0,1 ... 1do 6 for= 0,1 ... 1do 7 =2 =0 2 =0 8 end 9 end 10 for= 0,1 ... 1do 11 for= 0,1 ... 1do 12 0,1 ... 1 1 =0 13 end 14 end random values 1/and 1/and 1/ critical andbe randomized since exactly uniform values result local maximum model climb always ,andmust row stochastic complete solution hmm problem 3 summarized follows 1. initialize 2. compute 3. re-estimate model using algorithm 2.3 4. increases goto 2. practice would want stop increase predetermined threshold say could also alternatively set max- imum number iterations case important verify model converged usually determined perusing matrix.6 6while might seem obvious stop iterating change p o| small requires care practice typically change p o| small overdynamic programming completing discussion elementary aspects hmms make brief detour show close relationship dynamic pro- gramming dp hmms executive summary dp viewed -pass sum replaced max precisely andas dynamic programming algorithm also known viterbi algorithm given algorithm 2.4. algorithm 2.4 dyn amic programming 1 given mo del= observations 0,1 ... ,1 2 for= 0,1 ... 1do 3:0 0 4 end 5 for= 1,2 ... ,1do 6 for= 0,1 ... 1do 7 max 0,1 ... 1 1 8 end 9 end successive ynamic program determines probability best path ending states 0,1 ... 1. consequently probability best overall path max 0,1 ... 1 1 2.11 important realize 2.11 gives optimal probability corresponding path keeping track preceding state dp procedure given augmented recover optimal path tracing back highest-scoring nal state consider example section 2.2. initial probabilities =00 0 0.6 0.1 0.06 =11 0 0.4 0 7 0.28. probabilities paths length two given 0.06 0.7 0.4 0.0168 rst several iterations model goes period rap id improvement point model convergedafter change p o| small consequently simply set threshold re-estimation process might stop immediately might continue indenitely perhaps optimal approach combine threshold minimum number iterationsthe pseudo-code section 2.8 uses approach 0.06 0 3 0.2 0.0036 0.28 0 4 0.4 0.0448 0.28 0.6 0.2 0.0336 hence best probable path length two ending best path length two ending continuing construct diagram figure 2.2 one level stage time arrow points next element optimal path ending given state note stage dynamic programming algorithm needs maintain highest-scoring path ending statenot list possible paths key eciency algorithm h.06 c .28h.0448 c .0336h.003136 c .014112h.002822 c .000847 figure 2.2 dynamic programming f igure 2.2 maximum nal probability 0.002822 occurs nal state use arrows trace back nd optimal path note agrees brute force calculation table 2.2. underow concern dynamic programming problem formsince compute products probabilities result tend 0. fortunately underow easily avoided simply taking logarithms underow-resistant version dp given algorithm 2.5. algorithm 2.5 dyn amic programming without underow 1 given mo del= observations 0,1 ... ,1 2 for= 0,1 ... 1do 3:0 log 0 4 end 5 for= 1,2 ... ,1do 6 for= 0,1 ... 1do 7 max 0,1 ... 1 1 +log +log 8 end 9 end fornot surprisingly underow-resistant version algorithm 2.5 optimal score given max 0,1 ... ,1 1 additional bookkeeping required determine optimal path scaling three hmm solutions section 2.5 require computations involving products probabilities easy see example tends 0 exponentially increases therefore attempt implement hmm algorithms given section 2.5 inevitably result underow solution underow problem scale numbers however care must taken ensure algorithms remain valid first consider computation basic recurrence =1 =01 seems sensible normalize dividing 1 =0 fol lowing approach compute scaling factors scaled denote algorithm 2.6. verify algorithm 2.6 rst note 0 =00 suppose th =01 2.12 +1 =+1+1 =+1 1 =0 +1 =01 +11 =0 +1 =01 +1+1 hence 2.12 holds induction .algorithm 2.6 scali ng factors 1 given for= 0,1 ... ,1 and= 0,1 ... 1 2 for= 0,1 ... 1do 3:0 =0 4 end 5:0= 1/1 =00 6 for= 0,1 ... 1do 7:0 =00 8 end 9 for= 1,2 ... ,1do 10 for= 0,1 ... 1do 11 =1 =01 12 end 13 11 =0 14 for= 0,1 ... 1do 15 16 end 17 end 2.12 denitions andit follows 1 =0 2.13 equation 2.13 see desired scaled value indeed given 2.13 follows 1 =01 1. also 2.12 1 =01 =0111 =01 =011 .combining results gives us 11 =0 follows compute log directly scaling factorsas log =1 =0log 2.14 fairly easy show scale factors used backward algorithm simply computing deter- mine usingthesameformulaeasinsection2.5 butwith place respectively resulting gammas di-gammas used re-estimate writing original re-estimation formulae given lines 3 7 12 algorithm 2.3 directly terms straight- forward exercise show re-estimated andandare exact used place furthermore isnt required re-estimation formulae since case cancels numerator denominator therefore 2.14 determines score model used example decide whether modelisimprovingsucientlytocontinuetothenextiterationofthetraining algorithm together give complete pseudo-code solving hmm problem 3 including scaling pseudo-code also provides virtually everything needed solve hmm problems 1 2 1. given observation sequence 0,1 ... ,1 2. initialize select determine recall model de- noted= and= 1 b initialize three matrices use knowl- edge problem generating initial values suchinformation available often case let 1/and l et1/and 1/ always sure initial values satisfy row stochastic conditions i.e. elements row sum 1 element 0 1 also make sure elements row notexactly uniform c initialize following miniters minimum number re-estimation iterations threshold representing negligible improvement model iters= 0 oldlogprob 3. forward algorithm -pass //compute 0 0= 0 for= 0to1 0 0 0=0+0 nex //scale 0 0= 1/0 for= 0to1 0 =00 nex //compute f or= 1to1 0 for= 0to1 0 f or= 0to1 1 next =+ nex //scale 1/ for= 0to1 nex next4 backward algorithm -pass //let1 1 scaled 1 for= 0to1 1 =1 n ext //-pass for=2t o0by1 for= 0to1 0 f or= 0to1 +1 +1 next // scale wit h scale factor nex nextt 5. compu te gammas di-gammas for= 0to2 denom= 0 for= 0to1 f or= 0to1 denom=denom+ +1 +1 nex next for= 0to1 0 f or= 0to1 +1 +1 /denom next next next //special case 1 denom= 0 for= 0to1 denom=denom+ 1 next for= 0to1 1 =1 /denom next6 re-estimate model //re-estimate for= 0to1 =0 n ext //re-estimate for= 0to1 f or= 0to1 numer= 0 denom= 0 f or= 0to2 numer=numer+ denom=denom+ next =numer/ denom next next //re-estimate for= 0to1 f or= 0to1 numer= 0 denom= 0 f or= 0to1 f == th en numer=numer+ end denom=denom+ next =numer/ denom next next 7. compu te log logprob 0 for= 0to1 logprob =logprob log n ext logprob =logprob8 iterate iterate question iters=iters+1 =|logprob oldlogprob| f iters miniters oldlogprob =logprob goto3 else retu rn= en bottom line hidden markov models powerful ecient extremely useful prac- tice virtually assumptions need made yet hmm process extractsignicantstatisticalinformationfromdata thankstoecienttrain- ing scoring algorithms hmms practical proven useful wide range applications even cases underlying assump- tion hidden markov process questionable hmms often applied success chapter 9 consider selected applications hmms applications eld information security subsequent chapters often compare contrast machine learning techniques hmms consequently clear understanding material chapter crucial proceeding remainder book homework problem help dedicated reader clarify remaining issues applications chapter 9 highly recom- mended english text example section 9.2 especially highly recommended problems faced problem understand part understand look robert heinlein 1. suppose train hmm obtain model =0.7 0.3 0.4 0.6 =0.1 0.4 0.5 0.7 0.2 0.1 0.0 1.0 .furthermore suppose hidden states correspond r e- spectively observations mapped 0 1 2 respectively problem consider observation sequence 0,1,2 1 ,0,2 directly compute compute using probabilities following cases based given observation sequence == == == == == == =1.00.20.60.70.40.5= == desired probability sum eight probabilities b comp ute using pass compute 0 0 0 1 =1.00.2= 1 0 1 1 2 0 2 1 initialize 0 0 for= 0,1 ... 1 recurrence =1 =01 for= 1,2 ... ,1 and= 0,1 ... ,1. desired probability given =1 =01 .c terms counting multiplications work factor method part work factor method part b 2. problem use model observation sequence given problem 1. determine best hidden state sequence 0,1,2 dy- namic programming sense b determine best hidden state sequence 0,1,2 hmm sense 3. summing numbers probability column table 2.2 nd 0.009629 0,1,0,2 similar direct calculation compute observa- tionsequenceoftheform 0,1,2,3 0,1,2 verify 1 sum observation sequences length four note need use proba- bilities andgiven equations 2.4 2.5 2.6 section 2.2 respectively b use forward algorithm compute obser- vation sequences model part verify obtain results part 4. equation 2.9 denition equation 2.10 follows 00 0 0,11 1 2,11 1 1 where= 0,1 ... 1 use expression directly verify forward algorithm recurrence =1 =01 5. discussed chapter forward algorithm used solve hmm problem 1 forward algorithm backward algorithm to- gether used compute gammas used solve hmm problem 2. explain solve hmm problem 1 using backward algorithm instead forward algorithm.b using model observation sequence problem 1 compute using backward algorithm verify obtain result using forward algorithm 6. problem deals baum-welch re-estimation algorithm write re-estimation formulae given lines 3 7 12 algorithm 2.3 directly terms b using re-estimation formulae obtained part substitute scaled values respectively show resulting re-estimation formulae exact 7. instead using scale scale 11 =0 denition analogous given algorithm 2.6. using scaling factors andshow baum-welch re- estimation formulae algorithm 2.3 exact andin place ofand b write log terms 8. training elements initialized approximately uniform let 1/and1/and 1/ subject row stochastic conditions section 2.5.3 stated bad idea initialize values exactly uniform since hmm would stuck local maximum hence could climb improved solution suppose 1/and= 1/ 1/ verify re-estimation process leaves values unchanged 9. problem consider generalizations hmm formulation discussed chapter consider hmm state transition matrix time depen- dent row-stochastic used place hmm computations hmm provide pseudo-code solve hmm problem 1. b consider hmm order two hmm un- derlying markov process order two state time depends states time 1 and2 hmm provide pseudo-code solve hmm problem 1.10. write hmm program english text problem section 9.2 chap ter 9. test program following cases 2 hidden states explain results b 3 hidden states explain results c 4 hidden states explain results 26 hidden states explain results 11. problem use hmm break simple substitution ciphertext message hmm train using 200 iterations baum-welch re-estimation algorithm obtain english plaintext message 50,000 plaintext characters characters consist lower case athroughz i.e. re- move punctuation special characters spaces convert upper case lower case encrypt plaintext using randomly generated shift alphabet remember key b train hmm 2 26 ciphetext part nal matrix determine ciphertext letters correspond consonants vowels c generate digraph frequency matrix english text count number times letter followed letter weassumethat aisletter0 bisletter1 cisletter2 andsoon matrix must based 1,000,000 characters 26 letters alphabet used next add element 26 26 matrix finally normalize matrix dividing element row sum resulting matrix row stochastic contain 0 probabilities train hmm == 26 using rst 1000 characters ciphertext generated part matrix initialized matrix part c also hmm re- estimate use nal matrix determine putative key give fraction putative key elements match actual key decimal four places example 22 26 key positions correct answer would 22/26 0 8462 12. write hmm program solve problem discussed section 9.2 replacing english text following french text b russian text c chinese text.13 perform hmm analysis similar discussed section 9.2 re- placi ng english hamptonese mysterious writing system de- veloped james hampton information hamptonese see http //www.cs.sjsu.edu/faculty/stamp/hampton/hampton.html 14. since hmm training hill climb assured reaching local maximum aswithanyhillclimb thespeciclocalmaximum nd depend choice initial values therefore training hidden markov model multiple times dierent initial values would expect obtain better results training paper 16 authors use expectation maximization em approach multiple random restarts means attacking ho- mophonic substitution ciphers analogous hmm-based technique analyzed report 158 eectiveness multiple ran- dom restarts simple substitution cryptanalysis explored detail multiple random restarts especially helpful challenging cases little data i.e. ciphertext available however tradeo work factor high since number restarts required may large millions random restarts required cases obtain english plaintext message consisting 1000 plaintext characters consisting lower case athroughz i.e. remove punctuation special characters spaces convert upper case letters lower case encrypt plaintext using randomly selected shift alphabet remember key also generate digraph frequency matrix discussed part c problem 11. b train hmms 1 10 100 1000 following process problem 11 part using the= 1000 observations generated part problem given select best result based model scores give fraction putative key correct calculated problem 11 part c repeat part b use rst 400 observations repeat part c use rst 300 observations 15. thezodiackillermurderedatleastvepeopleinthesanfranciscobay area late 1960s early 1970s although police prime suspect arrest ever made murders remain ocially unsolved killer sent several messages police local newspapers taunting police failure catch one thesemessages contained homophonic substitution consisting 408 strange sym bols.7not surprisingly cipher known zodiac 408. within days release zodiac 408 broken donald bettye harden schoolteachers salinas california zodiac408ciphertextisgivenbelowontheleft whilethecorresponding plaintext appears right l k e k l l n g p e p l e b e c u e u c h f u n r e f u n h n k l l n g w l g e n h e f r r e b e c u e n h e n g e r u e n l f l l k l l e h n g g v e e h e h r l l n g e x p e r e n c e e v e n b e e r h n g e n g u r r c k f f w h g r l h e b e p r f h e w h e n e w l l b e r e b r n n p r c e n l l h e h v e k l l e w l l b e c e l v e w l l n g v e u n e b e c u e u w l l r l w n r p c l l e c g f l v e f r f e r l f e e beorietemethhpiti note apparently intentional misspellings plaintext inclu d- ing forrest anamal also nal 18 characters underlined plaintext appear random ller sol zodiac 408 cipher using hmm approach discussed section 9.4. initialize matrix part c problem 11 re-estimate use 1000 random restarts hmm 200 iterations baum-welch re-estimation case give answer percentage characters actual plaintext recovered correctly b repeat part use 10,000 random restarts c repeat part b use 100,000 random restarts repeat part c use 1,000,000 random restarts 7thezodiac 408 ciphertext actually sent three parts local newspapers give complete message three parts combined one also homophonic substitution like simple substitution except mapping many- to-one multiple ciphertext symbols map one plaintext symbol.e repeat part except also re-estimate matr ix f repeat part b except also re-estimate matrix g repeat part c except also re-estimate matrix h repeat part except also re-estimate matrix 16. addition zodiac 408 cipher zodiac killer see problem 15 releasedasimilar-lookingcipherwith340symbols thiscipherisknown zodiac 340 remains unsolved day.8the ciphertext given repeat problem 15 parts using zodiac 340 place zodiac 408. since plaintext unknown case simply print decryption obtained highest scoring model b repeat part problem except use parts e h problem 15 8it possible zodiac 340 cipher instead random collection symbols designed frustrate would-be cryptanalysts thats case easily frustrated author conrm cipher wildly successful.introduction reinforcement learning chapter excerpted statistical reinforcement learning masashi sugiyama 2018 taylor francis group rights reserved.4 learn moreintroduction reinforcement lear ning reinforcement learning aimed controlling computer agent target task achieved unknown environment chapter rstgivean informaloverviewofreinforcementlearning section 1.1. provide formal formulation reinforcement learning section 1.2. finally book summarized section 1.3 1.1 reinforcement learning asch ematicofreinforcementlearningisgiveninfigure1.1.inanunknown environment e.g. maze computer agent e.g. robot takes action e.g. walk based control policy state updated e.g. moving forward evaluation action given reward e.g. praise neutral scolding interaction environment agent trained achieve certain task e.g. getting maze without explicit guidance crucial advantage reinforcement learning non-greedynature thatis agentis trainednot improveperformancein short term e.g. greedily approaching exit maze optimize long-term achievement e.g. successfully getting maze reinforcement learning problem contains various technical components states actions transitions rewards policies values go- ing mathematical details provided section 1.2 intuitively explain concepts illustrative reinforcement learning problems letus considera maze problem figure 1.2 wherearobotagentislocated maze want guide goal without explicit supervision direction go statesare positions maze robot agent visit example illustrated figure 1.3 21 states maze actionsare possible directions along robot agent move example illustrated figure 1.4 4 actions corre- spond movement toward north south east west directions statesstat istical reinforcement learning agentsta teaction rewardenv ironment figure 1.1 rein forcement learning actions fundamental elements dene reinforcement learning problem transitions specify states areconnected actions figure1.5 .thus knowingthetransitionsintuitivelymeansknowingthemap maze rewards specify incomes/costs robot agent receives making transition one state another certain action case maze example robot agent receives positive reward reaches goal specically positive reward provided making transition state 12 state 17 action east state 18 state 17 action north figure 1.6 thus knowing rewards intuitively means knowing location goal state emphasize fact reward given robot agent right taking action making transition next state also referred immediate reward abovesetup goalofreinforcement learningto nd policy controlling robot agent allows receive maximum amount rewards long run policy species action robot agent takes state figure 1.7 policy series states ac- tions robot agent takes start state end state specied series called trajectory see figure 1.7 sum im- mediate rewards along trajectory called return practice rewards obtained distant future often discounted re- ceiving rewards earlier regarded preferable maze task discounting strategy urges robot agent reach goal quickly possible nd optimal policy eciently useful view return function initial state called state- value values eciently obtained via dynamic programming general method solving complex optimization problem breaking simpler subproblems recursively hope many subproblems actually dynamic programming solves overlapped subproblems reuses solutions reduce computation costs maze problem value state computed values neighboring states example let us compute value state 7 seeintroduction reinforcement learning figure 1.2 ze problem want guide robot agent goal 1 3 52 46 107 912 1613 1517 2118 2014 19 8 11 figure 1.3 sta tes visitable positions maze th southwe st east figure 1.4 act ions possible movements robot agent.stat istical reinforcement learning 1 3 52 46 107 912 1613 1517 2118 2014 19 8 11 figure 1.5 tra nsitions specify connections states via actions thus knowing transitions means knowing map maze 1 3 52 46 107 912 1613 1517 2118 2014 19 8 11 figure 1.6 po sitive reward given robot agent reaches goal thus reward species goal location figure 1.7 po licy species action robot agent takes state thus policy also species trajectory series states actions robot agent takes start state end state.introduction reinforcement learning .35 .43.39 .48.39 .48.43 .53.9 .59.81 .661 .66.9 .73.73 .81 .59.48 .43 figure 1.8 valu es state reward +1 given goal state reward discounted rate 0 .9 according number steps figure 1.5 state 7 robot agent reach state 2 state 6 state 8 single step robot agent knows values neighboring states best action robot agent take visit neighboring state largest value allows robot agent earn largest amount rewards long run however values neighboring states unknown practice thus also computed need solve 3 subproblems computing values state 2 state 6 state 8. way subproblems decomposed follows problem computing value state 2 decomposed 3 subproblems computing values state 1 state 3 state 7. problem computing value state 6 decomposed 2 subproblems computing values state 1 state 7. problem computing value state 8 decomposed 3 subproblems computing values state 3 state 7 state 9. thus removing overlaps original problem computing value state 7 hasbeen decomposedinto 6 unique subproblems computing values state 1 state 2 state 3 state 6 state 8 state 9. continue problem decomposition encounter prob- lem computing values state 17 robot agent receive reward +1 values state 12 state 18 explicitly com- puted indeed discounting factor multiplicative penalty delayed rewards 0 .9 values state 12 state 18 0 .9 1= 0.9. know values state 13 state 19 0 .9 2= 0.81. repeating procedure compute values states illus- trated figure 1.8 based values know optimal actionstat istical reinforcement learning robot agent take i.e. action leads robot agent neighboring state largest value note real-world reinforcement learning tasks transitions often deterministic stochastic external disturbance case maze example oor may slippery thus robot agent move perfectly desires also stochastic policies mapping state action deterministic often employed many reinforcement learning formulations cases formulation becomes slightly complicated essentially idea still used solving problem highlight notable advantage reinforcement learning immediate rewardsbut long-term accumulation rewardsis max- imized let us consider mountain-car problem figure 1.9 two mountains car located valley mountains goal guide car top right-hand hill however engine car powerful enough directly run right-hand hill reach goal optimal policy problem rst climb left-hand hill go slope right full acceleration get goal figure 1.10 suppose dene immediate reward moving car right gives positive reward +1 moving car left gives nega- tive reward1 greedysolution maximizes immediate reward moves car right allow car get goal due lack engine power hand reinforcement learning seeks solution maximizes return i.e. discounted sum immediate rewards agent collect entire trajectory means reinforcement learning solution rst move car left even though negative rewards given receive positive re- wards future thus notion prior investment naturally incorporated reinforcement learning framework 1.2 mathematical formulation th section reinforcement learning problem mathematically for- mulated problem controlling computer agent markov de- cision process consider problem controlling computer agent discrete- timemarkov decision process mdp discrete time-step agent observes state sts selects action ata makes transition st+1s receives immediate reward rt=r st st+1 r.introduction reinforcement learning goal car figure 1.9 mo untain-car problem want guide car goal however engine car powerful enough directly run right-hand hill goal figure 1.10 optimal policy reach goal rst climb left-hand hill head right-hand hill full acceleration sandaarecalled state space andtheaction space respectively r called immediate reward function initial position agent s1 drawn initial probability distribution state space sis discrete initial probability distribution specied probability mass function p 0p 1 ss /summationdisplay ssp 1. state space sis continuous initial probability distribution speci- ed probability density function p p 0 ss statistical reinforcement learning /integraldisplay ssp ds= 1. probability mass function p expressed probability density function p using dirac delta function1 p =/summationdisplay ss ss p focus continuous state space dynamics environment represent transition prob- ability state sto stateswhen action ais taken characterized thetransition probability distribution conditional probability density p s|s p s|s 0 aa /integraldisplay ssp s|s ds= 1 ss aa theagentsdecisionisdeterminedbya policy.whenweconsidera deter- ministic policy action take state uniquely determined regard policy function states ss actionacan either discrete continuous hand devel- oping sophisticated reinforcement learning algorithms often convenient consider stochastic policy action take state probabilistically determined mathematically stochastic policy con- ditional probability density taking action aat states a|s 0 ss /integraldisplay aa a|s da= 1 ss introducing stochasticity action selection actively explore theentirestatespace.notethatwhenaction aisdiscrete thestochasticpolicy expressed using diracs delta function case state densities sequence states actions obtained procedure described figure 1.11 called trajectory 1the dirac delta function allows us obtain value function fat point via theconvolution withf /integraldisplay f ds=f diracs delta function expressed gaussian density standard deviation 0 lim 01 22exp/parenleftbigg a2 22/parenrightbigg .introduction reinforcement learning 1. initial state s1is cho sen following initial probability p 2. fort= 1 ... action atis chosen following policy at|st b next state st+1is determined according transition probability p st+1|st figure 1.11 gen eration trajectory sample number steps nite innite situation called thenite horizon orinnite horizon respectively focus nite-horizon case trajectory length always nite practice denote trajectory h stands history h= s1 a1 ... st st+1 discounted sum immediate rewards along trajectory called thereturn r h =t/summationdisplay t=1t1r st st+1 0,1 called discount factor future rewards goal reinforcement learning learn optimal policy maximizes expected return argmax ep h /bracketleftbig r h /bracketrightbig whereep h denotes expectation overtrajectory hdrawn p h p h denotes probability density observing trajectory hunder policy p h =p s1 t/productdisplay t=1p st+1|st at|st argmax gives maximizer function figure 1.12 policy learning various methods developed far methods classied model-based reinforcement learning andmodel- free reinforcement learning term model indicates model tran- sition probability p s|s model-based reinforcement learning ap- proach transition probability learned advance learned tran- sition model explicitly used policy learning hand model-free reinforcement learning approach policies learned without ex- plicitly estimating transition probability strong prior knowledge thestat istical reinforcement learning argmaxmax figure 1.12 ar gmax gives maximizer function max gives maximum value function transition model available model-based approach would favor- able hand learning transition model without prior knowl- edge hard statistical estimation problem thus good prior knowl- edge transition model available model-free approach would promising 1.3 structure book th section explain structure book covers major reinforcement learning approaches 1.3.1 model-free policy iteration policy iteration popular well-studied approach reinforcement learning key idea policy iteration determine policies based value function let us rst introduce state-action value function q rfor policy dened expected return agent receive taking action aat statesand following policy thereafter q =ep h /bracketleftbig r h /vextendsingle/vextendsingle/vextendsingles1=s a1=a/bracketrightbig |s1=s a1=a means initial state s1and rst action a1 xed ats1=sanda1=a respectively right-hand side equation denotes conditional expectation r h givens1=s anda1=a letq betheoptimalstate-actionvalueatstate sforactionadened q max q based optimal state-action value function optimal action agent take state sis deterministically given maximizer q introduction reinforcement learning 1. initialize policy a|s 2. r epeat following two steps policy a|s converges policy evaluation compute state-action value function q current policy a|s b policy improvement update policy a|s /parenleftbigg aargmax aq /parenrightbigg figure 1.13 algo rithm policy iteration respect a. thus optimal policy a|s given a|s =/parenleftbigg aargmax aq /parenrightbigg denotes diracs delta function optimal state-action value qis unknown practice policy iteration algorithm alternately evaluates value qfor current policyandupdates policy based onthe currentvalue q figure 1.13 performance policy iteration algorithm depends quality policy evaluation i.e. learn state-action value function data key issue value function approximation corresponds re- gression problem statistics machine learning thus various statistical machine learning techniques utilized better value function approx- imation part ii book addresses issue including least-squares es- timation model selection chapter 2 basis function design chapter 3 ecient sample reuse chapter 4 active learning chapter 5 robust learning chapter 6 1.3.2 model-free policy search one potential weaknesses policy iteration policies learned via value functions thus improving quality value function approximation necessarily contribute improving quality resulting policies furthermore small change value functions cause big dierence policies problematic e.g. robot control instability damage robots physical system another weakness policy iteration policy improvement i.e. nding maximizer q respect computationally expensive dicult action spaceais continuous.stat istical reinforcement learning policy search directly learns policy functions without estimating value functions overcome limitations basic idea policy search nd policy maximizes expected return argmax ep h /bracketleftbig r h /bracketrightbig policy search nd good policy function vast function space thekeyissuetobeaddressed.partiii ofthisbookfocusesonpolicysearchand introducesgradient-basedmethods andtheexpectation-maximizationmethod chapter 7 chapter 8 respectively however potential weakness direct policy search methods instability due stochasticity ofpolicies.toovercometheinstabilityproblem analternativeapproachcalled policy-prior search learns policy-priordistribution deterministic policies introduced chapter 9. ecient sample reuse policy-prior search also discussed 1.3.3 model-based reinforcement learning model-free approaches policies learned without explicitly modeling unknown environment i.e. transition probability agent environment p s|s hand model-based approach explicitly learns environment advance uses learned environment model policy learning additionalsampling costis necessarytogeneratearticialsamplesfrom learned environment model thus model-based approach particu- larly useful data collection expensive e.g. robot control however accurately estimating transition model limited amount trajec- tory data multi-dimensional continuous state action spaces highly challenging part iv book focuses model-based reinforcement learn- ing.inchapter10 anon-parametrictransitionmodelestimatorthatpossesses optimalconvergencerate high computational eciency introduced however even optimal convergence rate estimating transition modelinhigh-dimensionalstateandactionspacesisstill challenging.inchap- ter 11 dimensionality reduction method eciently embedded intothe transitionmodel estimationprocedureisintroduced andits usefulness demonstrated experiments.deep learning feature representation chapter excerpted feature engineering machine learning data analytics guozhu dong huan liu 2018 taylor francis group rights reserved.5 learn moredeep learning feature representation suhang wang arizona state university huan liu arizona state university 11.1 introduction 279 11.2 restricted boltzmann machine 280 11.2.1 deep belief networks deep boltzmann machine ::281 11.2.2 rbm real-valued data 283 11.3 autoencoder 284 11.3.1 sparse autoencoder 286 11.3.2 denoising autoencoder 287 11.3.3 stacked autoencoder 287 11.4 convolutional neural networks 288 11.4.1 transfer feature learning cnn 290 11.5 word embedding recurrent neural networks 291 11.5.1 word embedding 291 11.5.2 recurrent neural networks 294 11.5.3 gated recurrent unit 295 11.5.4 long short-term memory 296 11.6 generative adversarial networks variational autoencoder 296 11.6.1 generative adversarial networks 297 11.6.2 variational autoencoder 298 11.7 discussion readings 299 bibliography 301 11.1 introduction deep learning methods become increasingly popular recent years tremendous success image classication 19 speech recog- nition 20 natural language processing tasks 60 fact deep learning methodshaveregularlywonmanyrecentchallengesinthesedomains 19 .thegreat success deep learning mainly comes specially designed structures deep nets able learn discriminative non-linear features facilitate task hand example specially designed convolu- tional layers cnn allow extract translation-invariant features images max pooling layers cnn help reduce parameters learned essence majority existing deep learning algorithms used powerful feature learning/extraction tools i.e. latent features extracted deep learning algorithms new learned representations chapter review classical popular deep learning algorithms explain used feature representation learning also discuss used hierarchical disentangled representation learning applied various domains 11.2 restricted boltzmann machine restricted boltzmann machine rbm undirected graphical model denes probability distribution vector observed visible variables v2f0 1gmand vector latent hidden variables h2f0 1gd wheremis dimension input features dis dimension latent features widely used unsupervised representation learning example vcan bag-of-words representation documents vectorized binary images learned representation input data typical choice i.e. learning compact representation figure 11.1 gives toy example rbm gure node hidden layer connected node visible layer connections hidden nodes visible nodes figure 11.1 b simplied representation rbm connection details hidden layers visible layers simplied begin assuming vandhas binary vectors i.e. elements vandhcan take value 0 1. extension real-valued input xwill introduced 11.2.2. rbm denes joint probability vandhas p v h =1 zexp \u0000e v h 11.1 wherezis partition function dened z=p vp hexp \u0000e v h eis energy function given e v h =\u0000htwv\u0000bth\u0000ctv 11.2 w2rd\u0002mis matrix pairwise weights elements vandh see figure 11.1 b2rd\u00021andc2rm\u00021are biases hidden visible variables respectively.1 1for simplicity bias terms shown figure 11.1.since explicit connections hidden units rbm given randomly selected training data v hidden units independent gives p hjv =qd i=1p hijv binary state hi i= 1 set 1 conditional probability given p hi= 1jv =\u001b\u0000mx j=1wijvj+bi\u0001 11.3 where\u001b \u0001 sigmoid function dened \u001b x 1 exp \u0000x \u00001 simi- larly given h visible units independent thus p vjh =qm j=1p vjjh binary state vj j= 1 set 1 conditional probability given p vj= 1jh =\u001b dx i=1wijhi+vj 11.4 simple conditional probabilities given eq 11.3 eq 11.4 sampling p hjv andp vjh becomes ecient rbms gen- erally trained using gradient ascent maximize log-likelihood l \u0012 set training vectors v2rm\u0002n where\u0012=fw b cgis set variables optimized log-likelihood l \u0012 written l \u0012 =1 nlogp v =1 nnx i=1logp vi 11.5 derivative logp v w.r.t variable wis given logp v w=x hp hjv hvt\u0000x ~vx hp ~v h h~vt 11.6 ~v2 f0 1gmis anm-dimensional binary vector rst term eq 11.6 computed exactly term often referred pos- itivegradient corresponds expected gradient energy respect top hjv second term eq 11.6 known negative gradient expectation model distribution p v h in- tractable compute negative gradients exactly thus need approx- imate negative gradients sampling vfromp vjh sampling hfrom p hjv maintaining gibbs chain details encourage readers refere contrastive divergence 62 11.2.1 deep belief networks deep boltzmann machine rbms stacked trained greedy manner form so-called deep belief networks dbn 21 dbns graphical models learn rbm b rbm c 3-layer dbn 3-layer dbm figure 11.1 illustration rbm dbn dbm extract deep hierarchical representation training data model joint distribution observed vector vand thelhidden layers p x h1 h2 hl l\u00002y k=0p hkjhk+1 p hl\u00001 hl 11.7 v=h0.p hk\u00001jhk conditional distribution visible units conditioned hidden units rbm level k andp hl\u00001 hl visible-hidden joint distribution top-level rbm illustrated figure 11.1 c dbn able learn hierarchical representation 33 low-level hidden representation h1captures low-level features high-level hidden representation h3captures complex high- level features training dbn done greedy layer-wise unsupervised training 21 specically rst train rst layer rbm raw input v. rst layer obtain latent representation mean activations p h1jh0 samples p h1jh0 used input second layer update w2 layers trained ne-tune parameters dbn respect proxy dbn log-likelihood respect supervised training criterion adding classier softmax function top dbn deep boltzmann machine dbm 51 another kind deep generative model figure 11.1 gives illustration dbm 3 hidden layers unlike dbn entirely undirected model unlike rbm dbm several layers latent variables rbms one within layer variables mutually independent conditioned variables neighboring layers case deep boltzmann machine one visible layer v andlhidden layers h1 h2andhl joint probability given p v h1 h2 hn =1 zexp \u0000e v h1 h2 hn 11.8 dbm energy function dened e v h1 h2 hn =\u0000 l\u00001x k=0hkwkhk+1 \u0000x kbkhk 11.9 andv=h0 wkis weight matrix capture interaction hk andhk+1 bkis bias conditional distribution one dbm layer given neighboring layers factorial example dbm two hidden layers distributions p vjh1 p h1jv h2 andp h2jh1 distribution hidden layers generally factorize interactions layers example two hidden layers p h1 h2jv factorize due interaction weights w1between h1andh2which render variables mutually dependent therefore sampling p h1 h2jv di- cult training dbm using gradient ascent methods require sampling fromp h1 h2jv solve problem use mean-eld approximation approximate p h1 h2jv specically dene q h1 h2 =y jq h1 jjv kq h2 kjv 11.10 mean eld approximation attempts nd member family distributions best ts true posterior p h1 h2jv minimizing kl- divergence q h1 h2 andp h1 h2jv approximation easily sample h1andh2fromq h1 h2 update parameters using gradient ascents samples 51 11.2.2 rbm real-valued data many real-world applications image audio modeling input features vare often real-valued data thus important extend rbm modeling real-valued inputs many variants rbm denes probability real-valued data gaussian-bernoulli rbms 69 mean variance rbms 22 spike slab rbms 8 gaussian-bernoulli rbm gbm common way handle real-valued data binary hidden units real-valued visible units assumes conditional distribution visible units gaussian distribution whose mean function hidden units assump- tion grbm denes joint probability vandhas eq 11.1 energy function given e v h =\u0000htw v \u0000bth\u00001 2v\u0000ct v\u0000c 11.11 2rm\u00021is precision vector i-th element ibeing precision vi hadamard operation conditional probability ofp vjh andp hjv p hjv =dy i=1p hijv =dy i=1\u001b bi+mx j=1wijvi 11.12 p vjh =my j=1p vjjh =my j=1n vjjbj+dx i=1wijhi \u00001 11.13 wheren vjjbj+pd i=1wijhi \u00001 gaussian distribution mean bj+pd i=1wijhiand variance \u00001 i. grbm canonical energy model real-valued data well suited statistical variations present types real-valued data especially natural images 31 problem much information content present natural images embedded covari- ance pixels rather raw pixel values solve prob- lems alternative models proposed attempt better account forthecovarianceofreal-valueddata.meanandcovariancerbm mcrbm one alternatives mcrbm uses hidden units independently encode conditional mean covariance observed units specically hidden layer mcrbm divided two groups units binary mean units h binary covariance units h c energy function mcrbm dened combination two energy functions emc v h h c =em v h +ec v h c 11.14 whereem v h isthestandardgaussian-bernoullienergyfunctiondened eq 11.11 models interaction real-valued vinput hidden units h andec v h c models conditional covariance informa- tion given ec v h c =1 2x jh c j vtr j 2\u0000x jb c jh c j 11.15 parameter r j corresponds covariance weight vector associated withh c jandb c vector covariance osets 11.3 autoencoder autoencoder ae neural network trained learn latent represen- tation good reconstructing input 4 generally autoencoder composed two parts i.e. encoder f \u0001 decoder g \u0001 illustra- tion autoencoder shown figure 11.2 encoder maps input x2rmto latent representation h2rdash=f x andf \u0001 usually one- layer neural network i.e. f x =s wx +b w2rd\u0002mandb2rd weights bias encoder \u0001 non-linear function sigmoid tanh decoder maps back latent representation hinto reconstruction ~x2rmas~x=g h andg \u0001 given asg h =s w0h+b0 w02rm\u0002dandb2rmare weights bias decoder note prime symbol indicate matrix transposition parameters autoencoder i.e. \u0012=fw b w0 b0gare optimized minimize re- construction error depending appropriate distribution assumptions input reconstruction error measured many ways widely used reconstruction error squared error l x ~x =kx\u0000~xk2 2. al- ternatively input interpreted either bit vectors vectors bit probabilities cross-entropy reconstruction used lh x ~x =\u0000dx k=1 xklog~xk+ 1\u0000xklog 1\u0000~xk 11.16 training autoencoder good reconstructing input data hope latent representation hcan capture useful features identity function seems particularly trivial function try learn doesnt result useful features therefore need add constraints autoencoder avoid trivial solution learn useful features theautoencodercanbeusedtoextractusefulfeaturesbyforcing htohave smaller dimension x i.e. m. autoencoder whose latent dimen- sion less input dimension called undercomplete autoencoder learning undercomplete representation forces autoencoder capture salient features training data 15 words latent representation distributed representation captures coordi- nates along main factors variation data 15 similar way projection principal components would capture main factors variation data indeed one linear hidden layer i.e. activation function applied mean squared error criterion used train network dhidden units learn project input span rst dprincipal components data hidden layer non-linear autoencoder behaves dierently pca ability capture multi-modal aspects input distribution anotherchoiceistoconstrain htohavealargerdimensionthan x i.e. m. autoencoder whose latent dimension larger input dimension called overcomplete autoencoder however due large dimension encoder decoder given much capacity cases even linear encoder linear decoder learn copy input output without learning anything useful data distribution fortunately still discover interesting structure imposing constraints network one widely used constraints sparsity constraint onh overcomplete autoencoder sparsity constraint called sparse autoencoder discussed next autoencoder b denoising ae figure 11.2 illustration autoencoder denoising autoencoder 11.3.1 sparse autoencoder sparse autoencoder overcomplete authoencorder tries learn sparse overcomplete codes good reconstruction 43 sparse overcomplete representation viewed alternative compressed representation implicit straightforward compressibility due large number zeros rather explicit lower dimensionality given train- ing data x2rm\u0002n objective function given min w b w0 b01 nnx i=1l xi ~xi hi 11.17 wherenis number training instances xiis thei-th training instance andhiand~xiare corresponding latent representation reconstructed features hi sparsity regularizer make hisparse scalar control sparsity many sparsityregularizers adopted one popularly used 1-norm i.e. hi =khik1=pd j=1jhi j j. however 1-norm non-smooth appropriate gradient descent alternative use smooth sparse constraint based kl-divergence let \u001aj j= 1 average activation hidden unit j averaged training set \u001aj=1 nnx i=1hi j 11.18 essential idea force \u001ajto close \u001a where\u001ais small value close zero say \u001a= 0.05 forcing \u001ajbe close \u001a would like average activation hidden neuron jto close 0.05 say constraint satised hidden unit activations mostly near 0. achieve \u001ajis close to\u001a use kl-divergence dx j=1kl \u001ajj\u001aj =dx j=1\u001alog\u001a \u001aj+ 1\u0000\u001a log1\u0000\u001a 1\u0000\u001aj 11.19 kl \u001ajj\u001aj convex function minimum \u001aj=\u001a thus minimizing penalty term eect causing \u001ajto close \u001a achieves sparse eect 1st autoencoder b 2nd autoencoder c fine-tuning figure 11.3 illustration 2-layer stacked autoencoder 11.3.2 denoising autoencoder aforementioned autoencoders add constraints latent representa- tions learn useful features alternatively denoising autoencoder uses denoising criteria learn useful features order force hidden layer discover robust features prevent simply learning identity denoising autoencoder trains autoencoder reconstruct input corrupted version 63 illustration denoising au- toencoder shown figure 11.2 b gure clean data xis rst corrupted noisy data \u0016xby means stochastic mapping qd \u0016xjx corrupted data \u0016xis used input autoencoder outputs reconstructed data ~x training objective denoising autoencoder make reconstructed data ~xclose clean data xasl x ~x many choices stochastic mapping 1 additive isotropic gaussian noise gs \u0016xjx\u0018n x \u001bi common noise model suitable real-valued inputs 2 masking noise mn fraction \u0017of elements x chosen random example forced 0 3 salt- and-pepper noise sp fraction \u0017of elements x chosen random example set minimum maximum possible value typically 0 1 according fair coin ip masking noise salt-and-pepper noise natural choices input domains interpretable binary ornearbinarysuchasblack-and-whiteimagesortherepresentationsproduced hidden layer sigmoid squashing function 63 11.3.3 stacked autoencoder denoising autoencoders stacked form deep network feeding latent representation dae found layer input current layer shown figure 11.3 generally called stacked denoising autoencoders sdaes unsupervised pre-training architectureisdoneonelayeratatime.eachlayeristrainedasadaebymin- imizing error reconstructing input example figure 11.3 wetraintherstlayerautoencoder.oncetherstlayeristrained wecantrain 2nd layer latent representation rst autoencoder i.e. h1 input shown figure 11.3 b layers pre-trained network goes second stage training called ne-tuning typically minimize prediction error supervised task ne-tuning rst add logistic regression layer top network shown fig- ure 11.3 c precisely output code output layer train entire network would train multilayer perceptron point consider encoding parts autoencoder stage supervised since use target class training 11.4 convolutional neural networks convolutional neural network cnn convnet achieved great success many computer vision tasks image classication 32 seg- mentation 36 video action recognition 55 specially designed archi- tecture cnn powerful extracting visual features images used various tasks example simplied cnn shown figure 11.4. comprised three basic types layers convolu- tional layers extracting translation-invariant features images pooling layers reducing parameters fully connected layers classication tasks cnns mainly formed stacking layers together recently dropout layers 56 residual layers 19 also introduced prevent cnn overtting ease training deep cnns respectively next introduce basic building blocks cnns cnns used feature learning figure 11.4 illustration cnn convolutional layer name implies conv layer core building block cnn essential idea conv layer ob- servation natural images property stationary means statistics one part image part example dog appear location image sug- gests dog feature detector learn one part image also applied parts image detect dogs use features locations precisely learned features small say 3x3 patches sampled randomly larger image apply learned 3x3 feature detector anywhere image specically take learned 3x3 features convolve larger image thus obtaining dierent feature activation value location inthe image feature detector called lter kernel convnet feature obtained called feature map figure 11.5 gives example convolution operation input 5x5 matrix kernel 3x3 matrix 3x3 kernel slides 5x5 matrix left right top generates feature map shown right convolution done multiplying kernel sub-patch input feature map sum together example calculation gray sub-patch 5x5 matrix kernel given gure figure 11.5 illustration convolution operation three parameters conv layer i.e. depth strideandzero-padding depthcorresponds number lters would like use conv layer many lters learning look something dierent input example rst conv layer takes input raw image dierent neurons along depth dimension may activate presence various oriented edges blobs color simple convnet shown fig- ure 11.4 depth rst convolution second convolution layers 4 6 respectively stridespecies many pixels skip slide lter input feature map stride 1 move lters one pixel time shown figure 11.5. stride 2 lters jump 2 pixels time slide around produce smaller output volumes spatially convenient pad input volume zeros around border called zero-padding size zero-padding hyperparameter nice feature zero-padding allow us control spatial size output volumes let input volume w\u0002h\u0002k wherewandhare width height feature map kis number feature maps example color image rgb channels k= 3. let receptive eld size lter size conv layer bef number lters ~k stride applied amount zero padding used border p output volume convolution ~w\u0002~h\u0002~k ~w= w\u0000f+ 2p =s+ 1and ~h= h\u0000f+ 2p =s+ 1. example 7\u00027\u00023input 4\u00023\u00023 lter stride 1 pad 0 would get 5\u00025\u00024output convolution using lters linear operation feature maps obtained conv layer nonlinear activation function applied feature maps learn non-linear features rectied linear unit relu widely used activation function convnet demonstratedfigure 11.6 illustration max pooling operation eective alleviating gradient vanishing problem rectier dened asf x max 0 x pooling layer pooling layers usually periodically inserted be- tween successive conv layers cnn aim progressively reduce spatial size representation help reduce number param- eters computation network hence also control overtting pooling layer operates independently activation map input scales dimensionality using maxfunction common form pooling layer lters size 2x2 applied stride 2 downsamples every depth slice input 2 along width height discarding 75 activations every max operation would case taking max 4 numbers maximum value 4 numbers go next layer example max pooling operation shown figure 11.6. forward pass pooling layer common keep track index max activation sometimes also called switches gradient routing ecient backpropagation though max pooling popular pooling layer cnn also contain general pooling general pooling layers comprised pooling neu- rons able perform multitude common operations including l1/l2-normalization average pooling example max pooling fully connected layer neurons fully connected layer fullconnectionstoallactivationsinthepreviouslayer asshowninfigure11.4 fully connected layers put end cnn architecture i.e. several layers conv layer max pooling layers high-level fea- tures extracted previous layers fully connected layers attempt produce class scores activations used classication output fully connected layer put softmax classi- cation also suggested relu may used activation function fully connected layer improve performance 11.4.1 transfer feature learning cnn practice training entire convolutional network scratch random initialization rare 1 time consuming requires many computation resources 2 relatively rare dataset sucient size train convnet therefore instead common topre-train convnet large dataset e.g. imagenet contains 1.2 million images 1000 categories use convnet either initialization xed feature extractor task interest 53 mainly two major transfer learning scenarios listed follows convnetasaxedfeatureextractor inthisscenario wetakeaconvnet pretrained imagenet remove last fully connected layer treat rest convnet xed feature extractor new dataset extracted features train linear classier linear svm logistic regression new dataset usually used new dataset small similar original dataset datasets training ne-tuning convnet practical convnets prone overtting small datasets since new dataset similar original dataset expect higher-level features convnet relevant dataset well fine-tuning convnet second way replace retrain classier top convnet new dataset also ne-tune weights pretrained network using backpropa- gation essential idea ne-tuning earlier features convnetcontainmoregenericfeatures e.g. edgedetectorsorcolorblob detectors useful many tasks later layers convnet become progressively specic details classes contained original dataset new dataset large enough ne-tune layers convnet new dataset small dierent original dataset keep earlier layers xed due overtting concerns ne-tune higher-level portion network 11.5 word embedding recurrent neural networks word embedding recurrent neural networks state-of-the-art deep learning models natural language processing tasks word embedding learns word representation recurrent neural networks utilize word embed- ding sentence document feature learning next introduce details word embedding recurrent neural networks 11.5.1 word embedding word embedding distributed representation words represents word low-dimensional dense vector vector represen- tation words capture synthetic semantic meanings words thelow-dimensional representation also alleviate curse dimensional- ity data sparsity problems suered traditional representations bag-of-words n-gram 66 essential idea word embedding distributional hypothesis shall know word company keeps 13 suggests word close relationships neigh- boring words example phrases win game andwin lottery appear frequently thus pair words winandgameand pair words winandlotterycould close relationship given word win would highly expect neighboring words words like gameorlotteryinstead words lightorair suggests good word representation useful predicting neighboring words essential idea skip-gram 41 words train- ing objective skip-gram model nd word representations useful predicting surrounding words sentence document formally given sequence training words w1 w2 wt objective skip-gram model maximize average log probability 1 ttx t=1x \u0000c\u0014j\u0014c j6=0logp wt+jjwt 11.20 wherecis size training context function center word wt largercresults training examples thus lead higher accuracy expense training time basic skip-gram formulation denes p wt+jjwt using softmax function p wojwi =exp ut wovwi pw w=1exp uwtvwi 11.21 vwanduware input output representations w andwis number words vocabulary learning representation usually done gradient descent however eq 11.21 impractical cost computingrlogp wojwi proportional w often large one way making computation tractable replace softmax eq 11.21 hierarchical softmax hierarchical softmax vocab- ulary represented human binary tree words leaves human tree probability p wojwi probability walking path root node leaf node wogiven word wi calculated decision making node along path simple function human trees assign short binary codes frequent words reduces number output units need evaluated another alternative make computation tractable negative sampling 41 essential idea negative sampling wtshould similar neighboring words say wt+j randomly sampled words thus objective func- tion negative sampling maximize similarity wtandwt+j minimize similarity wtand randomly sampled words negative sampling eq 11.21 approximated log\u001b ut wovwi +1 kkx i=1log\u001b \u0000ut wivwi 11.22 wherekis number negative words sampled input word wi found skip-gram negative sampling equivalent implicitly factorizing word-context matrix whose cells pointwise mutual infor- mation pmi respective word context pairs shifted global constant 34 insteadofusingthecenterwordstopredictthecontext surroundingwords sentence continuous bag-of-words model cbow predicts current word based context precisely cbow uses current word input log-linear classier continuous projection layer predicts words within certain range current word 39 objective function cbow maximize following log-likelihood function 1 ttx t=1logp wtjwt\u0000c wt\u00001 wt+1 wt+c 11.23 andp wtjwt\u0000c wt\u00001 wt+1 wt+c dened p wtjwt\u0000c wt\u00001 wt+1 wt+c =exp uwtt~vt pw w=1exp uwt~vt 11.24 ~vtis average representation contexts wt i.e. ~vt= 1 2cp \u0000c\u0014j\u0014c j6=0vt+j methodslikeskip-grammaydobetterontheanalogytask buttheypoorly utilize statistics corpus since train separate local context windows instead global co-occurrence counts based observation glove proposed 46 uses specic weighted least squares model trains global word-word co-occurrence counts thus makes ecient use statistics objective function glove given minx jf xij wt i~wj\u0000logxij 2 11.25 wherexijtabulates number times word joccurs context word i.wi2rdis word representation wiand~wjis separate context word vector.f weighting function word embedding capture syntactic semantic meanings words example found vec queen closest vector representation vec king vec man vec woman implies word representation learned skip-gram encodes semantic meanings words word embedding also used document representation averaging word vectors words appearing document vector representation documents.following distributional representation idea word embedding many network embedding algorithms proposed essential idea network embedding learn vector representations network nodes good predicting neighboring nodes since word representation learned word embedding algorithms low- dimensional dense vectors capture semantic meanings widely used preprocessing step deep learning methods recurrent neural networks recursive neural networks word mapped vector representation used input deep learning models 11.5.2 recurrent neural networks figure 11.7 illustration rnn recurrent neural networks rnn powerful concepts allow use loops within neural network architecture model sequential data sentences videos recurrent networks take input sequence inputs produce sequence outputs thus models particularly useful sequence-to-sequence learning figure 11.7 gives illustration rnn architecture left part gure shows folded rnn self-loop i.e. hidden state h used update given input x. better show rnn works unfold rnn sequential structure given right part figure 11.7. rnn takes sequence x1 x2 xt xtas input step xtis ad-dimensional feature vector example input sentence word wiof sentence represented vector xiusing word embedding methods skip-gram time-step output previous step ht\u00001 along next word vector document xt used update hidden state htas ht=\u001b whhht\u00001+whxxt 11.26 whh2rd\u0002dandwhx2rd\u0002dare weights inputs ht\u00001and xt respectively hidden states htis feature representation se- quence time tfor input sequence initial states h0are usually initialized 0. thus utilize htto perform various tasks sentence completion document classication example sentence completion task given partial sentence weather one gru cell b one lstm cell figure 11.8 illustration gru lstm cells want predict next word predict next word yt=softmax wh t+b yt= arg max yt 11.27 w2rv\u0002dare weights softmax function vbeing size vocabulary bthe bias term ytis predicted probability vector andytis predicted label think rnn modeling likelihood probability p ytjx1 xt training rnns usually done using backpropagation time bptt back propagates error time tto time 1 70 11.5.3 gated recurrent unit though theory rnn able capture long-term dependency practice old memory fade away sequence becomes longer make easier rnns capture long-term dependencies gated recurrent units grus 7 designed persistent memory unlike rnn uses simple ane transformation ht\u00001andhtfollowed tanhto update ht gru introduces reset gate determine wants forget past memory update gate control new inputs introduced ht mathematical equations achieved given follows illustration gru cell shown figure 11.8 zt=\u001b wzxxt+wzhht\u00001+bz update gate rt=\u001b wrxxt+wrhht\u00001+br reset gate ~ht= tanh rt uht\u00001+wxt new memory ht= 1\u0000zt ~ht+zt ht\u00001 hidden state 11.28 equation figure 11.8 treat gru four fundamental operational stages i.e. new memory update gate reset gate hidden state new memory ~htis consolidation new input word xt past hidden state ht\u00001 summarizes new word light contextual past reset signal rtis used determining importantht\u00001is summarization ~ht reset gate ability completely diminish past hidden state nds ht\u00001is irrelevant computa- tion new memory update signal ztis responsible determining much past state ht\u00001should carried forward next state instance zt\u00191 ht\u00001is almost entirely copied ht hid- den state htis nally generated using past hidden input htand new memory generated ~htwith control update gate 11.5.4 long short-term memory long short-term memories lstms 23 another variant rnn also capture long-term dependency similar grus lstm introduces complex gates control accept new information forget previous memory i.e. input gate forget gate output gate new memory cell update rules lstms given follows it=\u001b wixxt+wihht\u00001+bi input gate ft=\u001b wfxxt+wfhht\u00001+bf forget gate ot=\u001b woxxt+wohht\u00001+bo output gate gt= tanh wgxxt+wghht\u00001+bg new memory cell ct=ft ct\u00001+it gt final memory cell ht=ot tanh ct 11.29 itis input gate ftis forget gate otis forget fate ctis memory cell state tandxtis input features t. \u001b \u0001 means sigmoid function denotes hadamard product main idea lstm model memory cell ct records history inputs observed tot.ctis summation 1 previous memory cell ct\u00001modulated sigmoid gate ft 2 gt function previous hidden states current input modulated another sigmoid gate sigmoid gate ftis selectively forget previous memory itis selectively accept current input itis gate controlling output illustration cell lstm time step tis shown figure 11.8 b 11.6 generative adversarial networks variational autoencoder section introduce two popular deep generative models proposed recently i.e. generative adversarial networks variational autoencoder 11.6.1 generative adversarial networks figure 11.9 illustration framework gan generative adversarial network gan 16 one popu- lar generative deep models core gan play min-max game discriminator dand generator g i.e. adversarial training discriminator dtries dierentiate sample real world generated generator generator gtries generate samples fool discriminator i.e. make discriminator believe generated samples real world figure 11.9 gives illustration framework gan generator takes noise zsampled prior distribution pz z input maps noise data space g z \u0012g typical choices prior p z uniform distribution gaussian distribution also dene second multilayer perceptron x \u0012d outputs single scalar x represents probability xcame real-world data rather generated data dis trained maximize probability assigning correct label training examples sam- ples g. simultaneously train gto minimize log 1\u0000d g z words dandgplay following two-player minimax game value functionv g min gmax dv g ex\u0018pdata x logd x +ez\u0018p z z log 1\u0000d g z 11.30 training gan done using minibatch stochastic gradient de- scent training updating parameters ganddalternatively model trained without supervision treat discriminator feature extractor rst layers dextract features xwhile last layers map features probability xis real data thus remove last layers output dis features extracted sense treat gans unsupervised feature learning algorithms though main purpose gan learn p x gan general adversarial training framework used various domains designing dierent generator discriminator loss function 6,65,74 example infogan 6 learns disentangled representa-tion dividing noise two parts i.e. disentangled codes cand incom- pressible noise zso disentangled codes ccan control properties identity illumination images generated seqgan 74 models data generator stochastic policy reinforcement learning extends gan text generation 11.6.2 variational autoencoder figure 11.10 illustration framework vae variational autoencoder vae 30 popular generative model unsupervised representation learning trained purely gradient- based methods typically vae standard autoencoder component encodes input data latent code space minimizing recon- struction error bayesian regularization latent space forces posterior hidden code vector match prior distribution figure 11.10 gives illustration vae generate sample model vae rst draws sample zfrom prior distribution p\u0012 z sample zis used input dierentiable generator network g z finally x sampled distribution p\u0012 xjg z =p\u0012 xjz training approximate inference network i.e. encoder network q zjx used obtain zandp\u0012 xjz viewed decoder network core idea variational autoenoder trained maximizing variational lower boundl \u0012 x l \u0012 x =\u0000dkl q zjx jjp\u0012 z +eq zjx logp\u0012 xjz \u0014logp\u0012 x 11.31 wheredkl q zjx jjp\u0012 z isthekldivergencewhichmeasuresthesimilarity two distributions q zjx andp\u0012 z and\u0012are variational parameters andgenerativeparameters respectively.wewanttodierentiateandoptimize lower bound w.r.t and\u0012 however directly using gradient es- timator objective function exhibit high variance therefore vae adopts reparametric trick certain mild conditions chosen approximate posterior q zjx random variable ~z\u0018q zjx parameterized ~z=g \u000f x with\u000f\u0018p \u000f 11.32 whereg \u000f x dierentiable transformation function noise variable \u000f nonparametric trick also shown figure 11.10. technique variational lower bound eq 11.31 approximated la \u0012 x =1 llx l=1logp\u0012 x z l \u0000logq z l jx ~z l =g \u000f l x with\u000f l \u0018p \u000f 11.33 parameters learned via stochastic gradient descent eciently easy see encoder feature extractor learns latent representations x 11.7 discussion readings introduced representative deep learning models feature engi- neering section well discuss used hierarchical representation learning disentangled representation used popular domains text image graph table 11.1 hierarchical disentangled representation learning method hierarchical fea rep. disentangled fea rep. rbm/dbm 59 48 dbn 33 n/a ae/dae/sdae 37 28 rnn/gru/lstm 68,73 11,54 cnn 12,14 25,49 gan 47 6,38 vae 75 54,72 hierarchical representation generally hierarchical representation lets us learn features hierarchy combine top-down bottom-up process- ing image text instance lower layers could support object detection spotting low-level features indicative object parts conversely information objects higher layers could resolve lower-level am- biguities image infer locations hidden object parts features dierent hierarchy levels may good dierent tasks high-level features captures main objects resolve ambiguities thus good classication mid-level features include many details may good segmentation hierarchical feature representation common deep learning models list representative literature introduced model used hierarchical feature learning table 11.1.disentangled representation disentangled representation popular way learn explainable representations majority existing representa- tion learning frameworks learn representation h2rd\u00021 dicult explain i.e. d-latentdimensionsareentangledtogetherandwedontknow semantic meaning d-dimensions instead disentangled representa- tion learning tries disentangle latent factors know semantic meaning latent dimensions example handwritten digits mnist dataset may want disentangle digit shape writ- ing style part hcontrols digit shapes part represents writing style disentangled representation explains latent representation also helps generate controlled realistic images example changing part codes controls digit shape gen- erate new images target digit shapes using generator new latent representation therefore disentangled representation learning attracting increasing attention table 11.1 also lists representative deep learning methods disentangled representation learning still relatively new direction needs investigation table 11.2 deep learning methods dierent domains method text image audio linked data graph rbm/dbm 57,58 57 9 67 dbn 52 33 42 n/a ae/dae/sdae 3 63 44 64 cnn 29 19,45,71 1 10 word2vec 35,41 n/a n/a 61,66 rnn/gru/lstm 27,40,60 2 17,50 n/a gan 74 6,47 74 n/a vae 5,26 18,30 24 n/a deep feature representation various domains many deep learning algorithms initially developed specic domains exam- ple cnn initially developed image processing word2vec initially proposed learning word representation due great success methods developed applicable domains forexample inadditiontoimages thecnnhasalsobeensuccessfullyapplied texts audio linked data domain unique property example text data inherently sequential graph data non-i.i.d thus directly applying cnn impractical new architectures proposed adapt cnn domains holds deep learn- ing algorithms therefore summarize application discussed deep learning models four domains table 11.2. encourage interested users read papers understanding.combining deep learning models introduced various deep learning models applied dierent domains example lstm mainly used dealing sequential data texts cnn powerful images common need work tasks related dierent domains cases combine dierent deep learning models propose new framework applied task hand example information retrieval task given text query want retrieve images match query need use lstm word2vec learn representation captures semantic meanings query time need use cnn learn features describe image train lstm cnn similarity representations matched query-image pairs maximized similarity representations non-matched query-image pairs minimized another example video action recognition want classify action video since video composed frames nearby frames dependency video inherently sequential data lstm good modeling data however lstm good extracting images therefore rst need use cnn extract features frame video used input lstm learning representation video 68 similarly image captioning use cnn extract features use lstm generate image captions based image features 71 list examples many examples general treat deep learning algorithms feature extracting tools extract features certain domains design loss function top deep learning algorithms problemwewanttostudy.onethingtonoteisthatwhenwecombinedierent models together trained end-to-end words dont train models separately instead treat new model unied model usually gives better performance training model separately combining bibliography 1 ossama abdel-hamid abdel-rahman mohamed hui jiang li deng gerald penn dong yu convolutional neural networks speech recognition ieee/acm transactions audio speech language processing 22 10 :15331545 2014 2 stanislaw antol aishwarya agrawal jiasen lu margaret mitchell dhruv batra c. lawrence zitnick devi parikh vqa visual ques- tion answering cvpr pages 24252433 2015 3 sarath chandar ap stanislas lauly hugo larochelle mitesh khapra balaraman ravindran vikas c raykar amrita saha autoen- coderapproachtolearningbilingualwordrepresentations nips pages 18531861 2014 4 yoshua bengio et al learning deep architectures ai foundations trends machine learning 2 1 :1127 2009 5 samuel r bowman luke vilnis oriol vinyals andrew dai rafal jozefowicz samy bengio generating sentences continuous space.conll 2016 6 xi chen yan duan rein houthooft john schulman ilya sutskever pieter abbeel infogan interpretable representation learning informa- tion maximizing generative adversarial nets nips pages 21722180 2016 7 kyunghyun cho bart van merrinboer caglar gulcehre dzmitry bah- danau fethi bougares holger schwenk yoshua bengio learning phraserepresentationsusingrnnencoder-decoderforstatisticalmachine translation arxiv preprint arxiv:1406.1078 2014 8 aaron courville james bergstra yoshua bengio spike slab restricted boltzmann machine aistas pages 233241 2011 9 george dahl abdel-rahman mohamed georey e hinton et al phone recognition mean-covariance restricted boltzmann machine nips pages 469477 2010 10 michal deerrard xavier bresson pierre vandergheynst convo- lutional neural networks graphs fast localized spectral ltering innips pages 38443852 2016 11 emily denton vighnesh birodkar unsupervised learning disen- tangled representations video pages 44174426 nips 2017 12 clement farabet camille couprie laurent najman yann le- cun learning hierarchical features scene labeling ieee tpami 35 8 :19151929 2013 13 john r firth synopsis linguistic theory 1930-1955 1957 14 ross girshick je donahue trevor darrell jitendra malik rich feature hierarchies accurate object detection semantic segmenta- tion incvpr pages 580587 2014 15 ian goodfellow yoshua bengio aaron courville deep learning mit press 2016 16 ian goodfellow jean pouget-abadie mehdi mirza bing xu david warde-farley sherjil ozair aaron courville yoshua bengio gen- erative adversarial nets nips pages 26722680 2014 17 alex graves abdel-rahman mohamed georey hinton speech recognitionwithdeeprecurrentneuralnetworks icassp pages6645 6649. ieee 2013 18 karol gregor ivo danihelka alex graves danilo jimenez rezende daan wierstra draw recurrent neural network image generation arxiv preprint arxiv:1502.04623 2015 19 kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition cvpr pages 770778 2016 20 georey hinton li deng dong yu george e dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara n sainath et al deep neural networks acoustic model- ing speech recognition shared views four research groups ieee signal processing magazine 29 6 :8297 2012 21 georey e hinton deep belief networks scholarpedia 4 5 :5947 2009 22 georey e hinton et al modeling pixel means covariances using factorized third-order boltzmann machines cvpr pages 25512558. ieee 2010 23 sepp hochreiter jrgen schmidhuber long short-term memory neural computation 9 8 :17351780 1997 24 wei-ning hsu yu zhang james r. glass learning latent rep- resentations speech generation transformation annual confer- ence international speech communication association inter- speech pages 12731277 2017 25 wei-ning hsu yu zhang james r. glass unsupervised learning disentangled interpretable representations sequential data nips 2017 26 zhiting hu zichao yang xiaodan liang ruslan salakhutdinov eric p xing toward controllable text generation icml 2017 27 ozan irsoy claire cardie opinion mining deep recurrent neural networks emnlp pages 720728 2014 28 michael janner jiajun wu tejas kulkarn ilker yildirim josh tenenbaum learning generalize intrinsic images structured disentangling autoencoder nips 2017 29 nal kalchbrenner edward grefenstette phil blunsom convolu- tional neural network modelling sentences acl 2014 30 diederik p kingma max welling auto-encoding variational bayes arxiv preprint arxiv:1312.6114 2013 31 alex krizhevsky georey e hinton et al factored 3-way restricted boltzmann machines modeling natural images aistats pages 621628 2010 32 alex krizhevsky ilya sutskever georey e hinton imagenet clas- sication deep convolutional neural networks nips pages 1097 1105 2012 33 honglaklee rogergrosse rajeshranganath andandrewyng unsu- pervised learning hierarchical representations convolutional deep belief networks communications acm 54 10 :95103 2011 34 omer levy yoav goldberg neural word embedding implicit matrix factorization nips pages 21772185 2014 35 yang li quan pan tao yang suhang wang jiliang tang erik cambria learningwordrepresentationsforsentimentanalysis cognitive computation 2017 36 jonathan long evan shelhamer trevor darrell fully convolutional networks semantic segmentation cvpr pages 34313440 2015 37 jonathan masci ueli meier dan cirean jrgen schmidhuber stacked convolutional auto-encoders hierarchical feature extraction articial neural networks machine learningicann 2011 pages 5259 2011 38 michal mathieu junbo jake zhao pablo sprechmann aditya ramesh yann lecun disentangling factors variation deep representa- tion using adversarial training nips pages 50415049 2016 39 tomas mikolov kai chen greg corrado jerey dean e- cient estimation word representations vector space arxiv preprint arxiv:1301.3781 2013 40 tomas mikolov martin karat luks burget jan cernock san- jeev khudanpur recurrent neural network based language model interspeech pages 10451048 2010 41 tomasmikolov ilyasutskever kaichen gregscorrado andjedean distributed representations words phrases composition- ality innips pages 31113119 2013 42 abdel-rahmanmohamed georgedahl andgeoreyhinton deepbelief networks phone recognition nips workshop deep learning speech recognition related applications 2009 43 andrew ng sparse autoencoder cs294a lecture notes 72 2011 :119 2011 44 jiquan ngiam aditya khosla mingyu kim juhan nam honglak lee andrew ng multimodal deep learning icml pages 689696 2011 45 maxime oquab leon bottou ivan laptev josef sivic learning transferring mid-level image representations using convolutional neural networks cvpr pages 17171724 2014 46 jerey pennington richard socher christopher manning glove global vectors word representation emnlp volume 14 pages 15321543 2014 47 alec radford luke metz soumith chintala unsupervised represen- tation learning deep convolutional generative adversarial networks arxiv preprint arxiv:1511.06434 2015 48 scott reed kihyuk sohn yuting zhang honglak lee learning disentangle factors variation manifold interaction icml pages 14311439 2014 49 salah rifai yoshua bengio aaron courville pascal vincent mehdi mirza disentangling factors variation facial expression recognition eccv pages 808822 2012 50 haim sak andrew senior franoise beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling fifteenth annual conference international speech communication association 2014 51 ruslan salakhutdinov georey hinton deep boltzmann machines inarticial intelligence statistics pages 448455 2009 52 ruhi sarikaya georey e. hinton anoop deoras application deep belief networks natural language understanding ieee/acm trans audio speech language processing 22 4 :778784 2014 53 ali sharif razavian hossein azizpour josephine sullivan stefan carlsson cnn features o-the-shelf astounding baseline recogni- tion incvpr workshops pages 806813 2014 54 n. siddharth brooks paige jan-willem van de meent alban desmai- son frank wood noah d. goodman pushmeet kohli philip h. s. torr learning disentangled representations semi-supervised deep generative models nips 2017 55 karen simonyan andrew zisserman two-stream convolutional net- works action recognition videos nips pages 568576 2014 56 nitish srivastava georey e hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neu- ral networks overtting journal machine learning research 15 1 :19291958 2014 57 nitish srivastava ruslan r salakhutdinov multimodal learning deep boltzmann machines nips pages 22222230 2012 58 nitish srivastava ruslan r salakhutdinov georey e hinton mod- eling documents deep boltzmann machines uai 2013 59 heung-il suk seong-whan lee dinggang shen alzheimers dis- ease neuroimaging initiative et al hierarchical feature representation multimodal fusion deep learning ad/mci diagnosis neu- roimage 101:569582 2014 60 ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks nips pages 31043112 2014 61 jiantang mengqu mingzhewang mingzhang junyan andqiaozhu mei line large-scale information network embedding www pages 10671077 2015 62 tijmen tieleman training restricted boltzmann machines using approx- imations likelihood gradient icml pages 10641071. acm 2008 63 pascal vincent hugo larochelle isabelle lajoie yoshua bengio pierre-antoine manzagol stacked denoising autoencoders learning use- ful representations deep network local denoising criterion journal machine learning research 11 dec :33713408 2010 64 daixin wang peng cui wenwu zhu structural deep network em- bedding sigkdd pages 12251234. acm 2016 65 jun wang lantao yu weinan zhang yu gong yinghui xu benyou wang peng zhang dell zhang irgan minimax game unifying generative discriminative information retrieval models sigir 2017 66 suhang wang jiliang tang charu aggarwal huan liu linked document embedding classication cikm pages 115124. acm 2016 67 suhang wang jiliang tang fred morstatter huan liu paired restricted boltzmann machine linked data cikm pages 1753 1762. acm 2016 68 yilin wang suhang wang jiliang tang neil ohare yi chang baoxinli hierarchicalattentionnetworkforactionrecognitioninvideos corr abs/1607.06416 2016 69 max welling michal rosen-zvi georey e hinton exponential familyharmoniumswithanapplicationtoinformationretrieval nips volume 4 pages 14811488 2004 70 paul j werbos backpropagation time proceedings ieee 78 10 :15501560 1990 71 kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel yoshua bengio show attend tell neural image caption generation visual attention icml pages 20482057 2015 72 xinchen yan jimei yang kihyuk sohn honglak lee at- tribute2image conditional image generation visual attributes eccv pages 776791. springer 2016 73 zichao yang diyi yang chris dyer xiaodong alexander j smola eduard h hovy hierarchical attention networks document clas- sication hlt-naacl pages 14801489 2016 74 lantao yu weinan zhang jun wang yong yu seqgan sequence generative adversarial nets policy gradient aaai pages 2852 2858 2017 75 shengjia zhao jiaming song stefano ermon learning hierarchical features deep generative models icml pages 40914099 2017.neural networks deep learning chapter excerpted artificial intelligence introduction machine learning second edition richard e. neapolitan xia jiang 2018 taylor francis group rights reserved.6 learn moreneural networks deep learning previous three parts modeled intelligence either human cognitive level population-based level intelligence removed physiological processes involved intelligent reasoning part model neuronal processes involved brain `` intelligently '' controlling thoughts behavior life form networks construct fashion called arti cial neural networks neural networks used e ectively applications image recognition speech recognition hard model structured approach used rule-based systems bayesian networks case image recognition example learn identify images cars presented images labeled `` car '' `` car '' start modeling single neuron 15.1 perceptron figure 15.1 shows biological neuron dendrites transmit signals cell body cell body processes signal axon sends signals otherfigure 15.1 neuron art cial neuron b neurons input signals accumulated cell body neuron accu- mulated signal exceeds certain threshold output signal generated passed axon figure 15.1 b show arti cial neuron mimics process arti cial neuron takes input vector x1 x2 xk applies weights w0 w1 w2 wk input yielding weighted sum w0+kx i=1wixi next neuron applies activation function fto sum outputs value yof f. note inputs xiare square nodes distinguish arti cial neuron computational unit aneural network consists one many arti cial neurons communicate output one neuron input another neuron simplest neural network perceptron consists single arti cial neuron shown figure 15.1 b activation function perceptron follows f z =\u001a1 ifz 0 \u00001 otherwise therefore complete expression output yof perceptron follows y=\u001a1 ifw0+pk i=1wixi 0 \u00001 otherwise 15.1 perceptron binary classi er returns 1 activation function exceeds 0 otherwise returns -1.15.1 perceptron figure 15.2 line\u00003\u00004x1+ 5x 2= 0 line linearly separates data b approximately linearly separates data c approximately linearly separate data let 's look case input two-dimensional vector x1 x2 suppose weighted sum perceptron follows w0+w1x1+w2x2=\u00003\u00004x1+ 5x 2 15.2 figure 15.2 plots line \u00003\u00004x1+ 5x 2= 0 color 2-dimensional points gray black set gray points linearly separable set black points exists least one line plane gray points one side line black points side de nition extends readily higher-dimensional data points figure 15.2 b linearly separable line \u00003\u00004x1+ 5x 2= 0. perceptron weights equality 15.2 maps gray points y= 1 black points y=\u00001 perceptron perfect binary classi er data data figure 15.2 c perceptron pretty good classi er misclassi es two cases poor classi er data figure 15.2 gray black points gure approximately linearly separable perceptron would good classi er data perceptron linear binary classi er uses linear function classify instance 15.1.1 learning weights perceptron learning perceptron binary classi cation goal determine weights deter- mining line close possible linearly separates two classes next develop agradient descent algorithm learning weights see section 5.3.2 introduction gradient descent data item x1 x2 xk loss function loss ^y ^y\u0000y w0+kx i=1wixi ^yis estimate yusing equation 15.1. idea behind loss function ^y=ythere loss y= 1 andy=\u00001 thenw0+pk i=1wixi 1 loss 2\u0010 w0+pk i=1wixi\u0011 loss measure far obtaining value 0 would given correct answer similarly y=\u00001 andy= 1 w0+pk i=1wixi 1 loss 2\u0010 w0+pk i=1wixi\u0011 cost function follows cost y1 ^y1 yn ^yn =nx j=1loss yj ^yj =nx j=1 ^yj\u0000yj w0+kx i=1wixj 15.3 note thatxj idenotes ith vector element jth data item di erent notation used section 5.3. partial derivatives cost function follows \u0010pn j=1 ^yj\u0000yj w0+pk i=1wixj \u0011 w0=nx j=1 ^yj\u0000yj pn j=1\u0010 ^yj\u0000yj w0+pk i=1wixj \u0011 wm=nx j=1 ^yj\u0000yj xj rosenblatt 1958 developed perceptron algorithm presenting updated based item sequence stochastic gradient descent section 5.3.4 show version algorithm next algorithm 15.1 gradient descent perceptron input set real predictor data binary outcome data f x1 1x1 2 x1 k y1 x2 1x2 2 x2 k y2 xn 1xn 2 xn k yn g output weightsw0 w1 wkthat minimize cost function equality 15.3. functionminimizing values fori= 0 tok wi=arbitraryvalue endfor \u0015=learningrate repeatnumberiterations times forj= 1 ton y=\b1 ifw0+pk i=1wixj 0 \u00001 otherwise w0=w0\u0000\u0015 y\u0000yj form= 1 tok wm=wm\u0000\u0015 y\u0000yj xj endfor endfor endrepeatexample 15.1 suppose set \u0015= 0 1 following data x1x2y 1 2 1 3 4-1 algorithm 2 iterations repeat loop follows // initialize weights arbitrary values w0= 1 w1= 1 w 2= 1 // first iteration repeat loop //j= 1 for\u0000j loop w0+w1x1 1+w2x1 2= 1 1 1 1 2 4 0 y= 1 w0=w0\u0000\u0015 y\u0000y1 1\u0000 0:1 1\u00001 1 w1=w1\u0000\u0015 y\u0000y1 x1 1= 1\u0000 0:1 1\u00001 1 1 w2=w2\u0000\u0015 y\u0000y1 x1 2= 1\u0000 0:1 1\u00001 2 1 //j= 2 for\u0000j loop w0+w1x2 1+w2x2 2= 1 1 3 1 4 8 0 y= 1 w0=w0\u0000\u0015 y\u0000y1 1\u0000 0:1 1\u0000 \u00001 0:8 w1=w1\u0000\u0015 y\u0000y1 x1 1= 1\u0000 0:1 1\u0000 \u00001 3 0:4 w2=w2\u0000\u0015 y\u0000y1 x1 2= 1\u0000 0:1 1\u0000 \u00001 4 0:2 // second iteration repeat loop //j= 1 for\u0000j loop w0+w1x1 1+w2x1 2= 0:8 0:4 1 0:2 2 1 :6 0 y= 1 w0=w0\u0000\u0015 y\u0000y1 0:8\u0000 0:1 1\u00001 0:8 w1=w1\u0000\u0015 y\u0000y1 x1 1= 0:4\u0000 0:1 1\u00001 1 0:4 w2=w2\u0000\u0015 y\u0000y1 x1 2= 0:2\u0000 0:1 1\u00001 2 0:2 //j= 2 for\u0000j loop w0+w1x2 1+w2x2 2= 0:8 0:4 3 0:2 4 2 :8 0 y= 1 w0=w0\u0000\u0015 y\u0000y1 0:8\u0000 0:1 1\u0000 \u00001 0:6 w1=w1\u0000\u0015 y\u0000y1 x1 1= 0:4\u0000 0:1 1\u0000 \u00001 3 =\u00000:2 w2=w2\u0000\u0015 y\u0000y1 x1 2= 0:2\u0000 0:1 1\u0000 \u00001 4 =\u00000:6 table 15.1 sat scores parental income graduation status 12 students sat 100 income 10,000 graduate 4 18 6 7 8 4 10 6 12 2 10 10 6 6 yes 7 20 yes 8 16 yes 12 16 yes 14 7 yes 16 4 yes 15.1.2 perceptron logistic regression perceptron similar logistic regression see section 5.3.3 map continuous predictors binary outcome di erence perceptron determin- istically reports y= 1 ory=\u00001 logistic regression reports probability y= 1. recall logistic regression computes probability follows p y= 1jx =exp b 0+pk i=1bixi 1 exp b 0+pk i=1bixi p y=\u00001jx =1 1 exp pk i=1bixi use logistic regression equation binary classi er say y= 1 ifp y= 1 p y=\u00001 following sequence steps shows linear binary classi er p y= 1jx p y=\u00001jx exp b 0+pk i=1bixi 1 exp b 0+pk i=1bixi =1 1 exp pk i=1bixi exp\u0012 b0+xk i=1bixi\u0013 1 ln\u0012 exp b 0+xk i=1bixi \u0013 0 b0+xk i=1bixi= 0 sety= 1 b0+pk i=1bixi 0 example 15.2 suppose suspect sat scores parental income ect whether student graduates college obtain data table 15.1. data plotted figure 15.3 learn logistic regression model data using algorithm one outlined section 5.3.3 obtain p graduate =yesjsat income =exp \u00006:24 0:439sat 0:222income 1 exp \u00006:24 0:439sat 0:222income 15.2 feedforward neural networks figure 15.3 plot shows individuals graduated college gray points individuals graduate college black points plot b shows individuals includes line 6 :14 0:439sat 0:222income line obtain linear classi er \u00006:24 0:439sat 0:222income 0 line plotted data figure 15.3 b note line perfectly linearly separate data two points misclassi ed data linearly separable left exercise implement algorithm 15.1 apply data table 15.1 compare results obtained logistic regression 15.2 feedforward neural networks want classify objects figure 15.2 need go beyond simple per- ceptron next introduce complex networks classify objects linearly separable start simple example xor function 15.2.1 modeling xor domain xor function isf 0 0 0 1 1 0 1 1 g. xor mapping follows xor 0 0 0 xor 0 1 1 xor 1 0 1 xor 1 1 0 figure 15.4 plots domain xor function shows points mapping 0 black points points mapping 1 gray points clearly black gray points linearly separable perceptron could model xor function however complex network figure 15.5 model network 2-layer neural network two layers arti cial neurons rst layer containing nodesh1andh2 called hidden layer represents neither input output second layer contains single output node perceptron layer notefigure 15.4 xor function black points map 0 gray points map 1. figure 15.5 neural network modeling xor function.figure 15.6 original x-space transformed h-space b activation function hidden node h2in figure 15.5 max 0 z function called recti ed linear activation function let 's show network figure 15.5 indeed model xor function x1 x2 0 0 h1= 0 0 0 h2= max 0 \u00001 0 0 0 y= 0\u00002 0 0 x1 x2 0 1 h1= 0 1 1 h2= max 0 \u00001 0 1 0 y= 1\u00002 0 1 x1 x2 1 0 h1= 1 0 1 h2= max 0 \u00001 1 0 0 y= 1\u00002 0 1 x1 x2 1 1 h1= 1 1 2 h2= max 0 \u00001 1 1 1 y= 2\u00002 1 0 `` trick '' network h1andh2together map 0,0 0,0 0,1 1,0 1,0 1,0 1,1 2,1 black points 0,0 2,1 single gray point 1,0 data linearly separable figure 15.6 shows transformation thex-space h-space.figure 15.7 classc1consists shaded area class c2consists white area notation 100 example means region lies plus side line h11 minus side line h12 minus side line h13 15.2.2 example two hidden layers suppose classifying points plane points grey area figure 15.7 class c1 points white area class c2 di\u000ecult classi cation problem regions comprise class c1are even adjacent neural network figure 15.8 two hidden layers correctly accomplishes classi cation appropriate weights activation functions next show done linesh1 h2 andh3in figure 15.7 separate plane 7 regions notation +=\u0000 figure 15.7 indicates region side given line \u0000side assign region value 1 side line value 0 \u0000side region 100 therefore labeled side line h11 the\u0000side line h12 the\u0000side line h13 regions labeled similar fashion create hidden node h11with weights representing line h11 use activation function returns 0 x1 x2 the\u0000side line h11and 1 otherwise create hidden nodes h12andh13in similar fashion table 15.2 shows values output nodes h11 h12 andh13when x 1 x2 resides 7 regions figure 15.7 note 110 determine region regions map 7 corners unit cube 3-space two points represent class c1 0,0,0 1,1,1 linearly separate point 0,0,0 6 points plane 3-space create hidden node h21that use activation function returns 1 point 0,0,0 0 points cube similarly create hidden node h22that outputs 1 point 1,1,1 0 points cube table 15.2 shows values output two hidden nodes 1,0 figure 15.8 neural network correctly classi es points classes c1andc2in figure 15.7. table 15.2 values nodes neural network figure 15.7 input tuple located regions figure 15.6 region classh11h12h13h21h22y 000c1 0 0 0 1 0 1 001c2 0 0 1 0 0 0 010c2 0 1 0 0 0 0 011c2 0 1 1 0 0 0 100c2 1 0 0 0 0 0 101c2 1 0 1 0 0 0 110\u0000\u0000\u0000\u0000\u0000\u0000\u0000 111c1 1 1 1 0 1 1figure 15.9 points region 000 map 0,1 points region 111 map 1,0 points map 0,0 x 1 x2 region 000 0,1 x 1 x2 region 111. 0,0 x 1 x2 region three points shown figure 15.9. next create weights output node ythat yield line separates 1,0 0,1 0,0 line shown figure 15.9. use activation function returns 1 point lies line 0 otherwise way values x1 x2 classc1map 1 values x1 x2 classc2map 0. example 15.3 suppose three lines determining regions figure 15.7 follows h11 2\u0000x1\u0000x2= 0 h12 0:5\u0000x2= 0 h13 x1\u0000x2= 0 three lines plotted figure 15.10. given lines activation functions hidden nodes h11 h12 andh13are follows h11=\u001a1 2\u0000x1\u0000x2 0 0 otherwise h12=\u001a1 0:5\u0000x2 0 0 otherwise h12=\u001a1 ifx1\u0000x2 0 0 otherwise hidden node h21must provide plane linearly separates 0 0 0 points unit cube following one plane h11+h12+h13= 0:5:0.00.51.01.52.00.00.51.01.52.0 x1x2figure 15.10 three lines determining regions figure 15.7 example 15.3. make activation function hidden node h21as follows h21=\u001a1 ifh11+h12+h13\u00000:5 0 0 otherwise hidden node h22must provide plane linearly separates 1 1 1 points unit cube following one plane h11+h12+h13= 2:5 make activation function hidden node h22as follows h22=\u001a1 ifh11+h12+h13\u00002:5 0 0 otherwise finally node ymust provide line linearly separates 0 1 1 0 1 1 following one line h21+h22= 0:5 make activation function node yas follows y=\u001a1 ifh21+h22\u00000:5 0 0 otherwise 15.2.3 structure feedforward neural network shown neural network one hidden layer figure 15.5 neural network two hidden layers figure 15.8 present general structure feedforward neural network structure appears figure 15.11. far left input consists ofx1 x2 xk next 0 hidden layers far right output layer consists 1 nodes y1 y2 ym hidden layer contain di erent number nodes.figure 15.11 general structure feedforward neural network sections 15.2.1 15.2.2 constructed neural network assigned values weights achieve desired results typically rather learn weights using gradient descent algorithm similar complex algorithm 15.1 perceptron example could rst construct network figure 15.8 provide form activation functions hidden nodes output node discussed next section finally provide many data items algorithm value x1 x2 xk c wherecis class point x 1 x2 xk belongs algorithm learns weights resultant network classify new points di\u000eculty n't know network structure work given problem beforehand example without detailed analysis problem section 15.2.2 would know assign two hidden layers rst layer contains 3 nodes second layer contains 2 nodes general experiment di erent network con gurations using technique cross validation section 5.2.3 nd con guration gives best results various strategies con guring network one strategy make number hidden nodes equal number input variables assign various layer node per layer con gurations however probably result over- tting dataset size small compared input size another strategy make number hidden nodes greater number data items try various layer node per layer con gurations summary develop neural network application need data input vari- able output variable construct network con guration hidden node layers output layer nal step specify activation functions discuss next note implementing neural network scratch would also need program gradient descent algorithm nds optimal values weights however henceforth assume using neural network package algorithms implemented present packages nal section.15.3 activation functions next discuss activation functions ordinarily used neural networks 15.3.1 output nodes di erent activation functions output nodes depending task classifying data one two di erent classes need output nodes represent binary classi cation classifying data one three possible classes need output nodes represent multinomial classi cation output continuous distribution normal distribution need nodes represent properties distribution discuss turn 15.3.1.1 binary classi cation binary classi cation want classify predict variable two possible values example may want predict whether patient 's cancer metastasize based features patient 's cancer cases want system simply say `` yes '' `` '' rather want know example probability cancer metastasizing rather using discrete activation function used perceptron ordinarily use sigmoid function also used logistic regression section 5.3.3 assuming single output node yhas vector hidden nodes parents sigmoid activation function binary outcome f h =exp w 0+pwihi 1 exp w 0+pwihi 15.4 yields p y= 1jh example 15.4 suppose output function sigmoid function w0= 2 w 1=\u00004 w 2=\u00003 w 3= 5 suppose particular input h1= 6 h 2= 7 h 3= 9 f h =exp w 0+w1\u0002h1+w2\u0002h2+w3\u0002h3 1 exp w 0+w1\u0002h1+w2\u0002h2+w3\u0002h3 =exp 2\u00004\u00026\u00003\u00027 5\u00029 1 exp 2\u00004\u00026\u00003\u00027 5\u00029 0:881 p y= 1jh 0:881 figure 15.12 output layer classifying variable mvalues using softmax function 15.3.1.2 multinomial classi cation multinomial classi cation want classify predict variable mpossible values important example neural networks often applied image recog- nition example may handwritten symbol one digits 0-9 goal classify symbol digit intended writer multinomial classi- cation use extension sigmoid function called softmax function using function develop one output node mpossible outcomes assign following activation function kth output node fk h =exp wk0+pwkihi pm j=1exp wj0+pwjihi way kth output node provides p y=kjh figure 15.12 shows output layer classifying variable mvalues using softmax function example 15.5 suppose output function softmax function 3 outputs y1 y2 andy3 w10= 2 w 11=\u00004 w 12= 3 w20= 1 w 21= 9 w 22=\u00003 w30= 10 w 31= 7 w 32=\u00004 suppose particular input h1= 2 h 2= 4 w10+w11h1+w12h2= 2\u00004\u00022 3\u00024 6 w20+w21h1+w22h2= 1 9\u00022\u00003\u00024 7 w30+w31h1+w32h2= 10 7\u00022\u00004\u00024 8 f1 h =exp w 10+pw1ihi p3 j=1exp wj0+pwjihi =exp 6 exp 6 exp 7 exp 8 0:090 f2 h =exp w 20+pw2ihi p3 j=1exp wj0+pwjihi =exp 7 exp 6 exp 7 exp 8 0:245 f3 h =exp w 30+pw3ihi p3 j=1exp wj0+pwjihi =exp 8 exp 6 exp 7 exp 8 0:665 p 1jh 0 :090 p 2jh 0 :245 p 3jh 0 :665 15.3.1.3 normal output distributions instead modeling output one mvalues discrete assume normally distributed \u001a yjx normalden \u0016 \u001b2 xis input vector example may want predict normal distribution systolic blood pressure based patient 's features case single output node value mean normal distribution activation function simply linear activation function \u0016=f h =w0+x wihi example 15.6 suppose output function mean normal distribution w0= 2 w 1=\u00004 w 2=\u00003 w 3= 5 suppose particular input h1= 3 h 2= 7 h 3= 8 mean output normal distribution follows \u0016=f h =w0+w1\u0002h1+w2\u0002h2+w3\u0002h3 2\u00004\u00023\u00003\u00027 5\u00028 9 15.3.2 hidden nodes popular activation function hidden nodes especially applications involving image recognition recti ed linear activation function already used function follows f h max 0 w0+x wihi notice activation function similar linear activation function discussed except returns 0 linear combination negative.example 15.7 suppose activation function recti ed linear function w0=\u00005 w 1=\u00004 w 2=\u00003 w 3= 5 suppose particular input h1= 8 h 2= 7 h 3= 9 f h max 0 w 0+w1\u0002h1+w2\u0002h2+w3\u0002h3 max 0 \u00005\u00004\u00028\u00003\u00027 5\u00029 max 0 \u000013 0 another activation function used hidden nodes maxout activation func- tion function rweight vectors ris parameter take maximum weighted sums z1=w10+x w1ihi z2=w20+x w2ihi ... zr=wr0+x wrihi f h max z1 z2 zr example 15.8 suppose activation function maxout function r= 3 w10= 2 w 11=\u00004 w 12= 3 w20= 1 w 21= 9 w 22=\u00003 w30= 10 w 31= 7 w 32=\u00004 suppose particular input h1= 2 h 2= 4 z1=w10+w11h1+w12h2= 2\u00004\u00022 3\u00024 6 z2=w20+w12h1+w22h2= 1 9\u00022\u00003\u00024 7 z3=w30+w31h1+w32h2= 10 7\u00022\u00004\u00024 8 f h max z1 z2 z3 max 6 7 8 8 sigmoid activation function equation 15.4 also used activation func- tion hidden notes related function also used hyperbolic tangent activation function follows f h tanh w0+x wihi figure 15.13 examples handwritten digits mnist dataset 15.4 application image recognition mnist dataset http //yann.lecun.com/exdb/mnist/ academic dataset used evaluate performance classi cation algorithms dataset consists 60,000 training images 10,000 test images image one digits 0-9 handwritten standardized 28 28 pixel greyscale image 784 pixels figure 15.13 shows examples digits dataset candel et al 2015 developed neural network solve problem classifying images test dataset learning system training dataset network 784 inputs one pixel 10 outputs one digit three hidden layers layer containing 200 hidden nodes hidden node uses recti ed linear activation function output nodes use softmax function figure 15.14 depicts network neural network classi cation error rate 0.0083 ties best error rate previously achieved microsoft 15.5 discussion reading title chapter \\neural networks deep learning '' yet never mentioned '' deep learning '' text mentioned section 1.1.2 1940s foundational e orts ai involved modeling neurons brain resulted eld neural networks hebb 1949 logical approach ai became dominant 1950s neural networks fell popularity however new algorithms training neural networks dramatically increased computer processing speed resulted re-emergence use neural nets eld called deep learning goodfellow et al. 2016 deep learning neural network architectures di er older neural networks often hidden layers furthermore deep learning networks trained using unsupervised supervised learning presented supervised learning approach convolutional neural networks ones whose architectures make explicit assumption inputs images therefore encode speci c properties images recurrent neural networks class neural nets feed state previous time step current time step applied example automatic text generation discussed .figure 15.14 neural network used classify digits mnist dataset brief introduction basics neural networks thor- ough coverage including discussion gradient descent algorithms used learn neural network parameters referred goodfellow et al. 2016 theodoridis 2015 download software implements neural networks two products h20 https //www.h2o.ai/ tenser ow https //www.tensor ow.org/ deep learning used solve variety problems di\u000ecult approaches close listing speci c applications 1. object detection classi cation photograph krizhevsky et al. 2012 problem involves classifying objects photograph one set previously known objects 2. adding color black white photographs zang et al. 2016 problem con- cerns adding color black white photographs 3. automatic image caption generation karpathy fei-fei 2015 task concerns generating caption describes contents image 4. adding sound silent videos owens et al. 2016 problem concerns synthesiz- ing sounds best match happening scene video 5. automatic language translation sutskever et al. 2014 task involves translat- ing word phrase sentence one language another language 6. automatic handwriting generation graves 2014 problem involves generating new handwriting given word phrase based set handwriting examples 7. automatic text generation sutskever et al. 2011 problem involves using large amount text learn model generate new word phrase sentence based partial word text.8 automatic game playing mnih et al. 2015 problem concerns learning play computer game based pixels screen modeling problem using neural network model black box sense structure parameters layers provide us model reality grasp bayesian networks hand provide relationship among variables often interpreted causal furthermore bayesian networks enable us model understand complex human decisions although architectures used model many problems neural networks often successfully applied problems involve human intelligence described cognitive level problems include computer vision image processing text analysis bayesian networks hand often applied successfully problems involve determining relationships among related random variables exploiting relationships inference make decisions classic example medical decision support system see section 7.7 exercises section 15.1 exercise 15.1 resultant line example 15.1 linearly separate data work iterations repeat loop exercise 15.2 suppose set \u0015= 0 2 following data x1x2y 2 3-1 4 5 1 work iterations repeat loop algorithm 15.1 resultant line linearly separates data exercise 15.3 left exercise implement algorithm 15.1 apply data table 15.1 compare results obtained logistic regression this.section 15.2 exercise 15.4 suppose three lines determining regions figure 15.7 follows h11 4\u00002x1\u0000x2= 0 h12 1\u0000x1\u0000x2= 0 h13 3 2x 1\u0000x2= 0 plot three lines show regions corresponding classes c1andc2 develop parameters neural network figure 15.8 network properly classi es points classes c1andc2 section 15.3 exercise 15.5 suppose output function sigmoid function binary output w0= 1 w 1=\u00004 w 2=\u00005 w 3= 4 suppose particular input h1= 4 h 2= 5 h 3= 6 computef h p y= 1jh exercise 15.6 suppose output function softmax function 4 outputs y1 y2 y3 y4 suppose w10= 1 w 11=\u00003 w 12= 2 w20= 2 w 21= 7 w 22=\u00002 w30= 6 w 31= 5 w 32=\u00004 w40= 5 w 41=\u00003 w 42= 6 suppose particular input h1= 3 h 2= 4 h 3= 5 computef1 h f 2 h f 3 h f 4 h p y=ijh fori= 1 2 3 4 exercise 15.7 suppose activation function recti ed linear function w0= 5 w 1=\u00004 w 2= 2 w 3= 4 w 4= 8 suppose particular input h1= 8 h 2= 7 h 3= 6 h 4= 5 computef h exercise 15.8 suppose activation function maxout function r= 2 w10= 1 w 11=\u00002 w 12= 6 w 13= 5 w20= 2 w 21= 8 w 22=\u00002 w 23= 4 suppose particular input h1= 3 h 2= 2 h 3= 5 computef h .section 15.4 exercise 15.9 metabric dataset introduced curtis et al 2012 provides data breast cancer patient features tumor size outcomes death dataset obtained https //www.synapse.org/ synapse syn1688369/wiki/27311 gain access dataset download one neural network packages discussed section 15.5. divide dataset training dataset containing 2/3 data test dataset containing 1/3 data apply various parameter settings e.g. number hidden layers number hidden nodes per layer training set setting 5-fold cross validation analysis goal classify/predict whether patient dies determine area roc curve auroc settings apply settings best auroc test data determine auroc test data anaive bayesian network network one root nodes children root edges among children naive bayesian network used discrete classi cation making target root predictors children xlstat includes naive bayesian network module free naive bayesian network software available various sites including http //www.kdnuggets.com/software/bayesian.html download naive bayesian network software package study outlined using package vary whatever parameters available software 5-fold cross validation analysis parameter setting compare aurocs obtained neural network method naive bayesian network method applied test data exercise 15.10 discussed section 15.4 mnist dataset academic dataset used evaluate performance classi cation algorithms dataset consists 60,000 training images 10,000 test images image one digits 0-9 handwritten standardized 28 28 pixel greyscale image 784 pixels download dataset download one neural network packages discussed section 15.5. apply various parameter settings e.g. number hidden layers number hidden nodes per layer training set setting 5-fold cross validation analysis goal classify/predict correct digit determine area roc curve auroc settings apply settings best auroc test data determine auroc test data apply naive bayesian network dataset use various parameter settings 5-fold cross validation training dataset apply best parameter values test dataset compare aurocs obtained neural network method naive bayesian network method applied test dataset.ai-completeness problem domain superintelligent machines chapter excerpted artificial superintelligence futuristic approach roman v. yampolskiy 2018taylor francis group rights reserved.7 learn moreai-completeness problem domain superintelligent machines 1.1 introduction since inception 1950s field artificial intelligence ai produced unparalleled accomplishments failing formalize problem space concerns chapter addresses shortcom ing extending previous work yampolskiy 2012a contributing theory ai-completeness formalism designed field ai notion np-completeness np stands nondeterminis tic polynomial time computer science general belief formalization allow even faster progress solving remaining problems humankinds quest build intelligent machine.according wikipedia term ai-complete proposed fanya montalvo 1980s ai-complete 2011 somewhat gen eral definition term included 1991 jargon file raymond 1991 states ai-complete mit stanford analogy np-complete adj used describe problems subproblems ai indi cate solution presupposes solution strong ai reprinted roman v. yampolskiy artificial intelligence evolutionary computation andmetaheuristics studies computational intelligence 427:317 2013 kind permission springer science business media copyright 2013 springer science business media.problem synthesis human-level intelligence problem ai-complete words hard.as term ai-complete sometimes ai-hard part field many years frequently brought express difficulty specific problem investigated researchers see mueller 1987 mallery 1988 gentry ramzan stubblebine 2005 phillips beveridge 2009 bergmair 2004 ide vronis 1998 navigli velardi 2005 nejad 2010 chen et al 2009 mcintire havig mcintire 2009 mcintire mcintire havig 2009 mert dalkilic 2009 hendler 2008 leahu sengers mateas 2008 yampolskiy 2011 informal use encouraged similar concepts developed areas sci ence biometric-completeness phillips beveridge 2009 automatic speech recognition asr -complete morgan et al 2003 although recently numerous attempts formalize means say prob lem ai-complete published ahn et al 2003 shahaf amir 2007 demasi szwarcfiter cruz 2010 even formalization attempts systems relied humans solve problems perceived ai-complete utilized anticaptcha systems use humans break captcha completelyautomated public turing test tell computers humans apart security protocol ahn et al 2003 yampolskiy 2007a 2007b yampolskiy govindaraju 2007 either directly hiring cheap workers indeveloping countries bajaj 2010 rewarding correctly solvedcaptchas presentation pornographic images vaas 2007 chinese room philosophical argument john searle showsthat including human part computational system mayactually reduce perceived capabilities understanding andconsciousness searle 1980 content development online projects encyclopedias wikipedia conservapedia libraries project gutenberg video col lections youtube open-source software sourceforge relyon contributions people content production qualityassurance cyphermint check-cashing system relies human workersto compare snapshot person trying perform financialtransaction picture person initially enrolled system resulting accuracy outperforms biometric system almost completely spoof proof see http //cyphermint.com information data-tagging systems entice user providing metadata forimages sound video files popular approach involves develop ing online game by-product participation produces alarge amount accurately labeled data ahn 2006 distributed proofreaders employs number human volun teers eliminate errors books created relying opticalcharacter recognition process see http //pgdp.net/c/ moreinformation interactive evolutionary computation algorithms use humans inplace fitness function make judgments regarding difficult-to-formalize concepts aesthetic beauty taste takagi 2001 mechanical turk attempt amazon.com create artificialai humans paid varying amounts solving problems arebelieved beyond current abilities ai programs see https //www.mturk.com/mturk/welcome information gen eral idea behind turk broad appeal researchers arecurrently attempting bring masses via generalizedtask markets gtms shahaf horvitz 2010 horvitz paek2007 horvitz 2007 kapoor et al 2008 spam prevention easy accomplish humans vote one-mails receive spam certain threshold reached particular piece e-mail could said high degree accu racy spam dimmock maddison 2004 .recent work attempted formalize intuitive notion ai-completeness particular three endowers worth reviewing next yampolskiy 2012a 2003 ahn et al attempted formalize notion ai-problem concept ai-hardness context computer security ai-problem defined triple =sdf set problem instances prob- ability distribution problem set f 0 1 answers instances let 0 1 require 0 fraction humans h pr xd h x f x ai problem said -solved exists program running time input pr xd r r x f x said solution said -hard ai problem current program solution ahn et al 2003 298 .it interesting observe proposed definition terms dem ocratic consensus ai community researchers say problem hard must also time solve problem taken account definition simply requires humans able solve problem ahn et al 2003 .in 2007 shahaf amir presented work theory ai-completeness work puts forward concept human-assisted turing machine formalizes notion different human oracles hos see section hos technical details main contribution paper comes form method classify ing problems terms human-versus-machine effort required find solution common problems natural language standing nlu work proposes method reductions allow conversion nlu problem speech understanding via text-to-speech software.in 2010 demasi et al demasi szwarcfiter cruz 2010 presented work problem classification artificial general intelligence agi proposed framework groups problem space three sectors non-agi-bound problems interest agi researchers agi-bound problems require human-level intelligence besolved agi-hard problems least hard agi-boundproblem.the work also formalizes notion hos provides number definitions regarding properties valid operations.1.2 theory ai-completeness people mental disabilities geniuses human minds cog nitively diverse well known different people exhibit different mental abilities define notion ho function capable comput ing function computable union human minds words cognitive ability human repeatable ho make ho easier understand provide figure1.1 illus trates human function.such function would easy integrate modern program ming language would require input function pro vided single string length n function would return string length m. encoding specified content strings n could either binary representations data english language phrasesboth computationally equivalent necessary human function could call regular turing machine tm functions help processing data example simple computer program would dis play input string picture make human comprehension easier could executed humans could assumed cooperating perhaps reward alternatively one construct human function instead union minds computes average decision human minds problem encoded input string number minds goes infinity avoid confusion propose nam ing first ho human best second ho humanaverage problems ai domain tend large degree ambiguity terms acceptable correct answers depending problem hand sim plistic notion average answer could replaced aggregate string human string input return output figure 1.1 human oracle humanbest union minds.answer defined wisdom-of-crowds approach surowiecki 2004 functions could formalized human-assisted turing machines shahaf amir 2007 .the human function easy-to-understand -use generalization ho one perceive way connect exchange informa tion real human sitting computer terminal although easy intuitively understand description sufficiently formal shahaf et al formalized notion ho human-assisted turing machine htm shahaf amir 2007 model human oracle machine decide set languages l constant time h l li time complexity taken account answering ques tion might take nonconstant time h l fi li fi f time-complexity function language li meaning human decide x l fi |x| time realistically address capabilities individual humans probabilistic oracle also presented pro vided correct answers probability p h l pi li 0 pi 1 finally notion reward introduced model capture humans improved performance paid tasks h l ui li ui u utility function shahaf amir 2007 .1.2.1 definitionsdefinition 1 problem c ai-complete two properties 1.it set ai problems ho solvable 2.any ai problem converted c polynomial timealgorithm.definition 2 ai-hard problem h ai-hard ai-complete problem c polynomial time turing reducible h definition 3 ai-easy complexity class ai-easy set problems solvable polynomial time deterministic turing machine oracle ai problem words problem x ai-easy exists ai problem x poly nomial time turing reducible y. means given oracle exists algorithm solves x polynomial time.figure1.2 illustrates relationship different ai complexity classes right side figure shows situation ever proven ai problems ai-complete problems left side shows converse.1.2.2 turing test first ai-complete problemin section show turing test tt turing 1950 problem ai-complete first need establish tt indeed ai problem ho solvable trivially follows definition test test measures human-like performance demonstrated test taker hos defined produce human-level performance human intelligence test intuitively understood terms already shown hos could expressed strictly formal terms tt also could formalized interactive proof shieber 2006 2007 bradford wollowski 1995 .the second requirement problem proven ai-complete ai problem convertible instance problem consideration polynomial time via turing reduc tion therefore need show problem solvable human function could encoded instance tt ho-solvable problem h string input encodes problem string output encodes solution taking input question used tt output answer expected administer- ing tt see ho-solvable problem could reduced polynomial time instance tt clearly described process polynomial time similar algorithm ai problem could reduced tt even theoretically possible construct complete tt utilizes problems solvable ho generating one question problem.ai-har da i-hardai-completeai-problems ai-complete =ai-problems ai-problems ai-complete ai-problems ai-complete figure 1.2 relationship ai complexity classes.1.2.3 reducing problems tthaving shown first problem tt ai-complete next step see well-known ai problems also ai-complete effort similar work richard karp showed 21 problems np-complete 1972 work started new field computational complexity karp 1972 according encyclopedia artificial intelligence shapiro 1992 following problems believed ai-complete constitute primary targets effort proving formal ai-completeness shapiro 1992 5457 natural language understanding encyclopedic knowledge isrequired understand natural language therefore completenatural language system also complete intelligent system problem solving since area investigated ai researchersmay seen consisting problems solved ai may beseen involving problem solving search knowledge representation reasoning intended use isto use explicitly stored knowledge produce additional explicitknowledge reasoning together knowledge represen tation reasoning seen necessary sufficientfor producing general intelligenceit another ai-complete area vision image understanding take interpreting broadlyenough clear general intelligence may needed thisinterpretation correct interpretation implies general intel ligence another ai-complete area.now tt proven ai-complete addi tional way showing problems ai-complete either show problem set ai problems ai problems converted polynomial time algorithm reduce instance tt problem problem already proven ai-complete instance problem trying show ai-complete second approach seems particularly powerful general heuristic approach see information encoding question could asked administration tt could encoded instance problem question likewise potential solution problem would constitute answer relevant tt question heuristic easy see exam-ple chess ai-complete limited information encoded starting position standard-size chessboard surprisingly chess one greatest successes ai currently chess-playing programs dominate human players including world champions.question answering qa hirschman gaizauskas 2001 salloum 2009 subproblem natural language processing answering ques tions level human something hos particularly good based definition consequently qa ai-problem one two requirements showing ai-complete access oracle capable solving qa allows us solve tt via simple reduction statement presented administration tt transform said statement question qa oracle answers produced oracle used replies tt allowing program pass tt important note access qa oracle sufficient pass tt questions restricted stand-alone queries could contain information previous questions otherwise prob lem readily solvable even todays machines ibms watson showed remarkable performance human jeopardy cham pions pepitone 2011 .speech understanding su anusuya katti 2009 another sub problem natural language processing understanding speech level human something hos particularly good based defi nition consequently su ai-problem one two require ments showing ai-complete access oracle capable solving su allows us solve qa via simple reduction reduce qa su utilizing text-to-speech software taylor black 1999 chan 2003 fast accurate reduction effectively transforms written questions spoken ones making possible solve every instance qa referring su oracle.1.2.4 probably ai-complete problemsi hope work challenge ai community prove impor tant problems either belonging belonging class although following problems explicitly shown ai-complete strong candidates classification problems great practical importance making classification worthy endeavor problem explicitly conjectured ai-complete published paper include source speculation dreaming salloum 2009 commonsense planning shahaf amir 2007 foreign policy mallery 1988 problem solving shapiro 1992 judging tt shahaf amir 2007 commonsense knowledge andrich novosel hrnkas 2009 su shahaf amir 2007 knowledge representation reasoning shapiro 1992 word sense disambiguation chen et al 2009 navigli velardi 2005 machine translation ai-complete 2011 ubiquitous computing leahu sengers mateas 2008 change management biomedical ontologies nejad 2010 nlu shapiro 1992 software brittle ness ai-complete 2011 vision image understanding shapiro 1992 1.3 first ai-hard problem programming define problem programming taking natural language descrip tion program producing source code compiled readily available hardware/software produce computer program satisfies implicit explicit requirements provided natural language description programming problem assignment simple examples programming typical assignments given students computer science classes example write program play tic-tac-toe successful students write source code correctly compiled allows grader engage computer instance game many requirements assignment remain implicit response time computer less minute implicit requirements usually easily inferred students access culture-instilled common sense writing program capable solving programming outside strictly restricted domains.having access oracle capable solving programming allows us solve tt via simple reduction statement presented tt transform said statement programming assignment form write program would respond statement indistinguish able statement provided average human full transcript tt may also provided disambiguation purposes applied set possible tt statements procedure clearly allows us pass tt however programming ai-problems many instances programming solvable hos example write program pass turing test known ai-problem proposed definition consequently programming ai-hard problem.1.4 beyond ai-completeness ho function presented chapter assumes human behind assistance computer order process cer tain human unfriendly data formats example binary string rep resenting video completely impossible human interpret could easily played computer program intended format making possible human solve video understanding-related ai-complete problem obvious human provided access computer perhaps internet connection powerful intel ligence compared unenhanced way human consequently important limit help computer human worker inside ho function assistance domain input/output conversion beyond resulting function would ai-complete computer complete.figure1.3 utilizes venn diagram illustrate subdivisions problem space produced different types intelligent computational devices region 1 represents known universal intelligence legg hutter 2007 super intelligence legg 2008 yampolskiy 2011a 2011b 2012b computational agent outperforms intelligent universal intelligence2human inteligence1568arti/f_icial intelligence374animal intelligence figure 1.3 venn diagram four different types intelligence.agents possible environments region 2 standard unen hanced human-level intelligence type capable passing tt time incapable computation involving large numbers sig nificant amount memorization region 3 currently possible accomplish via state-of-the-art ai programs finally region 4 represents abstract view animal intelligence.ai intelligence researchers strive produce universal intelligence certainly likely happen given recent trends hardware software developments theoretical underpinning church/turing thesis turing 1936 also likely able enhance human minds additional memory port higher-speed hardware essentially obtain universal intelligence sandberg bostrm 2008 .while universal intelligence incorporates abilities lower intelligences interesting observe human ai animal intelligences many interesting regions intersection yampolskiy fox 2012 example animal minds good human minds visual understanding natural scenes regions 5 6 7 illustrate common problem spaces two different types intelligent agents region 8 represents common problem solving abilities humans com puters animals understanding regions commonality may help us better separate involved computational classes represented abilities specific computational agent minus com monalities computational agent trying draw distinction example captcha ahn et al 2003 type tests rely inability computers perform certain pattern recognition tasks level accuracy humans order separate ai agents human agents alternatively test could devised tell humans armed calculators ais looking upper level abil ity test easy defeat effort made compile formalize limitations biases human mind.it also interesting consider problem solving abilities hybrid agents already noted human equipped computer lot capable compared unaided person research brain computer interfaces vidal 1973 provides potential path future devel opments area interestingly combining pattern recognition abilities animals symbol processing abilities ai could produce computational agent large domain human-like abilities see work roborats talwar et al 2002 monkey controlled robots nicolelis et al 2000 likely near future different types intelligent agents combined even greater extent work way believe may useful introduce additional terminology field ai problem classification complete space problems pro pose computational agents capable solving specific sub set problems get represent set question therefore propose additional terms computer-complete animals-complete repre sent computational classes solvable agents understood humans differ abilities animals computers aggregation averaging utilized human function could similarly applied definition respective oracles research progresses common names may needed different combinations regions figure1.3 illustrat ing concepts human-ai hybrid animal-robot hybrid.certain aspects human cognition map well onto space problems seen lot success ai research field internal states human mind consciousness stream self-awareness understanding emotions love hate feelings pain pleasure etc. currently addressable methods current state-of-the-art tech nologies sufficient unambiguously measure detect inter nal states consequently even existence universally accepted many scientists propose ignoring internal states claim noth ing byproduct flawed self-analysis scientists want us restrict science measurable behavioral actions however since persons access internal states least one thinking machine interest trying investigate internal states human mind unlikely vanish.while able present formal theory ai-completeness based concept hos theory strong enough address prob lems involving internal states mind fact one fundamental arguments ability implement understanding system based symbol manipulation searles chinese room thought experiment relies generalized concept human part computational cycle seems current turing/von neumann architecture incapable dealing set problems related internal states human mind perhaps new type com putational architecture capable mimicking internal states developed future likely inspired better understanding human biology cognitive science research cre ating artificial consciousness ac attracting lot attention least terms number ac papers published.as part ongoing effort classify ai related problems propose new category specifically devoted problems reproducing internal states human mind artificial ways call group problems consciousness-complete c-complete short oracle capable solving c-complete problems would fundamentally different oracle machines proposed turing c-oracles would take input way standard counterparts would produce sym bolic output result work would novel internal state oracle may become accessible us new type hardware discussed developed.just sat shown first np-complete problem tt first ai-complete problem suspect consciousness shown first c-complete problem internal-state related problems reducible internal state problems also c-complete beyond scope preliminary work even consciousness-capable hardware available moment writing theory c-completeness still useful tool allows formal classification classical problems field artificial intelligence two important categories potentially solvable current technol- ogy unsolvable current technology since information available hos output internal states funda mentally different c-oracles creating two disjoint sets problems.the history ai research full unwarranted claims anticipated breakthroughs conversely overestimations regarding difficulty problems viewed prism ai-complete/c-complete theories history ai starts make sense solutions problems classify ai-complete subject continuous steady improvement falling realm c-completeness effectively seen zero progress computer pain bishop 2009 dennett 1978 artificial consciousness searle 1980 dreyfus 1972 etc. proceed science needs better understand difference feeling thought feeling pain knowing pain certainly internal states hopeful future research area bring long-awaited answers 1.5 conclusions progress field artificial intelligence requires access well-defined problems measurable complexity theory ai-completeness aims provide base formalization showing certain problems ai-complete/-hard useful developing novel ways telling comput ers humans also problem shown ai-complete would great alternative way testing artificial intelligent agent see attained human level intelligence shahaf amir 2007 references ahn luis von june 2006. games purpose ieee computer magazine 9698.ahn luis von manuel blum nick hopper john langford 2003. captcha using hard ai problems security paper read eurocrypt advances cryptology eurocrypt 2003. international conference theory applications cryptographic techniques warsaw poland may 48 2003. published lecture notes computer science 2656 2003 294311.ai-complete 2011. accessed january 7.http //en.wikipedia.org/wiki/ai-complete.andrich christian leo novosel bojan hrnkas 2009. common sense knowledge exercise paperinformation search retrieval http //www.iicm.tu-graz.ac.at/cguetl/courses/isr/uearchive/uews2009/ue06-commonsenseknowledge.pdfanusuya m. a. s. k. katti 2009. speech recognition machine review international journal computer science information security ijcsis 6 3 :181205.bajaj vikas april 25 2010. spammers pay others answer security tests new york times.bergmair richard december 2004. natural language steganography ai-complete security primitive 21st chaos communication congress berlin.bishop mark 2009. computers cant feel pain minds machines 19 4 :507516.bradford philip g. michael wollowski 1995. formalization turing test sigart bulletin 6 4 :310.chan tsz-yan 2003. using text-to-speech synthesizer generate reverse turing test paper presented 15th ieee international conference tools artificial intelligence ictai03 washington dc november 35.chen junpeng juan liu wei yu peng wu november 30 2009. combining lexical stability improved lexical chain unsupervised word sense disambiguation paper presented second international symposium knowledge acquisition modeling kam 09 wuhan china.demasi pedro jayme l. szwarcfiter adriano j. o. cruz march 58 2010. theoretical framework formalize agi-hard problems paper presented third conference artificial general intelligence lugano switzerland.dennett daniel c. july 1978. cant make computer feels pain synthese 38 3 :415456.dimmock nathan ian maddison december 2004. peer-to-peer collabora tive spam detection crossroads 11 2 1725.dreyfus hubert l. 1972. computers cant critique artificial reason new york harper row.gentry craig zulfikar ramzan stuart stubblebine june 58 2005. secure distributed human computation paper presented 6th acm conference electronic commerce vancouver bc canada.hendler james september 2008. weve come long way maybe ieee intelligent systems 23 5 :23.hirschman l. r gaizauskas 2001. natural language question answering view natural language engineering 7 4 :275300.horvitz e. 2007. reflections challenges promises mixed-initiative interac tion ai magazinespecial issue mixed-initiative assistants 28 2 1118.horvitz e. t. paek 2007. complementary computing policies transfer ring callers dialog systems human receptionists user modeling user adapted interaction 17 1 :159182.ide n. j. vronis 1998. introduction special issue word sense dis ambiguation state art computational linguistics 24 1 :140. kapoor a. d. tan p. shenoy e. horvitz september 1719 2008. complementary computing visual tasks meshing computer vision human visual processing paper presented ieee international conference automatic face gesture recognition amsterdam.karp richard m. 1972. reducibility among combinatorial problems complexity computer computations edited r. e. miller j. w. thatcher 85103. new york plenum.leahu lucian phoebe sengers michael mateas september 2124 2008. interactionist ai promise ubicomp put box world without putting world box paper presented tenth international conference ubiquitous computing seoul south korea legg shane june 2008. machine super intelligence phd thesis university lugano switzerland http //www.vetta.org/documents/machine_super_intelligence.pdflegg shane marcus hutter december 2007. universal intelligence defini tion machine intelligence minds machines 17 4 :391444.mallery john c. 1988. thinking foreign policy finding appropriate role artificial intelligence computers ph.d. dissertation mit political science department cambridge ma.mcintire john p. paul r. havig lindsey k. mcintire july 2123 2009. ideas authenticating humanness collaborative systems using ai-hard problems perception cognition paper presented ieee national aerospace electronics conference naecon dayton oh.mcintire john p. lindsey k. mcintire paul r. havig may 1822 2009. variety automated turing tests network security using ai-hard problems perception cognition ensure secure collaborations paper presented international symposium collaborative technologies systems cts 09 baltimore.mert ezgi cokhan dalkilic september 1416 2009. word sense disambiguation turkish paper presented 24th international symposium computer information sciences iscis 2009 guzelyurt turkey.morgan nelson d. baron s. bhagat h. carvey r. dhillon j. edwards d. gelbart a. janin a. krupski b. peskin t. pfau e. shriberg a. stolcke c. wooters april 610 2003. meetings meetings research icsi speech multiparty conversations paper presented ieee international conference acoustics speech signal processing icassp 03 hong kong.mueller erik t. march 1987. daydreaming computation phd dissertation university california los angeles.navigli roberto paola velardi july 2005. structural semantic inter connections knowledge-based approach word sense disambigua tion ieee transactions pattern analysis machine intelligence 27 7 :10751086.nejad arash shaban april 2010. framework analyzing changes health care lexicons nomenclatures phd dissertation concordia university montreal qc canada.nicolelis miguel a. l. johan wessberg christopher r. stambaugh jerald d. kralik pamela d. beck mark laubach john k. chapin jung kim 2000. real-time prediction hand trajectory ensembles cortical neu rons primates nature 408 6810 :361.pepitone julianne 2011. ibms jeopardy supercomputer beats humans prac tice bout cnnmoney http //money.cnn.com/2011/01/13/technology/ibm_jeopardy_watson accessed january 13.phillips p. jonathon j. ross beveridge september 2830 2009. introduction biometric-completeness equivalence matching quality paper presented ieee 3rd international conference biometrics theory applications systems btas 09 washington dc raymond eric s. march 22 1991. jargon file version 2.8.1. http //catb.org/esr/jargon/oldversions/jarg282.txtsalloum w. november 30 2009. question answering system based conceptual graph formalism paper presented 2nd international symposium knowledge acquisition modeling kam 2009 wuhan china.sandberg anders nick bostrm 2008. whole brain emulation roadmap technical report 2008-3. future humanity institute oxford university http //www.fhi.ox.ac.uk/reports/2008-3.pdfsearle john 1980. minds brains programs behavioral brain sciences 3 3 :417457.shahaf dafna eyal amir march 2628 2007. towards theory ai completeness paper presented 8th international symposium logical formalizations commonsense reasoning commonsense 2007 stanford university stanford ca.shahaf d. e. horvitz july 2010. generalized task markets human machine computation paper presented twenty-fourth aaai conference artificial intelligence atlanta ga.shapiro stuart c. 1992. artificial intelligence encyclopedia artificial intelligence edited stuart c. shapiro 5457. new york wiley.shieber stuart m. july 1620 2006. turing test demonstrate intelligence paper presented twenty-first national conference artificial intelligence aaai-06 boston.shieber stuart m. december 2007. turing test interactive proof nous 41 4 :686713.surowiecki james 2004. wisdom crowds many smarter collective wisdom shapes business economies societies nations new york little brown.takagi h. 2001. interactive evolutionary computation fusion capaci ties ec optimization human evaluation proceedings ieee 89 9:12751296.talwar sanjiv k. shaohua xu emerson s. hawley shennan a. weiss karen a. moxon john k. chapin may 2 2002. behavioural neuroscience rat navigation guided remote control nature 417:3738. taylor p. a. black 1999. speech synthesis phonological structure matching paper presented eurospeech99 budapest hungary.turing 1950. computing machinery intelligence mind 59 236 :433460. turing alan m. 1936. computable numbers application entscheidungs problem proceedings london mathematical society 42:230265.vaas lisa december 1 2007. striptease used recruit help cracking sites pc magazine http //www.pcmag.com/article2/0,2817,2210671,00.aspvidal j. j 1973. toward direct brain-computer communication annual review biophysics bioengineering 2:157180.yampolskiy r. v. 2011. ai-complete captchas zero knowledge proofs access artificially intelligent system isrn artificial intelligence 2012:271878.yampolskiy roman v. april 13 2007a embedded captcha online poker paper presented 20th annual cse graduate conference grad-conf2007 buffalo ny.yampolskiy roman v. september 28 2007b graphical captcha embedded cards paper presented western new york image processing workshop wnyipw ieee signal processing society rochester ny.yampolskiy roman v. october 34 2011a artificial intelligence safety engineering machine ethics wrong approach paper presented philosophy theory artificial intelligence pt-ai2011 thessaloniki greece.yampolskiy roman v. october 34 2011b singularity paradox paper presented philosophy theory artificial intelligence pt-ai2011 thessaloniki greece.yampolskiy roman v. april 2122 2012a ai-complete ai-hard ai-easyclassification problems ai paper presented 23rd midwest artificial intelligence cognitive science conference cincinnati oh.yampolskiy roman v. 2012b leakproofing singularityartificial intel ligence confinement problem journal consciousness studies jcs 19 12 :194214.yampolskiy roman v. joshua fox 2012. artificial general intelligence human mental model singularity hypothesis scientific philosophical assessment edited amnon eden jim moor johnny soraker eric steinhart 129146. new york springer.yampolskiy roman v. venu govindaraju 2007. embedded non-interactive continuous bot detection acm computers entertainment 5 4 :111\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fetch_text_from_pdf(pdf_link):\n",
    "    try:\n",
    "        # Download the PDF file from the provided link\n",
    "        response = requests.get(pdf_link)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check if the response content type is PDF\n",
    "        if response.headers.get('content-type') == 'application/pdf':\n",
    "            # You have successfully fetched the PDF content\n",
    "            pdf_content = response.content\n",
    "\n",
    "            # Create a BytesIO stream from the PDF content\n",
    "            pdf_stream = BytesIO(pdf_content)\n",
    "\n",
    "            # Create a PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfFileReader(pdf_stream)\n",
    "\n",
    "            # Initialize a variable to store the extracted text\n",
    "            extracted_text = \"\"\n",
    "\n",
    "            # Extract text from each page of the PDF\n",
    "            for page_num in range(pdf_reader.numPages):\n",
    "                page = pdf_reader.getPage(page_num)\n",
    "                extracted_text += page.extractText()\n",
    "\n",
    "            return extracted_text  # Return the extracted text\n",
    "\n",
    "        else:\n",
    "            print(\"The fetched content is not a PDF.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text is None:\n",
    "        return \"\"  # Return an empty string if text is None\n",
    "\n",
    "    # Remove non-printable characters and Unicode escape sequences\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # TODO: Add preprocess steps as per data, Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text into individual words\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and punctuation from the tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)  # Access the punctuation characters\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Collect and preprocess data from the PDFs\n",
    "corpus = []  # Use a list to store preprocessed text for each book\n",
    "\n",
    "# List of books on Chanakya Neeti with their PDF links\n",
    "books = [\n",
    "    {\"title\": \"Language Models are Few-Shot Learners\", \n",
    "     \"author\": \"Tom B. Brown\", \"pdf_link\": \"https://arxiv.org/pdf/2005.14165.pdf\"},\n",
    "    {\"title\": \"Explorations in Artificial Intelligence and Machine Learning\", \n",
    "     \"author\": \"Prof. Roberto V. Zicari\", \"pdf_link\": \"https://www.routledge.com/rsc/downloads/AI_FreeBook.pdf\"},\n",
    "    \n",
    "    # Add more books to the list\n",
    "]\n",
    "\n",
    "\n",
    "for book in books:\n",
    "    pdf_link = book[\"pdf_link\"]\n",
    "    text = fetch_text_from_pdf(pdf_link)\n",
    "    \n",
    "    if text is not None:\n",
    "        processed_text = preprocess_text(text)\n",
    "        corpus.append(processed_text)  # Append the preprocessed text for each book to the corpus list\n",
    "\n",
    "# Print the preprocessed data\n",
    "for i, book in enumerate(books):\n",
    "    print(f\"Book {i + 1} - Title: {book['title']}, Author: {book['author']}\")\n",
    "    print(corpus[i])  # Print the preprocessed text for each book\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e5317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
