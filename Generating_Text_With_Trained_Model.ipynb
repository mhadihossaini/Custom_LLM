{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb30cbe",
   "metadata": {},
   "source": [
    "### Generating_Text_With_Trained_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c311e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 06:28:06.136443: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-09-14 06:28:06.138823: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-09-14 06:28:06.141063: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# -Importing the required libraries-\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -Define the vocabulary size, which determines the number of unique words the model can work with-.\n",
    "vocab_size = 10000\n",
    "\n",
    "# -Define the maximum sequence length, which is the maximum number of words in an input sequence-.\n",
    "max_seq_length = 50\n",
    "\n",
    "# -Specify the number of words to generate in the output text-.\n",
    "num_of_words_to_generate = 50\n",
    "\n",
    "\n",
    "# -Load the trained model-\n",
    "loaded_model = load_model('custom_llm_model.h5')\n",
    "\n",
    "# -Load the tokenizer from the saved file using pickle-\n",
    "with open(\"tokenizer.pkl\", \"rb\") as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "746ac2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -The generate_text function generates a text as the topic or prompt is sent to it-\n",
    "def generate_text(topic=\"\"): \n",
    "    \n",
    "    generated_text = topic\n",
    "    \n",
    "    # -Initialize an empty seed sequence-\n",
    "    seed_seq = []\n",
    "    for _ in range(num_of_words_to_generate):\n",
    "        # -Predict the probabilities of the next word-\n",
    "        predicted_probabilities = loaded_model.predict(pad_sequences([seed_seq], maxlen=max_seq_length-1, padding='pre'))[0]\n",
    "        \n",
    "        # -Sample the next word based on the predicted probabilities-\n",
    "        next_word_id = np.random.choice(len(predicted_probabilities), p=predicted_probabilities)\n",
    "        \n",
    "        # -Find the word corresponding to the predicted ID-\n",
    "        next_word = tokenizer.index_word.get(next_word_id, \"<OOV>\")\n",
    "        \n",
    "        # -Update the generated text-\n",
    "        generated_text += \" \" + next_word\n",
    "        \n",
    "        # -Update the seed sequence for the next iteration-\n",
    "        seed_seq.append(next_word_id)\n",
    "        seed_seq = seed_seq[-(max_seq_length-1):]\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -Specify the topic or prompt-\n",
    "topic = \"Meta learning\"\n",
    "\n",
    "# -Call the generate_text function-\n",
    "generated_text = generate_text(topic=topic)\n",
    "\n",
    "# -Printing the generated text-\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea20f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
