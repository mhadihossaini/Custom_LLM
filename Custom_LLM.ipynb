{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339fcd2b",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44262ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 04:39:13.586316: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# -Import the requied Libraries-\n",
    "import re\n",
    "import nltk\n",
    "import PyPDF2\n",
    "import pickle\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439f2fc",
   "metadata": {},
   "source": [
    "### Fetching data from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14908101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 1 - Title: Language Models are Few-Shot Learners, Author: Tom B. Brown\n",
      "\n",
      "\n",
      "Book 2 - Title: Explorations in Artificial Intelligence and Machine Learning, Author: Prof. Roberto V. Zicari\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fetch_text_from_pdf(pdf_link):\n",
    "    try:\n",
    "        # -Download the PDF file from the provided link-\n",
    "        response = requests.get(pdf_link)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # -Check if the response content type is PDF-\n",
    "        if response.headers.get('content-type') == 'application/pdf':\n",
    "            # -You have successfully fetched the PDF content-\n",
    "            pdf_content = response.content\n",
    "\n",
    "            # -Create a BytesIO stream from the PDF content-\n",
    "            pdf_stream = BytesIO(pdf_content)\n",
    "\n",
    "            # -Create a PDF reader object-\n",
    "            pdf_reader = PyPDF2.PdfFileReader(pdf_stream)\n",
    "\n",
    "            # -Initialize a variable to store the extracted text-\n",
    "            extracted_text = \"\"\n",
    "\n",
    "            # -Extract text from each page of the PDF-\n",
    "            for page_num in range(pdf_reader.numPages):\n",
    "                page = pdf_reader.getPage(page_num)\n",
    "                extracted_text += page.extractText()\n",
    "\n",
    "            return extracted_text  # -Return the extracted text-\n",
    "\n",
    "        else:\n",
    "            print(\"The fetched content is not a PDF.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching text from PDF: {e}\")\n",
    "        return None\n",
    "# -Function to process text-\n",
    "def preprocess_text(text):\n",
    "    if text is None:\n",
    "        return \"\"  # -Return an empty string if text is None-\n",
    "\n",
    "    # -Remove non-printable characters and Unicode escape sequences-\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # -TODO: Add preprocess steps as per data, Convert the text to lowercase-\n",
    "    text = text.lower()\n",
    "    # -Tokenize the text into individual words-\n",
    "    tokens = word_tokenize(text)\n",
    "    # -Remove stopwords and punctuation from the tokens-\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)  # -Access the punctuation characters-\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# -Collect and preprocess data from the PDFs-\n",
    "corpus = []  # -Use a list to store preprocessed text for each book-\n",
    "\n",
    "# -List of books on Chanakya Neeti with their PDF links-\n",
    "books = [\n",
    "    {\"title\": \"Language Models are Few-Shot Learners\", \n",
    "     \"author\": \"Tom B. Brown\", \"pdf_link\": \"https://arxiv.org/pdf/2005.14165.pdf\"},\n",
    "    {\"title\": \"Explorations in Artificial Intelligence and Machine Learning\", \n",
    "     \"author\": \"Prof. Roberto V. Zicari\", \"pdf_link\": \"https://www.routledge.com/rsc/downloads/AI_FreeBook.pdf\"},\n",
    "     #{\"title\": \"Artificial Intelligence A Modern Approach Third Edition\",\n",
    "     #\"author\": \"Stuart J. Russell and Peter Norvig\",\n",
    "     #\"pdf_link\": \"https://people.engr.tamu.edu/guni/csce421/files/AI_Russell_Norvig.pdf\"}\n",
    "     # Add more books to the list\n",
    "]\n",
    "\n",
    "\n",
    "for book in books:\n",
    "    pdf_link = book[\"pdf_link\"]\n",
    "    text = fetch_text_from_pdf(pdf_link)\n",
    "    \n",
    "    if text is not None:\n",
    "        processed_text = preprocess_text(text)\n",
    "        corpus.append(processed_text)  # -Append the preprocessed text for each book to the corpus list-\n",
    "\n",
    "# -Print the preprocessed data\n",
    "for i, book in enumerate(books):\n",
    "    print(f\"Book {i + 1} - Title: {book['title']}, Author: {book['author']}\")\n",
    "    #print(corpus[i])  # -Print the preprocessed text for each book-\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686bfe9",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1bc09ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 04:40:41.204035: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-09-14 04:40:52.175057: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-09-14 04:40:52.176774: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-09-14 04:40:52.178154: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 04:40:52.720236: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-09-14 04:40:52.722563: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-09-14 04:40:52.724112: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-09-14 04:40:53.558260: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-09-14 04:40:53.560172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-09-14 04:40:53.561855: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1637/1637 [==============================] - ETA: 0s - loss: 7.7350 - accuracy: 0.0275"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 04:45:22.941885: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-09-14 04:45:22.943507: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-09-14 04:45:22.944938: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1637/1637 [==============================] - 291s 176ms/step - loss: 7.7350 - accuracy: 0.0275 - val_loss: 8.7704 - val_accuracy: 0.0160\n",
      "Epoch 2/10\n",
      "1637/1637 [==============================] - 307s 188ms/step - loss: 6.8496 - accuracy: 0.0536 - val_loss: 8.8460 - val_accuracy: 0.0208\n",
      "Epoch 3/10\n",
      "1637/1637 [==============================] - 311s 190ms/step - loss: 6.1929 - accuracy: 0.0842 - val_loss: 9.0640 - val_accuracy: 0.0231\n",
      "Epoch 4/10\n",
      "1637/1637 [==============================] - 319s 195ms/step - loss: 5.4753 - accuracy: 0.1256 - val_loss: 9.5839 - val_accuracy: 0.0237\n",
      "Epoch 5/10\n",
      "1637/1637 [==============================] - 314s 192ms/step - loss: 4.7383 - accuracy: 0.1712 - val_loss: 9.9518 - val_accuracy: 0.0256\n",
      "Epoch 6/10\n",
      "1637/1637 [==============================] - 309s 188ms/step - loss: 4.0212 - accuracy: 0.2342 - val_loss: 10.4762 - val_accuracy: 0.0238\n",
      "Epoch 7/10\n",
      "1637/1637 [==============================] - 309s 189ms/step - loss: 3.3655 - accuracy: 0.3216 - val_loss: 10.8780 - val_accuracy: 0.0254\n",
      "Epoch 8/10\n",
      "1637/1637 [==============================] - 315s 193ms/step - loss: 2.7922 - accuracy: 0.4186 - val_loss: 11.1434 - val_accuracy: 0.0265\n",
      "Epoch 9/10\n",
      "1637/1637 [==============================] - 308s 188ms/step - loss: 2.3139 - accuracy: 0.5068 - val_loss: 11.5536 - val_accuracy: 0.0241\n",
      "Epoch 10/10\n",
      "1637/1637 [==============================] - 330s 202ms/step - loss: 1.9199 - accuracy: 0.5847 - val_loss: 11.8173 - val_accuracy: 0.0230\n"
     ]
    }
   ],
   "source": [
    "# -Define hyperparameters for the custom language model:\n",
    "#  - vocab_size: The size of the vocabulary, which determines the number of unique words the model can work with.\n",
    "#  - embedding_dim: The dimensionality of word embeddings (vector representations) for words in the vocabulary.\n",
    "#  - max_seq_length: The maximum length of input sequences that the model will accept during training and generation.\n",
    "#  - lstm_units: The number of LSTM (Long Short-Term Memory) units in the model's hidden layers.\n",
    "#  - output_units: The number of units in the output layer, which matches the vocabulary size for text generation.\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "max_seq_length = 50\n",
    "lstm_units = 256\n",
    "output_units = vocab_size\n",
    "\n",
    "# -Initialize a Tokenizer with a specified vocabulary size and an out-of-vocabulary (OOV) token.-\n",
    "# -Then, fit the Tokenizer on the provided 'corpus' to build a vocabulary and prepare text data for tokenization.-\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# -Converting text to sequences (tokenization)-\n",
    "X_sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "# -Create training sequences (X_train) and labels (y_train) for text generation-\n",
    "sequences = []\n",
    "for seq in X_sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        sequences.append(seq[:i+1])\n",
    "\n",
    "# -Creating pad sequences-\n",
    "X_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='pre', truncating='pre')\n",
    "\n",
    "# -Spliting the data into training and validation sets-\n",
    "# - 80% for training and 20% for validation\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X_padded) * split_ratio)\n",
    "\n",
    "X_train = X_padded[:split_index, :-1]\n",
    "y_train = X_padded[:split_index, -1]\n",
    "\n",
    "X_val = X_padded[split_index:, :-1]\n",
    "y_val = X_padded[split_index:, -1]\n",
    "\n",
    "# -Building the model-\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_seq_length-1),\n",
    "    LSTM(lstm_units),\n",
    "    Dense(output_units, activation='softmax')\n",
    "])\n",
    "\n",
    "# -Compile and train the model-\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa42ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
